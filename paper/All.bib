
@inproceedings{antwerpenConstraintLanguageStatic2016,
  address = {{New York, NY, USA}},
  series = {{{PEPM}} '16},
  title = {A {{Constraint Language}} for {{Static Semantic Analysis Based}} on {{Scope Graphs}}},
  isbn = {978-1-4503-4097-7},
  abstract = {In previous work, we introduced scope graphs as a formalism for describing program binding structure and performing name resolution in an AST-independent way. In this paper, we show how to use scope graphs to build static semantic analyzers. We use constraints extracted from the AST to specify facts about binding, typing, and initialization. We treat name and type resolution as separate building blocks, but our approach can handle language constructs---such as record field access---for which binding and typing are mutually dependent. We also refine and extend our previous scope graph theory to address practical concerns including ambiguity checking and support for a wider range of scope relationships. We describe the details of constraint generation for a model language that illustrates many of the interesting static analysis issues associated with modules and records.},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  publisher = {{ACM}},
  doi = {10.1145/2847538.2847543},
  author = {van Antwerpen, Hendrik and N{\'e}ron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
  year = {2016},
  keywords = {Domain Specific Languages,Language Specification,Meta-Theory,Name Binding,Types},
  pages = {49--60}
}

@inproceedings{augustssonParadiseTwostageDSL2008,
  address = {{New York, NY, USA}},
  series = {{{ICFP}} '08},
  title = {Paradise: {{A Two}}-Stage {{DSL Embedded}} in {{Haskell}}},
  isbn = {978-1-59593-919-7},
  shorttitle = {Paradise},
  abstract = {We have implemented a two-stage language, Paradise, for building reusable components which are used to price financial products. Paradise is embedded in Haskell and makes heavy use of type-class based overloading, allowing the second stage to be compiled into a variety of backend platforms. Paradise has enabled us to begin moving away from implementation directly in monolithic Excel spreadsheets and towards a more modular and retargetable approach.},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/1411204.1411236},
  author = {Augustsson, Lennart and Mansell, Howard and Sittampalam, Ganesh},
  year = {2008},
  keywords = {dsels,Haskell,metaprogramming,paradise},
  pages = {225--228}
}

@inproceedings{axelssonAnalyzingContextFreeGrammars2008,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Analyzing {{Context}}-{{Free Grammars Using}} an {{Incremental SAT Solver}}},
  isbn = {978-3-540-70583-3},
  abstract = {We consider bounded versions of undecidable problems about context-free languages which restrict the domain of words to some finite length: inclusion, intersection, universality, equivalence, and ambiguity. These are in (co)-NP and thus solvable by a reduction to the (un-)satisfiability problem for propositional logic. We present such encodings \textendash{} fully utilizing the power of incrementat SAT solvers \textendash{} prove correctness and validate this approach with benchmarks.},
  language = {en},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Axelsson, Roland and Heljanko, Keijo and Lange, Martin},
  editor = {Aceto, Luca and Damg{\aa}rd, Ivan and Goldberg, Leslie Ann and Halld{\'o}rsson, Magn{\'u}s M. and Ing{\'o}lfsd{\'o}ttir, Anna and Walukiewicz, Igor},
  year = {2008},
  keywords = {Ambiguous Word,Conjunctive Normal Form,Parse Tree,Propositional Formula,Word Problem},
  pages = {410-422}
}

@article{aycockPracticalEarleyParsing2002,
  title = {Practical {{Earley Parsing}}},
  volume = {45},
  issn = {0010-4620},
  abstract = {Earley's parsing algorithm is a general algorithm, able to handle any context-free grammar. As with most parsing algorithms, however, the presence of grammar rules having empty right-hand sides complicates matters. By analyzing why Earley's algorithm struggles with these grammar rules, we have devised a simple solution to the problem. Our empty-rule solution leads to a new type of finite automaton expressly suited for use in Earley parsers and to a new statement of Earley's algorithm. We show that this new form of Earley parser is much more time efficient in practice than the original.},
  number = {6},
  journal = {The Computer Journal},
  doi = {10.1093/comjnl/45.6.620},
  author = {Aycock, John and Horspool, R. Nigel},
  year = {2002},
  keywords = {Algorithms,Article,Automation,Computing Milieux (General) (Ci),Copyrights,Grammars,Handles,Mathematical Analysis,Mathematical Models,Parsers,Parsing Algorithms},
  pages = {620--630}
}

@inproceedings{bjesseLavaHardwareDesign1998,
  address = {{New York, NY, USA}},
  series = {{{ICFP}} '98},
  title = {Lava: {{Hardware Design}} in {{Haskell}}},
  isbn = {978-1-58113-024-9},
  shorttitle = {Lava},
  abstract = {Lava is a tool to assist circuit designers in specifying, designing, verifying and implementing hardware. It is a collection of Haskell modules. The system design exploits functional programming language features, such as monads and type classes, to provide multiple interpretations of circuit descriptions. These interpretations implement standard circuit analyses such as simulation, formal verification and the generation of code for the production of real circuits.Lava also uses polymorphism and higher order functions to provide more abstract and general descriptions than are possible in traditional hardware description languages. Two Fast Fourier Transform circuit examples illustrate this.},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/289423.289440},
  author = {Bjesse, Per and Claessen, Koen and Sheeran, Mary and Singh, Satnam},
  year = {1998},
  pages = {174--184}
}

@inproceedings{brabrandAnalyzingAmbiguityContextFree2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Analyzing {{Ambiguity}} of {{Context}}-{{Free Grammars}}},
  isbn = {978-3-540-76336-9},
  abstract = {It has been known since 1962 that the ambiguity problem for context-free grammars is undecidable. Ambiguity in context-free grammars is a recurring problem in language design and parser generation, as well as in applications where grammars are used as models of real-world physical structures.We observe that there is a simple linguistic characterization of the grammar ambiguity problem, and we show how to exploit this to conservatively approximate the problem based on local regular approximations and grammar unfoldings. As an application, we consider grammars that occur in RNA analysis in bioinformatics, and we demonstrate that our static analysis of context-free grammars is sufficiently precise and efficient to be practically useful.},
  language = {en},
  booktitle = {Implementation and {{Application}} of {{Automata}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Brabrand, Claus and Giegerich, Robert and M{\o}ller, Anders},
  editor = {Holub, Jan and {\v Z}{\v d}{\'a}rek, Jan},
  year = {2007},
  keywords = {CFG ambiguity,regular approximation,RNA analysis},
  pages = {214-225}
}

@article{bravenboerStrategoXT172008,
  title = {Stratego/{{XT}} 0.17. {{A Language}} and {{Toolset}} for {{Program Transformation}}},
  volume = {72},
  issn = {0167-6423},
  abstract = {Stratego/XT is a language and toolset for program transformation. The Stratego language provides rewrite rules for expressing basic transformations, programmable rewriting strategies for controlling the application of rules, concrete syntax for expressing the patterns of rules in the syntax of the object language, and dynamic rewrite rules for expressing context-sensitive transformations, thus supporting the development of transformation components at a high level of abstraction. The XT toolset offers a collection of flexible, reusable transformation components, and tools for generating such components from declarative specifications. Complete program transformation systems are composed from these components. This paper gives an overview of Stratego/XT 0.17, including a description of the Stratego language and XT transformation tools; a discussion of the implementation techniques and software engineering process; and a description of applications built with Stratego/XT.},
  number = {1-2},
  journal = {Sci. Comput. Program.},
  doi = {10.1016/j.scico.2007.11.003},
  author = {Bravenboer, Martin and Kalleberg, Karl Trygve and Vermaas, Rob and Visser, Eelco},
  month = jun,
  year = {2008},
  keywords = {Concrete syntax,Dynamic rewrite rules,Program transformation,Rewrite rules,Rewriting strategies,Stratego,Stratego/XT},
  pages = {52--70}
}

@techreport{bromanModelyzeGraduallyTyped2012,
  title = {Modelyze: A {{Gradually Typed Host Language}} for {{Embedding Equation}}-{{Based Modeling Languages}}},
  number = {UCB/EECS-2012-173},
  institution = {{EECS Department, University of California, Berkeley}},
  author = {Broman, David and Siek, Jeremy G.},
  month = jun,
  year = {2012}
}

@inproceedings{bromanGraduallyTypedSymbolic2018,
  address = {{New York, NY, USA}},
  series = {{{PEPM}} '18},
  title = {Gradually {{Typed Symbolic Expressions}}},
  isbn = {978-1-4503-5587-2},
  abstract = {Embedding a domain-specific language (DSL) in a general purpose host language is an efficient way to develop a new DSL. Various kinds of languages and paradigms can be used as host languages, including object-oriented, functional, statically typed, and dynamically typed variants, all having their pros and cons. For deep embedding, statically typed languages enable early checking and potentially good DSL error messages, instead of reporting runtime errors. Dynamically typed languages, on the other hand, enable flexible transformations, thus avoiding extensive boilerplate code. In this paper, we introduce the concept of gradually typed symbolic expressions that mix static and dynamic typing for symbolic data. The key idea is to combine the strengths of dynamic and static typing in the context of deep embedding of DSLs. We define a gradually typed calculus {$\lambda{}<\star{}>$}, formalize its type system and dynamic semantics, and prove type safety. We introduce a host language called Modelyze that is based on {$\lambda{}<\star{}>$}, and evaluate the approach by embedding a series of equation-based domain-specific modeling languages, all within the domain of physical modeling and simulation.},
  booktitle = {Proceedings of the {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  publisher = {{ACM}},
  doi = {10.1145/3162068},
  author = {Broman, David and Siek, Jeremy G.},
  year = {2018},
  keywords = {DSL,Symbolic expressions,Type systems},
  pages = {15--29}
}

@article{cantorAmbiguityProblemBackus1962,
  title = {On {{The Ambiguity Problem}} of {{Backus Systems}}},
  volume = {9},
  issn = {0004-5411},
  abstract = {Backus [1] has developed an elegant method of defining well-formed formulas for computer languages such as ALGOL. It consists of (our notation is slightly different from that of Backus):    A finite alphabet: a1, a2, \ldots, at; Predicates: P1, P2, \ldots, P{$\epsilon$}; Productions, either of the form (a) aj {$\in$} Pi;},
  number = {4},
  journal = {Journal of the ACM},
  doi = {10.1145/321138.321145},
  author = {Cantor, David G.},
  month = oct,
  year = {1962},
  pages = {477--479}
}

@incollection{chomskyAlgebraicTheoryContextFree1963,
  series = {Computer {{Programming}} and {{Formal Systems}}},
  title = {The {{Algebraic Theory}} of {{Context}}-{{Free Languages}}*},
  volume = {35},
  abstract = {This chapter discusses the several classes of sentence-generating devices that are closely related, in various ways, to the grammars of both natural languages and artificial languages of various kinds. By a language it simply mean a set of strings in some finite set V of symbols called the vocabulary of the language. By a grammar a set of rules that give a recursive enumeration of the strings belonging to the language. It can be said that the grammar generates these strings. The chapter discusses the aspect of the structural description of a sentence, namely, its subdivision into phrases belonging to various categories. A major concern of the general theory of natural languages is to define the class of possible strings; the class of possible grammars; the class of possible structural descriptions; a procedure for assigning structural descriptions to sentences, given a grammar; and to do all of this in such a way that the structural description assigned to a sentence by the grammar of a natural language will provide the basis for explaining how a speaker of this language would understand this sentence.},
  booktitle = {Studies in {{Logic}} and the {{Foundations}} of {{Mathematics}}},
  publisher = {{Elsevier}},
  author = {Chomsky, N. and Sch{\"u}tzenberger, M. P.},
  editor = {Braffort, P. and Hirschberg, D.},
  month = jan,
  year = {1963},
  pages = {118-161},
  doi = {10.1016/S0049-237X(08)72023-8}
}

@inproceedings{clingerMacrosThatWork1991,
  address = {{New York, NY, USA}},
  series = {{{POPL}} '91},
  title = {Macros {{That Work}}},
  isbn = {978-0-89791-419-2},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/99583.99607},
  author = {Clinger, William and Rees, Jonathan},
  year = {1991},
  pages = {155--162}
}

@article{dybvigSyntacticAbstractionScheme1993,
  title = {Syntactic Abstraction in Scheme},
  volume = {5},
  issn = {1573-0557},
  abstract = {Naive program transformations can have surprising effects due to the interaction between introduced identifier references and previously existing identifier bindings, or between introduced bindings and previously existing references. These interactions can result in inadvertent binding, or capturing, of identifiers. A further complication is that transformed programs may have little resemblance to original programs, making correlation of source and object code difficult. This article describes an efficient macro system that prevents inadvertent capturing while maintaining the correlation between source and object code. The macro system allows the programmer to define program transformations using an unrestricted, general-purpose language. Previous approaches to the capturing problem have been inadequate, overly restrictive, or inefficient, and the problem of source-object correlation has been largely unaddressed. The macro system is based on a new algorithm for implementing syntactic transformations and a new representation for syntactic expressions.},
  language = {en},
  number = {4},
  journal = {LISP and Symbolic Computation},
  doi = {10.1007/BF01806308},
  author = {Dybvig, R. Kent and Hieb, Robert and Bruggeman, Carl},
  month = dec,
  year = {1993},
  keywords = {Hygienic Macros,Macros,Program Transformation,Syntactic Abstraction},
  pages = {295-326}
}

@article{earleyEfficientContextfreeParsing1970,
  title = {An {{Efficient Context}}-Free {{Parsing Algorithm}}},
  volume = {13},
  issn = {0001-0782},
  abstract = {A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick.},
  number = {2},
  journal = {Communications of the ACM},
  doi = {10.1145/362007.362035},
  author = {Earley, Jay},
  month = feb,
  year = {1970},
  keywords = {compilers,computational complexity,context-free grammar,parsing,syntax analysis},
  pages = {94--102}
}

@article{earleyAmbiguityPrecedenceSyntax1975,
  title = {Ambiguity and Precedence in Syntax Description},
  volume = {4},
  issn = {1432-0525},
  abstract = {SummaryThis paper describes a method of syntax description for programming languages which allows one to factor out that part of the description which deals with the relative precedences of syntactic units. This has been found to produce simpler and more flexible syntax descriptions. It is done by allowing the normal part of the description, which is done in BNF, to be ambiguous; these ambiguities are then resolved by a separate part of the description which gives precedence relations between the conflicting productions from the grammar. The method can be used with any left-to-right parser which is capable of detecting ambiguities and recognizing which productions they come from; We have studied its use with an LR(1) parser, and it requires a small and localized addition to the parser to enable it to deal with the precedence relations.},
  language = {en},
  number = {2},
  journal = {Acta Informatica},
  doi = {10.1007/BF00288747},
  author = {Earley, Jay},
  month = jun,
  year = {1975},
  keywords = {Communication Network,Data Structure,Information System,Information Theory,Operating System},
  pages = {183-192}
}

@article{ekmanJastAddSystemModular2007,
  series = {Special Issue on {{Experimental Software}} and {{Toolkits}}},
  title = {The {{JastAdd}} System \textemdash{} Modular Extensible Compiler Construction},
  volume = {69},
  issn = {0167-6423},
  abstract = {The JastAdd system enables modular specifications of extensible compiler tools and languages. Java has been extended with the Rewritable Circular Reference Attributed Grammars formalism that supports modularization and extensibility through several synergistic mechanisms. Object-orientation and static aspect-oriented programming are combined with declarative attributes and context-dependent rewrites to allow highly modular specifications. The techniques have been verified by implementing a full Java 1.4 compiler with modular extensions for non-null types and Java 5 features.},
  number = {1},
  journal = {Science of Computer Programming},
  doi = {10.1016/j.scico.2007.02.003},
  author = {Ekman, Torbj{\"o}rn and Hedin, G{\"o}rel},
  month = dec,
  year = {2007},
  keywords = {Compiler construction,Extensible languages,Modular implementation},
  pages = {14-26}
}

@inproceedings{erdwegLayoutSensitiveGeneralizedParsing2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Layout-{{Sensitive Generalized Parsing}}},
  isbn = {978-3-642-36089-3},
  abstract = {The theory of context-free languages is well-understood and context-free parsers can be used as off-the-shelf tools in practice. In particular, to use a context-free parser framework, a user does not need to understand its internals but can specify a language declaratively as a grammar. However, many languages in practice are not context-free. One particularly important class of such languages is layout-sensitive languages, in which the structure of code depends on indentation and whitespace. For example, Python, Haskell, F\#, and Markdown use indentation instead of curly braces to determine the block structure of code. Their parsers (and lexers) are not declaratively specified but hand-tuned to account for layout-sensitivity.To support declarative specifications of layout-sensitive languages, we propose a parsing framework in which a user can annotate layout in a grammar. Annotations take the form of constraints on the relative positioning of tokens in the parsed subtrees. For example, a user can declare that a block consists of statements that all start on the same column. We have integrated layout constraints into SDF and implemented a layout-sensitive generalized parser as an extension of generalized LR parsing. We evaluate the correctness and performance of our parser by parsing 33 290 open-source Haskell files. Layout-sensitive generalized parsing is easy to use, and its performance overhead compared to layout-insensitive parsing is small enough for practical application.},
  language = {en},
  booktitle = {Software {{Language Engineering}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Erdweg, Sebastian and Rendel, Tillmann and K{\"a}stner, Christian and Ostermann, Klaus},
  editor = {Czarnecki, Krzysztof and Hedin, G{\"o}rel},
  year = {2013},
  keywords = {Parse Tree,Abstract Syntax Tree,Curly Brace,Parse Time,Statement List},
  pages = {244-263}
}

@article{flattMacrosThatWork2012,
  title = {Macros That {{Work Together}}: {{Compile}}-Time Bindings, Partial Expansion, and Definition Contexts},
  volume = {22},
  issn = {1469-7653, 0956-7968},
  shorttitle = {Macros That {{Work Together}}},
  abstract = {Racket is a large language that is built mostly within itself. Unlike the usual approach taken by non-Lisp languages, the self-hosting of Racket is not a matter of bootstrapping one implementation through a previous implementation, but instead a matter of building a tower of languages and libraries via macros. The upper layers of the tower include a class system, a component system, pedagogic variants of Scheme, a statically typed dialect of Scheme, and more. The demands of this language-construction effort require a macro system that is substantially more expressive than previous macro systems. In particular, while conventional Scheme macro systems handle stand-alone syntactic forms adequately, they provide weak support for macros that share information or macros that use existing syntactic forms in new contexts. This paper describes and models features of the Racket macro system, including support for general compile-time bindings, sub-form expansion and analysis, and environment management. The presentation assumes a basic familiarity with Lisp-style macros, and it takes for granted the need for macros that respect lexical scope. The model, however, strips away the pattern and template system that is normally associated with Scheme macros, isolating a core that is simpler, can support pattern and template forms themselves as macros, and generalizes naturally to Racket's other extensions.},
  language = {en},
  number = {2},
  journal = {Journal of Functional Programming},
  doi = {10.1017/S0956796812000093},
  author = {Flatt, Matthew and Culpepper, Ryan and Darais, David and Findler, Robert Bruce},
  month = mar,
  year = {2012},
  pages = {181-216}
}

@inproceedings{flattBindingSetsScopes2016,
  address = {{New York, NY, USA}},
  series = {{{POPL}} '16},
  title = {Binding {{As Sets}} of {{Scopes}}},
  isbn = {978-1-4503-3549-2},
  abstract = {Our new macro expander for Racket builds on a novel approach to hygiene. Instead of basing macro expansion on variable renamings that are mediated by expansion history, our new expander tracks binding through a set of scopes that an identifier acquires from both binding forms and macro expansions. The resulting model of macro expansion is simpler and more uniform than one based on renaming, and it is sufficiently compatible with Racket's old expander to be practical.},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/2837614.2837620},
  author = {Flatt, Matthew},
  year = {2016},
  keywords = {Macros,binding,hygiene,scope},
  pages = {705--717}
}

@article{ginsburgAmbiguityContextFree1966,
  title = {Ambiguity in {{Context Free Languages}}},
  volume = {13},
  issn = {0004-5411},
  abstract = {Four principal results about ambiguity in languages (i.e., context free languages) are proved. It is first shown that the problem of determining whether an arbitrary language is inherently ambiguous is recursively unsolvable. Then a decision procedure is presented for determining whether an arbitrary bounded grammar is ambiguous. Next, a necessary and sufficient algebraic condition is given for a bounded language to be inherently ambiguous. Finally, it is shown that no language contained in w1*w2*, each w1 a word, is inherently ambiguous.},
  number = {1},
  journal = {Journal of the ACM},
  doi = {10.1145/321312.321318},
  author = {Ginsburg, Seymour and Ullian, Joseph},
  month = jan,
  year = {1966},
  pages = {62--89}
}

@article{heeringSyntaxDefinitionFormalism1989,
  title = {The {{Syntax Definition Formalism SDF}}\textemdash{{Reference Manual}}\textemdash{}},
  volume = {24},
  issn = {0362-1340},
  abstract = {SDF is a formalism for the definition of syntax which is comparable to BNF in some respects, but has a wider scope in that it also covers the definition of lexical and abstract syntax. Its design and implementation are tailored towards the language designer who wants to develop new languages as well as implement existing ones in a highly interactive manner. It emphasizes compactness of syntax definitions by offering (a) a standard interface between lexical and context-free syntax; (b) a standard correspondence between context-free and abstract syntax; (c) powerful disambiguation and list constructs; and (d) an efficient incremental implementation which accepts arbitrary context-free syntax definitions. SDF can be combined with a variety of programming and specification languages. In this way these obtain fully general user-definable syntax.},
  number = {11},
  journal = {SIGPLAN Not.},
  doi = {10.1145/71605.71607},
  author = {Heering, J. and Hendriks, P. R. H. and Klint, P. and Rekers, J.},
  month = nov,
  year = {1989},
  pages = {43--75}
}

@inproceedings{hermanTheoryHygienicMacros2008,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Theory}} of {{Hygienic Macros}}},
  isbn = {978-3-540-78739-6},
  abstract = {Hygienic macro systems, such as Scheme's, automatically rename variables to prevent unintentional variable capture\textemdash{}in short, they ``just work.'' Yet hygiene has never been formally presented as a specification rather than an algorithm. According to folklore, the definition of hygienic macro expansion hinges on the preservation of alpha-equivalence. But the only known notion of alpha-equivalence for programs with macros depends on the results of macro expansion! We break this circularity by introducing explicit binding specifications into the syntax of macro definitions, permitting a definition of alpha-equivalence independent of expansion. We define a semantics for a first-order subset of Scheme-like macros and prove hygiene as a consequence of confluence.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Herman, David and Wand, Mitchell},
  editor = {Drossopoulou, Sophia},
  year = {2008},
  keywords = {Core Form,Pattern Variable,Scheme Program,Shape Type,Type Annotation},
  pages = {48-62}
}

@inproceedings{hickeyClojureProgrammingLanguage2008,
  address = {{New York, NY, USA}},
  series = {{{DLS}} '08},
  title = {The {{Clojure Programming Language}}},
  isbn = {978-1-60558-270-2},
  abstract = {Customers and stakeholders have substantial investments in, and are comfortable with the performance, security and stability of, industry-standard platforms like the JVM and CLR. While Java and C\# developers on those platforms may envy the succinctness, flexibility and productivity of dynamic languages, they have concerns about running on customer-approved infrastructure, access to their existing code base and libraries, and performance. In addition, they face ongoing problems dealing with concurrency using native threads and locking. Clojure is an effort in pragmatic dynamic language design in this context. It endeavors to be a general-purpose language suitable in those areas where Java is suitable. It reflects the reality that, for the concurrent programming future, pervasive, unmoderated mutation simply has to go. Clojure meets its goals by: embracing an industry-standard, open platform - the JVM; modernizing a venerable language - Lisp; fostering functional programming with immutable persistent data structures; and providing built-in concurrency support via software transactional memory and asynchronous agents. The result is robust, practical, and fast. This talk will focus on the motivations, mechanisms and experiences of the implementation of Clojure.},
  booktitle = {Proceedings of the 2008 {{Symposium}} on {{Dynamic Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/1408681.1408682},
  author = {Hickey, Rich},
  year = {2008},
  pages = {1:1--1:1}
}

@article{hudakBuildingDomainspecificEmbedded1996,
  title = {Building {{Domain}}-Specific {{Embedded Languages}}},
  volume = {28},
  issn = {0360-0300},
  number = {4es},
  journal = {ACM Comput. Surv.},
  doi = {10.1145/242224.242477},
  author = {Hudak, Paul},
  month = dec,
  year = {1996}
}

@article{kaminskiReliablyComposableLanguage2017,
  title = {Reliably Composable Language Extensions},
  abstract = {Many programming tasks are dramatically simpler when an appropriate domain-specific language can be used to accomplish them. These languages offer a variety of potential advantages, including programming at a higher level of abstraction, custom analyses specific to the problem domain, and the ability to generate very efficient code. But they also suffer many disadvantages as a result of their implementation techniques. Fully separate languages (such as YACC, or SQL) are quite flexible, but these are distinct monolithic entities and thus we are unable to draw on the features of several in combination to accomplish a single task. That is, we cannot compose their domain-specific features. "Embedded" DSLs (such as parsing combinators) accomplish something like a different language, but are actually implemented simply as libraries within a flexible host language. This approach allows different libraries to be imported and used together, enabling composition, but it is limited in analysis and translation capabilities by the host language they are embedded within. A promising combination of these two approaches is to allow a host language to be directly extended with new features (syntactic and semantic.) However, while there are plausible ways to attempt to compose language extensions, they can easily fail, making this approach unreliable. Previous methods of assuring reliable composition impose onerous restrictions, such as throwing out entirely the ability to introduce new analysis. This thesis introduces reliably composable language extensions as a technique for the implementation of DSLs. This technique preserves most of the advantages of both separate and "embedded" DSLs. Unlike many prior approaches to language extension, this technique ensures composition of multiple language extensions will succeed, and preserves strong properties about the behavior of the resulting composed compiler. We define an analysis on language extensions that guarantees the composition of several extensions will be well-defined, and we further define a set of testable properties that ensure the resulting compiler will behave as expected, along with a principle that assigns "blame" for bugs that may ultimately appear as a result of composition. Finally, to concretely compare our approach to our original goals for reliably composable language extension, we use these techniques to develop an extensible C compiler front-end, together with several example composable language extensions.},
  language = {en},
  doi = {https://doi.org/10.24926/2017.188954},
  author = {Kaminski, Ted},
  month = may,
  year = {2017}
}

@article{leoGeneralContextfreeParsing1991,
  title = {A General Context-Free Parsing Algorithm Running in Linear Time on Every {{LR}}(k) Grammar without Using Lookahead},
  volume = {82},
  issn = {0304-3975},
  abstract = {A new general context-free parsing algorithm is presented which runs in linear time and space on every LR(k) grammar without using any lookahead and without making use of the LR property. Most of the existing implementations of tabular parsing algorithms, including those using lookahead, can easily be adapted to this new algorithm without a noteworthy loss of efficiency. For some natural right recursive grammars both the time and space complexity will be improved from {$\Omega$}(n2) to O(n). This makes this algorithm not only of theoretical but probably of practical interest as well.},
  number = {1},
  journal = {Theoretical Computer Science},
  doi = {10.1016/0304-3975(91)90180-A},
  author = {Leo, Joop M. I. M.},
  month = may,
  year = {1991},
  pages = {165-176}
}

@inproceedings{lorenzenSoundTypedependentSyntactic2016,
  address = {{New York, NY, USA}},
  series = {{{POPL}} '16},
  title = {Sound {{Type}}-Dependent {{Syntactic Language Extension}}},
  isbn = {978-1-4503-3549-2},
  abstract = {Syntactic language extensions can introduce new facilities into a programming language while requiring little implementation effort and modest changes to the compiler. It is typical to desugar language extensions in a distinguished compiler phase after parsing or type checking, not affecting any of the later compiler phases. If desugaring happens before type checking, the desugaring cannot depend on typing information and type errors are reported in terms of the generated code. If desugaring happens after type checking, the code generated by the desugaring is not type checked and may introduce vulnerabilities. Both options are undesirable. We propose a system for syntactic extensibility where desugaring happens after type checking and desugarings are guaranteed to only generate well-typed code. A major novelty of our work is that desugarings operate on typing derivations instead of plain syntax trees. This provides desugarings access to typing information and forms the basis for the soundness guarantee we provide, namely that a desugaring generates a valid typing derivation. We have implemented our system for syntactic extensibility in a language-independent fashion and instantiated it for a substantial subset of Java, including generics and inheritance. We provide a sound Java extension for Scala-like for-comprehensions.},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/2837614.2837644},
  author = {Lorenzen, Florian and Erdweg, Sebastian},
  year = {2016},
  keywords = {metaprogramming,automatic verification,Language extensibility,macros,type soundness,type-dependent desugaring},
  pages = {204--216}
}

@inproceedings{matsakisRustLanguage2014,
  address = {{New York, NY, USA}},
  series = {{{HILT}} '14},
  title = {The {{Rust Language}}},
  isbn = {978-1-4503-3217-0},
  abstract = {Rust is a new programming language for developing reliable and efficient systems. It is designed to support concurrency and parallelism in building applications and libraries that take full advantage of modern hardware. Rust's static type system is safe and expressive and provides strong guarantees about isolation, concurrency, and memory safety.

Rust also offers a clear performance model, making it easier to predict and reason about program efficiency. One important way it accomplishes this is by allowing fine-grained control over memory representations, with direct support for stack allocation and contiguous record storage. The language balances such controls with the absolute requirement for safety: Rust's type system and runtime guarantee the absence of data races, buffer overflows, stack overflows, and accesses to uninitialized or deallocated memory.},
  booktitle = {Proceedings of the 2014 {{ACM SIGAda Annual Conference}} on {{High Integrity Language Technology}}},
  publisher = {{ACM}},
  doi = {10.1145/2663171.2663188},
  author = {Matsakis, Nicholas D. and Klock, II, Felix S.},
  year = {2014},
  keywords = {affine type systems,memory management,rust,systems programming},
  pages = {103--104}
}

@inproceedings{neronTheoryNameResolution2015,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Theory}} of {{Name Resolution}}},
  isbn = {978-3-662-46669-8},
  abstract = {We describe a language-independent theory for name binding and resolution, suitable for programming languages with complex scoping rules including both lexical scoping and modules. We formulate name resolution as a two-stage problem. First a language-independent scope graph is constructed using language-specific rules from an abstract syntax tree. Then references in the scope graph are resolved to corresponding declarations using a language-independent resolution process. We introduce a resolution calculus as a concise, declarative, and languageindependent specification of name resolution. We develop a resolution algorithm that is sound and complete with respect to the calculus. Based on the resolution calculus we develop language-independent definitions of {$\alpha$}-equivalence and rename refactoring. We illustrate the approach using a small example language with modules. In addition, we show how our approach provides a model for a range of name binding patterns in existing languages.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Neron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
  editor = {Vitek, Jan},
  year = {2015},
  keywords = {Abstract Syntax Tree,Binding Pattern,Code Completion,Resolution Algorithm,Visibility Policy},
  pages = {205-231}
}

@article{oderskySimplicitlyFoundationsApplications2017,
  title = {Simplicitly: {{Foundations}} and {{Applications}} of {{Implicit Function Types}}},
  volume = {2},
  issn = {2475-1421},
  shorttitle = {Simplicitly},
  abstract = {Understanding a program entails understanding its context; dependencies, configurations and even implementations are all forms of contexts. Modern programming languages and theorem provers offer an array of constructs to define contexts, implicitly. Scala offers implicit parameters which are used pervasively, but which cannot be abstracted over.  This paper describes a generalization of implicit parameters to implicit function types, a powerful way to abstract over the context in which some piece of code is run. We provide a formalization based on bidirectional type-checking that closely follows the semantics implemented by the Scala compiler.  To demonstrate their range of abstraction capabilities, we present several applications that make use of implicit function types. We show how to encode the builder pattern, tagless interpreters, reader and free monads and we assess the performance of the monadic structures presented.},
  number = {POPL},
  journal = {Proc. ACM Program. Lang.},
  doi = {10.1145/3158130},
  author = {Odersky, Martin and Blanvillain, Olivier and Liu, Fengyun and Biboudis, Aggelos and Miller, Heather and Stucki, Sandro},
  month = dec,
  year = {2017},
  keywords = {Dotty,implicit parameters,Scala},
  pages = {42:1--42:29}
}

@book{palmkvistBuildingProgrammingLanguages2018,
  series = {{{TRITA}}-{{EECS}}-{{EX}}},
  title = {Building {{Programming Languages}}, {{Construction}} by {{Construction}}},
  abstract = {The task of implementing a programming language is a task that entails a great deal of work. Yet much of this work is similar for different programming languages: most languages require, e.g., parsing, name resolution, type-checking, and optimization. When implementing domain-specific languages (DSLs) the reimplementation of these largely similar tasks seems especially redundant. A number of approaches exist to alleviate this issue, including embedded DSLs, macro-rewriting systems, and more general systems intended for language implementation. However, these tend to have at least one of the following limitations: They present a leaky abstraction, e.g., error messages do not refer to the DSL but rather some other programming language, namely the one used to implement the DSL. They limit the flexibility of the DSL, either to the constructs present in another language, or merely to the syntax of some other language. They see an entire language as the unit of composition. Complete languages are extended with other complete language extensions. Instead, this thesis introduces the concept of a syntax construction, which represents a smaller unit of composition. A syntax construction defines a single language feature, e.g., an if-statement, an anonymous function, or addition. Each syntax construction specifies its own syntax, binding semantics, and runtime semantics, independent of the rest of the language. The runtime semantics are defined using a translation into another target language, similarly to macros. These translations can then be checked to ensure that they preserve binding semantics and introduce no binding errors. This checking ensures that binding errors can be presented in terms of code the programmer wrote, rather than generated code in some underlying language. During evaluation several limitations are encountered. Removing or minimizing these limitations appears possible, but is left for future work},
  language = {eng},
  number = {2018:408},
  publisher = {{KTH, School of Electrical Engineering and Computer Science (EECS)}},
  author = {Palmkvist, Viktor},
  year = {2018},
  keywords = {domain-specific language,programming language construction}
}

@inproceedings{parrLLFoundationANTLR2011,
  address = {{New York, NY, USA}},
  series = {{{PLDI}} '11},
  title = {{{LL}}(*): {{The Foundation}} of the {{ANTLR Parser Generator}}},
  isbn = {978-1-4503-0663-8},
  shorttitle = {{{LL}}(*)},
  abstract = {Despite the power of Parser Expression Grammars (PEGs) and GLR, parsing is not a solved problem. Adding nondeterminism (parser speculation) to traditional LL and LR parsers can lead to unexpected parse-time behavior and introduces practical issues with error handling, single-step debugging, and side-effecting embedded grammar actions. This paper introduces the LL(*) parsing strategy and an associated grammar analysis algorithm that constructs LL(*) parsing decisions from ANTLR grammars. At parse-time, decisions gracefully throttle up from conventional fixed k{$>$}=1 lookahead to arbitrary lookahead and, finally, fail over to backtracking depending on the complexity of the parsing decision and the input symbols. LL(*) parsing strength reaches into the context-sensitive languages, in some cases beyond what GLR and PEGs can express. By statically removing as much speculation as possible, LL(*) provides the expressivity of PEGs while retaining LL's good error handling and unrestricted grammar actions. Widespread use of ANTLR (over 70,000 downloads/year) shows that it is effective for a wide variety of applications.},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/1993498.1993548},
  author = {Parr, Terence and Fisher, Kathleen},
  year = {2011},
  keywords = {augmented transition networks,backtracking,context-sensitive parsing,deterministic finite automata,glr,memoization,nondeterministic parsing,peg,semantic predicates,subset construction,syntactic predicates},
  pages = {425--436}
}

@inproceedings{rompfLightweightModularStaging2010,
  address = {{New York, NY, USA}},
  series = {{{GPCE}} '10},
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  isbn = {978-1-4503-0154-1},
  shorttitle = {Lightweight {{Modular Staging}}},
  abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used. We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process. We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  publisher = {{ACM}},
  doi = {10.1145/1868294.1868314},
  author = {Rompf, Tiark and Odersky, Martin},
  year = {2010},
  keywords = {code generation,domain-specific languages,language virtualization,multi-stage programming},
  pages = {127--136}
}

@inproceedings{sheardTemplateMetaprogrammingHaskell2002,
  address = {{New York, NY, USA}},
  series = {Haskell '02},
  title = {Template {{Meta}}-Programming for {{Haskell}}},
  isbn = {978-1-58113-605-0},
  abstract = {We propose a new extension to the purely functional programming language Haskell that supports compile-time meta-programming. The purpose of the system is to support the algorithmic construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc.},
  booktitle = {Proceedings of the 2002 {{ACM SIGPLAN Workshop}} on {{Haskell}}},
  publisher = {{ACM}},
  doi = {10.1145/581690.581691},
  author = {Sheard, Tim and Jones, Simon Peyton},
  year = {2002},
  keywords = {meta programming,templates},
  pages = {1--16}
}

@inproceedings{silkensenWellTypedIslandsParse2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Well-{{Typed Islands Parse Faster}}},
  isbn = {978-3-642-40447-4},
  abstract = {This paper addresses the problem of specifying and parsing the syntax of domain-specific languages (DSLs) in a modular, user-friendly way. We want to enable the design of composable DSLs that combine the natural syntax of external DSLs with the easy implementation of internal DSLs. The challenge in parsing these DSLs is that the composition of several languages is likely to contain ambiguities. We present the design of a system that uses a type-oriented variant of island parsing to efficiently parse the syntax of composable DSLs. In particular, we argue that the running time of type-oriented island parsing doesn't depend on the number of DSLs imported. We also show how to use our tool to implement DSLs on top of a host language such as Typed Racket.},
  language = {en},
  booktitle = {Trends in {{Functional Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Silkensen, Erik and Siek, Jeremy},
  editor = {Loidl, Hans-Wolfgang and Pe{\~n}a, Ricardo},
  year = {2013},
  keywords = {Parse Tree,Concrete Syntax,Deductive System,Grammar Rule,Input String},
  pages = {69-84}
}

@article{stansiferRomeoSystemMore2016,
  title = {Romeo: {{A}} System for More Flexible Binding-Safe Programming*},
  volume = {26},
  issn = {0956-7968, 1469-7653},
  shorttitle = {Romeo},
  abstract = {Current systems for safely manipulating values containing names only support simple binding structures for those names. As a result, few tools exist to safely manipulate code in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects {$\alpha$}-equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complex-binding support of David Herman's {$\lambda$}m
, but is a full-fledged binding-safe language like Pure FreshML.},
  language = {en},
  journal = {Journal of Functional Programming},
  doi = {10.1017/S0956796816000137},
  author = {Stansifer, Paul and Wand, Mitchell},
  year = {2016/ed}
}

@inproceedings{steeleOverviewCOMMONLISP1982,
  address = {{New York, NY, USA}},
  series = {{{LFP}} '82},
  title = {An {{Overview}} of {{COMMON LISP}}},
  isbn = {978-0-89791-082-8},
  abstract = {A dialect of LISP called ``COMMON LISP'' is being cooperatively developed and implemented at several sites. It is a descendant of the MACLISP family of LISP dialects, and is intended to unify the several divergent efforts of the last five years. We first give an extensive history of LISP, particularly of the MACLISP branch, in order to explain in context the motivation for COMMON LISP. We enumerate the goals and non-goals of the language design, discuss the language features of primary interest, and then consider how these features help to meet the expressed goals. Finally, the status (as of May 1982) of six implementations of COMMON LISP is summarized.},
  booktitle = {Proceedings of the 1982 {{ACM Symposium}} on {{LISP}} and {{Functional Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/800068.802140},
  author = {Steele, Jr., Guy L.},
  year = {1982},
  pages = {98--107}
}

@article{sujeethDeliteCompilerArchitecture2014,
  title = {Delite: {{A Compiler Architecture}} for {{Performance}}-{{Oriented Embedded Domain}}-{{Specific Languages}}},
  volume = {13},
  issn = {1539-9087},
  shorttitle = {Delite},
  abstract = {Developing high-performance software is a difficult task that requires the use of low-level, architecture-specific programming models (e.g., OpenMP for CMPs, CUDA for GPUs, MPI for clusters). It is typically not possible to write a single application that can run efficiently in different environments, leading to multiple versions and increased complexity. Domain-Specific Languages (DSLs) are a promising avenue to enable programmers to use high-level abstractions and still achieve good performance on a variety of hardware. This is possible because DSLs have higher-level semantics and restrictions than general-purpose languages, so DSL compilers can perform higher-level optimization and translation. However, the cost of developing performance-oriented DSLs is a substantial roadblock to their development and adoption. In this article, we present an overview of the Delite compiler framework and the DSLs that have been developed with it. Delite simplifies the process of DSL development by providing common components, like parallel patterns, optimizations, and code generators, that can be reused in DSL implementations. Delite DSLs are embedded in Scala, a general-purpose programming language, but use metaprogramming to construct an Intermediate Representation (IR) of user programs and compile to multiple languages (including C++, CUDA, and OpenCL). DSL programs are automatically parallelized and different parts of the application can run simultaneously on CPUs and GPUs. We present Delite DSLs for machine learning, data querying, graph analysis, and scientific computing and show that they all achieve performance competitive to or exceeding C++ code.},
  number = {4s},
  journal = {ACM Trans. Embed. Comput. Syst.},
  doi = {10.1145/2584665},
  author = {Sujeeth, Arvind K. and Brown, Kevin J. and Lee, Hyoukjoong and Rompf, Tiark and Chafi, Hassan and Odersky, Martin and Olukotun, Kunle},
  month = apr,
  year = {2014},
  keywords = {code generation,language virtualization,Domain-specific languages,multistage programming},
  pages = {134:1--134:25}
}

@article{tobin-hochstadtExtensiblePatternMatching2011,
  title = {Extensible {{Pattern Matching}} in an {{Extensible Language}}},
  language = {en},
  author = {{Tobin-Hochstadt}, Sam},
  month = jun,
  year = {2011},
  keywords = {Computer Science - Programming Languages}
}

@inproceedings{tobin-hochstadtLanguagesLibraries2011,
  address = {{New York, NY, USA}},
  series = {{{PLDI}} '11},
  title = {Languages {{As Libraries}}},
  isbn = {978-1-4503-0663-8},
  abstract = {Programming language design benefits from constructs for extending the syntax and semantics of a host language. While C's string-based macros empower programmers to introduce notational shorthands, the parser-level macros of Lisp encourage experimentation with domain-specific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope.  The design of Racket---a descendant of Scheme---goes even further with the introduction of a full-fledged interface to the static semantics of the language. A Racket extension programmer can thus add constructs that are indistinguishable from "native" notation, large and complex embedded domain-specific languages, and even optimizing transformations for the compiler backend. This power to experiment with language design has been used to create a series of sub-languages for programming with first-class classes and modules, numerous languages for implementing the Racket system, and the creation of a complete and fully integrated typed sister language to Racket's untyped base language. This paper explains Racket's language extension API via an implementation of a small typed sister language. The new language provides a rich type system that accommodates the idioms of untyped Racket. Furthermore, modules in this typed language can safely exchange values with untyped modules. Last but not least, the implementation includes a type-based optimizer that achieves promising speedups. Although these extensions are complex, their Racket implementation is just a library, like any other library, requiring no changes to the Racket implementation.},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/1993498.1993514},
  author = {{Tobin-Hochstadt}, Sam and {St-Amour}, Vincent and Culpepper, Ryan and Flatt, Matthew and Felleisen, Matthias},
  year = {2011},
  keywords = {macros,extensible languages,modules,typed racket},
  pages = {132--141}
}

@inproceedings{vanwykContextawareScanningParsing2007,
  address = {{New York, NY, USA}},
  series = {{{GPCE}} '07},
  title = {Context-Aware {{Scanning}} for {{Parsing Extensible Languages}}},
  isbn = {978-1-59593-855-8},
  abstract = {This paper introduces new parsing and context-aware scanning algorithms in which the scanner uses contextual information to disambiguate lexical syntax. The parser uses a slightly modified LR-style algorithm that passes to the scanner the set of valid symbols that the scanner may return at that point in parsing. This set is those terminals whose entries in the parse table for the current parse state are shift, reduce, or accept, but not error. The scanner then only returns tokens in this set. An analysis is given that can statically verify that the scanner will never return more than one token for a single input. Context-aware scanning is especially useful when parsing and scanning extensible languages in which domain specific languages can be embedded. It has been used in extensible versions of Java 1.4 and ANSI C. We illustrate this approach with a declarative specification of a subset of Java and extensions that embed SQL queries and Boolean expression tables into Java.},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  publisher = {{ACM}},
  doi = {10.1145/1289971.1289983},
  author = {Van Wyk, Eric R. and Schwerdfeger, August C.},
  year = {2007},
  keywords = {extensible languages,context-aware scanning},
  pages = {63--72}
}

@article{vanwykSilverExtensibleAttribute2010,
  series = {Special {{Issue}} on {{ETAPS}} 2006 and 2007 {{Workshops}} on {{Language Descriptions}}, {{Tools}}, and {{Applications}} ({{LDTA}} '06 and '07)},
  title = {Silver: {{An}} Extensible Attribute Grammar System},
  volume = {75},
  issn = {0167-6423},
  shorttitle = {Silver},
  abstract = {Attribute grammar specification languages, like many domain-specific languages, offer significant advantages to their users, such as high-level declarative constructs and domain-specific analyses. Despite these advantages, attribute grammars are often not adopted to the degree that their proponents envision. One practical obstacle to their adoption is a perceived lack of both domain-specific and general purpose language features needed to address the many different aspects of a problem. Here we describe Silver, an extensible attribute grammar specification system, and show how it can be extended with general purpose features such as pattern matching and domain-specific features such as collection attributes and constructs for supporting data-flow analysis of imperative programs. The result is an attribute grammar specification language with a rich set of language features. Silver is implemented in itself by a Silver attribute grammar and utilizes forwarding to implement the extensions in a cost-effective manner.},
  number = {1},
  journal = {Science of Computer Programming},
  doi = {10.1016/j.scico.2009.07.004},
  author = {Van Wyk, Eric and Bodin, Derek and Gao, Jimin and Krishnan, Lijesh},
  month = jan,
  year = {2010},
  keywords = {Extensible languages,Attribute grammars,Extensible compilers,Forwarding,Silver attribute grammar system},
  pages = {39-54}
}

@inproceedings{wanFunctionalReactiveProgramming2000,
  address = {{New York, NY, USA}},
  series = {{{PLDI}} '00},
  title = {Functional {{Reactive Programming}} from {{First Principles}}},
  isbn = {978-1-58113-199-4},
  abstract = {Functional Reactive Programming, or FRP, is a general framework for programming hybrid systems in a high-level, declarative manner. The key ideas in FRP are its notions of behaviors and events. Behaviors are time-varying, reactive values, while events are time-ordered sequences of discrete-time event occurrences. FRP is the essence of Fran, a domain-specific language embedded in Haskell for programming reactive animations, but FRP is now also being used in vision, robotics and other control systems applications. 
In this paper we explore the formal semantics of FRP and how it
relates to an implementation based on streams that represent (and therefore only approximate) continuous behaviors. We show that, in the limit as the sampling interval goes to zero, the implementation is faithful to the formal, continuous semantics, but only when certain constraints on behaviors are observed. We explore the nature of these constraints, which vary amongst the FRP primitives. Our results show both the power and limitations of this approach to language design and implementation. As an example of a limitation, we show that streams are incapable of representing instantaneous predicate events over behaviors.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2000 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/349299.349331},
  author = {Wan, Zhanyong and Hudak, Paul},
  year = {2000},
  pages = {242--252}
}

@article{kaminskiReliableAutomaticComposition2017,
  title = {Reliable and {{Automatic Composition}} of {{Language Extensions}} to {{C}}: {{The ableC Extensible Language Framework}}},
  volume = {1},
  issn = {2475-1421},
  shorttitle = {Reliable and {{Automatic Composition}} of {{Language Extensions}} to {{C}}},
  abstract = {This paper describes an extensible language framework, ableC, that allows programmers to import new, domain-specific, independently-developed language features into their programming language, in this case C. Most importantly, this framework ensures that the language extensions will automatically compose to form a working translator that does not terminate abnormally. This is possible due to two modular analyses that extension developers can apply to their language extension to check its composability. Specifically, these ensure that the composed concrete syntax specification is non-ambiguous and the composed attribute grammar specifying the semantics is well-defined. This assurance and the expressiveness of the supported extensions is a distinguishing characteristic of the approach.   The paper describes a number of techniques for specifying a host language, in this case C at the C11 standard, to make it more amenable to language extension. These include techniques that make additional extensions pass these modular analyses, refactorings of the host language to support a wider range of extensions, and the addition of semantic extension points to support, for example, operator overloading and non-local code transformations.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  doi = {10.1145/3138224},
  author = {Kaminski, Ted and Kramer, Lucas and Carlson, Travis and Van Wyk, Eric},
  month = oct,
  year = {2017},
  keywords = {context-aware scanning,attribute grammars,domain specific languages,extensible compiler frameworks,language composition},
  pages = {98:1--98:29}
}

@inproceedings{farrowComposableAttributeGrammars1992,
  address = {{New York, NY, USA}},
  series = {{{POPL}} '92},
  title = {Composable {{Attribute Grammars}}: {{Support}} for {{Modularity}} in {{Translator Design}} and {{Implementation}}},
  isbn = {978-0-89791-453-6},
  shorttitle = {Composable {{Attribute Grammars}}},
  abstract = {This paper introduces Composable Attribute Grammars (CAGs), a formalism that extends classical attribute grammars to allow for the modular composition of translation specifications and of translators. CAGs bring to complex translator writing systems the same benefits of modularity found in modern programming languages, including comprehensibility, reusability, and incremental meta-compilation.
A CAG is built from several smaller component AGs, each of which solves a particular subproblem, such as name analysis or register allocation. A component AG is based upon a simplified phrase-structure that reflects the properties of its subproblem rather than the phrase-structure of the source language. Different component phrase-structures for various subproblems are combined by mapping them into a phrase-structure for the source language. Both input and output attributes can be associated with the terminal symbols of a component AG. Output attributes enable the results of solving a subproblem to be distributed back to anywhere that originally contributed part of the subproblem, e.g. transparently distributing the results of global name analysis back to every symbolic reference in the source program.
After introducing CAGs by way of an example, we provide a formal definition of CAGs and their semantics. We describe a subclass of CAGs and their semantics. We describe a subclass of CAGs, called separable CAGs, that have favorable implementation properties. We discuss the novel aspects of CAGs, compare them to other proposals for inserting modularity into attribute grammars, and relate our experience using CAGs in the Linguist translator-writing system.},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/143165.143210},
  author = {Farrow, R. and Marlowe, T. J. and Yellin, D. M.},
  year = {1992},
  pages = {223--234}
}

@inproceedings{omarSafelyComposableTypeSpecific2014,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Safely {{Composable Type}}-{{Specific Languages}}},
  isbn = {978-3-662-44202-9},
  abstract = {Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.},
  language = {en},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Omar, Cyrus and Kurilova, Darya and Nistor, Ligia and Chung, Benjamin and Potanin, Alex and Aldrich, Jonathan},
  editor = {Jones, Richard},
  year = {2014},
  keywords = {parsing,hygiene,extensible languages,bidirectional typechecking},
  pages = {105-130}
}

@techreport{flattReferenceRacket2010,
  title = {Reference: {{Racket}}},
  number = {PLT-TR-2010-1},
  institution = {{PLT Design Inc.}},
  author = {Flatt, Matthew and {PLT}},
  year = {2010}
}

@article{hermanTheoryTypedHygienic2010,
  title = {A {{Theory}} of {{Typed Hygienic Macros}}},
  volume = {4960},
  issn = {03029743},
  abstract = {We present the {$\lambda$}m-calculus, a semantics for a language of hygienic macros with a non-trivial theory. Unlike Scheme, where programs must be macro- expanded to be analyzed, our semantics admits reasoning about programs as they appear to programmers. Our contributions include a semantics of hygienic macro expansion, a formal definition of {$\alpha$}-equivalence that is independent of expansion, and a proof that expansion preserves {$\alpha$}-equivalence. The key technical component of our language is a type system similar to Culpepper and Felleisens shape types, but with the novel contribution of binding signature types, which specify the bindings and scope of a macros arguments.},
  journal = {Proceedings of the 17th European Symposium on Programming},
  doi = {10.1007/978-3-540-78739-6_4},
  author = {Herman, David and Wand, Mitchell},
  year = {2010},
  pages = {48}
}

@inproceedings{stansiferRomeo2014,
  address = {{New York, New York, USA}},
  title = {Romeo},
  volume = {49},
  isbn = {978-1-4503-2873-9},
  abstract = {Current languages for safely manipulating values with names only support term languages with simple binding syntax. As a result, no tools exist to safely manipulate code written in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects {$\alpha$}-equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complex-binding support of David Herman's {$\lambda$}m, but is a full-fledged binding-safe language like Pure FreshML.},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming - {{ICFP}} '14},
  publisher = {{ACM Press}},
  doi = {10.1145/2628136.2628162},
  author = {Stansifer, Paul and Wand, Mitchell},
  year = {2014},
  pages = {53--65}
}

@article{augustssonParadiseTwostageDSL2008a,
  title = {Paradise: {{A Two}}-Stage {{DSL Embedded}} in {{Haskell}}},
  volume = {43},
  issn = {0362-1340},
  number = {9},
  journal = {SIGPLAN Not.},
  doi = {10.1145/1411203.1411236},
  author = {Augustsson, Lennart and Mansell, Howard and Sittampalam, Ganesh},
  month = sep,
  year = {2008},
  keywords = {dsels,Haskell,metaprogramming,paradise},
  pages = {225--228}
}

@article{veldhuizenUsingTemplateMetaprograms1995a,
  title = {Using {{C}}++ Template Metaprograms},
  volume = {7},
  number = {4},
  journal = {C++ Report},
  author = {Veldhuizen, Todd},
  year = {1995},
  pages = {36--43}
}

@inproceedings{brabrandTypedUnambiguousPattern2010,
  address = {{New York, NY, USA}},
  series = {{{PPDP}} '10},
  title = {Typed and {{Unambiguous Pattern Matching}} on {{Strings Using Regular Expressions}}},
  isbn = {978-1-4503-0132-9},
  abstract = {We show how to achieve typed and unambiguous declarative pattern matching on strings using regular expressions extended with a simple recording operator. We give a characterization of ambiguity of regular expressions that leads to a sound and complete static analysis. The analysis is capable of pinpointing all ambiguities in terms of the structure of the regular expression and report shortest ambiguous strings. We also show how pattern matching can be integrated into statically typed programming languages for deconstructing strings and reproducing typed and structured values. We validate our approach by giving a full implementation of the approach presented in this paper. The resulting tool, reg-exp-rec, adds typed and unambiguous pattern matching to Java in a standalone and non-intrusive manner. We evaluate the approach using several realistic examples.},
  booktitle = {Proceedings of the 12th {{International ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Declarative Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/1836089.1836120},
  author = {Brabrand, Claus and Thomsen, Jakob G.},
  year = {2010},
  keywords = {parsing,ambiguity,disambiguation,pattern matching,regular expressions,static analysis,type inference},
  pages = {243--254}
}

@inproceedings{palmkvistCreatingDomainSpecificLanguages2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Creating {{Domain}}-{{Specific Languages}} by {{Composing Syntactical Constructs}}},
  isbn = {978-3-030-05998-9},
  abstract = {Creating a programming language is a considerable undertaking, even for relatively small domain-specific languages (DSLs). Most approaches to ease this task either limit the flexibility of the DSL or consider entire languages as the unit of composition. This paper presents a new approach using syntactical constructs (also called syncons) for defining DSLs in much smaller units of composition while retaining flexibility. A syntactical construct defines a single language feature, such as an if statement or an anonymous function. Each syntactical construct is fully self-contained: it specifies its own concrete syntax, binding semantics, and runtime semantics, independently of the rest of the language. The runtime semantics are specified as a translation to a user defined target language, while the binding semantics allow name resolution before expansion. Additionally, we present a novel approach for dealing with syntactical ambiguity that arises when combining languages, even if the languages are individually unambiguous. The work is implemented and evaluated in a case study, where small subsets of OCaml and Lua have been defined and composed using syntactical constructs.},
  language = {en},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  publisher = {{Springer International Publishing}},
  author = {Palmkvist, Viktor and Broman, David},
  editor = {Alferes, Jos{\'e} J{\'u}lio and Johansson, Moa},
  year = {2019},
  pages = {187-203}
}

@inproceedings{alurVisiblyPushdownLanguages2004,
  address = {{New York, NY, USA}},
  series = {{{STOC}} '04},
  title = {Visibly {{Pushdown Languages}}},
  isbn = {978-1-58113-852-8},
  abstract = {We propose the class of visibly pushdown languages as embeddings of context-free languages that is rich enough to model program analysis questions and yet is tractable and robust like the class of regular languages. In our definition, the input symbol determines when the pushdown automaton can push or pop, and thus the stack depth at every position. We show that the resulting class Vpl of languages is closed under union, intersection, complementation, renaming, concatenation, and Kleene-*, and problems such as inclusion that are undecidable for context-free languages are Exptime-complete for visibly pushdown automata. Our framework explains, unifies, and generalizes many of the decision procedures in the program analysis literature, and allows algorithmic verification of recursive programs with respect to many context-free properties including access control properties via stack inspection and correctness of procedures with respect to pre and post conditions. We demonstrate that the class Vpl is robust by giving two alternative characterizations: a logical characterization using the monadic second order (MSO) theory over words augmented with a binary matching predicate, and a correspondence to regular tree languages. We also consider visibly pushdown languages of infinite words and show that the closure properties, MSO-characterization and the characterization in terms of regular trees carry over. The main difference with respect to the case of finite words turns out to be determinizability: nondeterministic B{\"u}chi visibly pushdown automata are strictly more expressive than deterministic Muller visibly pushdown automata.},
  booktitle = {Proceedings of the {{Thirty}}-Sixth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/1007352.1007390},
  author = {Alur, Rajeev and Madhusudan, P.},
  year = {2004},
  keywords = {$ømega$-languages,context-free languages,logic,pushdown automata,regular tree languages,verification},
  pages = {202--211}
}

@article{vantangTighterBoundDeterminization2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0911.3275},
  title = {A {{Tighter Bound}} for the {{Determinization}} of {{Visibly Pushdown Automata}}},
  volume = {10},
  issn = {2075-2180},
  abstract = {Visibly pushdown automata (VPA), introduced by Alur and Madhusuan in 2004, is a subclass of pushdown automata whose stack behavior is completely determined by the input symbol according to a fixed partition of the input alphabet. Since its introduce, VPAs have been shown to be useful in various context, e.g., as specification formalism for verification and as automaton model for processing XML streams. Due to high complexity, however, implementation of formal verification based on VPA framework is a challenge. In this paper we consider the problem of implementing VPA-based model checking algorithms. For doing so, we first present an improvement on upper bound for determinization of VPA. Next, we propose simple on-the-fly algorithms to check universality and inclusion problems of this automata class. Then, we implement the proposed algorithms in a prototype tool. Finally, we conduct experiments on randomly generated VPAs. The experimental results show that the proposed algorithms are considerably faster than the standard ones.},
  journal = {Electronic Proceedings in Theoretical Computer Science},
  doi = {10.4204/EPTCS.10.5},
  author = {Van Tang, Nguyen},
  month = nov,
  year = {2009},
  keywords = {Computer Science - Formal Languages and Automata Theory,Computer Science - Logic in Computer Science,F.4.1,F.4.3},
  pages = {62-76}
}

@inproceedings{afroozehSafeSpecificationOperator2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Safe {{Specification}} of {{Operator Precedence Rules}}},
  isbn = {978-3-319-02654-1},
  abstract = {In this paper we present an approach to specifying operator precedence based on declarative disambiguation constructs and an implementation mechanism based on grammar rewriting. We identify a problem with existing generalized context-free parsing and disambiguation technology: generating a correct parser for a language such as OCaml using declarative precedence specification is not possible without resorting to some manual grammar transformation. Our approach provides a fully declarative solution to operator precedence specification for context-free grammars, is independent of any parsing technology, and is safe in that it guarantees that the language of the resulting grammar will be the same as the language of the specification grammar. We evaluate our new approach by specifying the precedence rules from the OCaml reference manual against the highly ambiguous reference grammar and validate the output of our generated parser.},
  language = {en},
  booktitle = {Software {{Language Engineering}}},
  publisher = {{Springer International Publishing}},
  author = {Afroozeh, Ali and {van den Brand}, Mark and Johnstone, Adrian and Scott, Elizabeth and Vinju, Jurgen},
  editor = {Erwig, Martin and Paige, Richard F. and Van Wyk, Eric},
  year = {2013},
  keywords = {Parse Tree,Derivation Tree,Operator Precedence,Precedence Rule,Production Rule},
  pages = {137-156}
}

@book{sudkampLanguagesMachinesIntroduction1997,
  address = {{Boston, MA, USA}},
  title = {Languages and {{Machines}}: {{An Introduction}} to the {{Theory}} of {{Computer Science}}},
  isbn = {978-0-201-82136-9},
  shorttitle = {Languages and {{Machines}}},
  publisher = {{Addison-Wesley Longman Publishing Co., Inc.}},
  author = {Sudkamp, Thomas A.},
  year = {1997}
}

@inproceedings{danielssonParsingMixfixOperators2011,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Parsing {{Mixfix Operators}}},
  isbn = {978-3-642-24452-0},
  abstract = {A simple grammar scheme for expressions containing mixfix operators is presented. The scheme is parameterised by a precedence relation which is only restricted to be a directed acyclic graph; this makes it possible to build up precedence relations in a modular way. Efficient and simple implementations of parsers for languages with user-defined mixfix operators, based on the grammar scheme, are also discussed. In the future we plan to replace the support for mixfix operators in the language Agda with a grammar scheme and an implementation based on this work.},
  language = {en},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Danielsson, Nils Anders and Norell, Ulf},
  editor = {Scholz, Sven-Bodo and Chitil, Olaf},
  year = {2011},
  pages = {80-99}
}

@book{ahoCompilersPrinciplesTechniques2006,
  address = {{Boston}},
  edition = {2nd},
  title = {Compilers: {{Principles}}, {{Techniques}}, and {{Tools}}},
  isbn = {978-0-321-48681-3},
  shorttitle = {Compilers},
  abstract = {Compilers: Principles, Techniques and Tools, known to professors, students, and developers worldwide as the "Dragon Book," is available in a new edition.~ Every chapter has been completely revised to reflect developments in software engineering, programming languages, and computer architecture that have occurred since 1986, when the last edition published.~ The authors, recognizing that few readers will ever go on to construct a compiler, retain their focus on the broader set of problems faced in software design and software development.},
  language = {English},
  publisher = {{Addison Wesley}},
  author = {Aho, Alfred V. and Lam, Monica S. and Sethi, Ravi and Ullman, Jeffrey D.},
  month = sep,
  year = {2006}
}

@book{webberModernProgrammingLanguages2003,
  title = {Modern {{Programming Languages}}: {{A Practical Introduction}}},
  isbn = {978-1-887902-76-2},
  shorttitle = {Modern {{Programming Languages}}},
  abstract = {Typical undergraduate CS/CE majors have a practical orientation: they study computing because they like programming and are good at it. This book has strong appeal to this core student group. There is more than enough material for a semester-long course. The challenge for a course in programming language concepts is to help practical ......},
  language = {en},
  publisher = {{Franklin, Beedle \& Associates}},
  author = {Webber, Adam Brooks},
  year = {2003},
  keywords = {Computers / General}
}

@book{cooperEngineeringCompiler2011,
  edition = {2nd},
  title = {Engineering a {{Compiler}}},
  isbn = {978-0-08-091661-3},
  abstract = {This entirely revised second edition of Engineering a Compiler is full of technical updates and new material covering the latest developments in compiler technology. In this comprehensive text you will learn important techniques for constructing a modern compiler. Leading educators and researchers Keith Cooper and Linda Torczon combine basic principles with pragmatic insights from their experience building state-of-the-art compilers. They will help you fully understand important techniques such as compilation of imperative and object-oriented languages, construction of static single assignment forms, instruction scheduling, and graph-coloring register allocation.In-depth treatment of algorithms and techniques used in the front end of a modern compilerFocus on code optimization and code generation, the primary areas of recent research and developmentImprovements in presentation including conceptual overviews for each chapter, summaries and review questions for sections, and prominent placement of definitions for new termsExamples drawn from several different programming languages},
  language = {en},
  publisher = {{Elsevier}},
  author = {Cooper, Keith and Torczon, Linda},
  month = jan,
  year = {2011},
  keywords = {Computers / Computer Engineering,Computers / Programming Languages / General}
}

@article{scottGLLParsing2010,
  series = {Proceedings of the {{Ninth Workshop}} on {{Language Descriptions Tools}} and {{Applications}} ({{LDTA}} 2009)},
  title = {{{GLL Parsing}}},
  volume = {253},
  issn = {1571-0661},
  abstract = {Recursive Descent (RD) parsers are popular because their control flow follows the structure of the grammar and hence they are easy to write and to debug. However, the class of grammars which admit RD parsers is very limited. Backtracking techniques may be used to extend this class, but can have explosive runtimes and cannot deal with grammars with left recursion. Tomita-style RNGLR parsers are fully general but are based on LR techniques and do not have the direct relationship with the grammar that an RD parser has. We develop the fully general GLL parsing technique which is recursive descent-like, and has the property that the parse follows closely the structure of the grammar rules, but uses RNGLR-like machinery to handle non-determinism. The resulting recognisers run in worst-case cubic time and can be built even for left recursive grammars.},
  number = {7},
  journal = {Electronic Notes in Theoretical Computer Science},
  doi = {10.1016/j.entcs.2010.08.041},
  author = {Scott, Elizabeth and Johnstone, Adrian},
  month = sep,
  year = {2010},
  keywords = {context free languages,generalised parsing,recursive descent,RNGLR and RIGLR parsing},
  pages = {177-189}
}

@inproceedings{langDeterministicTechniquesEfficient1974,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Deterministic {{Techniques}} for {{Efficient Non}}-{{Deterministic Parsers}}},
  isbn = {978-3-662-21545-6},
  abstract = {A general study of parallel non-deterministic parsing and translation {\`a} la Earley is developped formally, based on non-deterministic pushdown acceptor-transducers. Several results (camplexity and efficiency) are established, same new and other previously proved only in special cases. As an application, we show that for every family of deterministic context-free pushdown parsers (e.g. precedence, LR(k), LL(k), ...) there is a family of general context-free parallel parsers that have the same efficiency in most practical cases (e.g. analysis of programming languages).},
  language = {en},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Lang, Bernard},
  editor = {Loeckx, Jacques},
  year = {1974},
  pages = {255-269}
}

@article{youngerRecognitionParsingContextFree1967,
  title = {Recognition and Parsing of Context-Free Languages in Time $n^3$},
  volume = {10},
  issn = {0019-9958},
  abstract = {A recognition algorithm is exhibited whereby an arbitrary string over a given vocabulary can be tested for containment in a given context-free language. A special merit of this algorithm is that it is completed in a number of steps proportional to the “cube” of the number of symbols in the tested string. As a byproduct of the grammatical analysis, required by the recognition algorithm, one can obtain, by some additional processing not exceeding the “cube” factor of computational complexity, a parsing matrix—a complete summary of the grammatical structure of the sentence. It is also shown how, by means of a minor modification of the recognition algorithm, one can obtain an integer representing the ambiguity of the sentence, i.e., the number of distinct ways in which that sentence can be generated by the grammar. The recognition algorithm is then simulated on a Turing Machine. It is shown that this simulation likewise requires a number of steps proportional to only the “cube” of the test string length.},
  number = {2},
  journal = {Information and Control},
  doi = {10.1016/S0019-9958(67)80007-X},
  author = {Younger, Daniel H.},
  month = feb,
  year = {1967},
  pages = {189-208}
}

@phdthesis{bastenAmbiguityDetectionProgramming2011,
  title = {Ambiguity {{Detection}} for {{Programming Language Grammars}}},
  abstract = {Context-free grammars are the most suitable and most widely used method for describing the syntax of programming languages. They can be used to generate parsers, which transform a piece of source code into a tree-shaped representation of the code's syntactic structure. These parse trees can then be used for further processing or analysis of the source text. In this sense, grammars form the basis of many engineering and reverse engineering applications, like compilers, interpreters and tools for software analysis and transformation. Unfortunately, context-free grammars have the undesirable property that they can be ambiguous, which can seriously hamper their applicability. A grammar is ambiguous if at least one sentence in its language has more than one valid parse tree. Since the parse tree of a sentence is often used to infer its semantics, an ambiguous sentence can have multiple meanings. For programming languages this is almost always unintended. Ambiguity can therefore be seen as a grammar bug. A specific category of context-free grammars that is particularly sensitive to ambiguity are character-level grammars, which are used to generate scannerless parsers. Unlike traditional token-based grammars, character-level grammars include the full lexical definition of their language. This has the advantage that a language can be specified in a single formalism, and that no separate lexer or scanner phase is necessary in the parser. However, the absence of a scanner does require some additional lexical disambiguation. Character-level grammars can therefore be annotated with special disambiguation declarations to specify which parse trees to discard in case of ambiguity. Unfortunately, it is very hard to determine whether all ambiguities have been covered. The task of searching for ambiguities in a grammar is very complex and time consuming, and is therefore best automated. Since the invention of context-free grammars, several ambiguity detection methods have been developed to this end. However, the ambiguity problem for context-free grammars is undecidable in general, so the perfect detection method cannot exist. This implies a trade-off between accuracy and termination. Methods that apply exhaustive searching are able to correctly find all ambiguities, but they might never terminate. On the other hand, approximative search techniques do always produce an ambiguity report, but these might contain false positives or false negatives. Nevertheless, the fact that every method has flaws does not mean that ambiguity detection cannot be useful in practice. This thesis investigates ambiguity detection with the aim of checking grammars for programming languages. The challenge is to improve upon the state-of-the-art, by finding accurate enough methods that scale to realistic grammars. First we evaluate existing methods with a set of criteria for practical usability. Then we present various improvements to ambiguity detection in the areas of accuracy, performance and report quality. The main contributions of this thesis are two novel techniques. The first is an ambi- guity detection method that applies both exhaustive and approximative searching, called AMBIDEXTER. The key ingredient of AMBIDEXTER is a grammar filtering technique that can remove harmless production rules from a grammar. A production rule is harmless if it does not contribute to any ambiguity in the grammar. Any found harmless rules can therefore safely be removed. This results in a smaller grammar that still contains the same ambiguities as the original one. However, it can now be searched with exhaustive techniques in less time. The grammar filtering technique is formally proven correct, and experimentally validated. A prototype implementation is applied to a series of programming language grammars, and the performance of exhaustive detection methods are measured before and after filtering. The results show that a small investment in filtering time can substantially reduce the run-time of exhaustive searching, sometimes with several orders of magnitude. After this evaluation on token-based grammars, the grammar filtering technique is extended for use with character-level grammars. The extensions deal with the increased complexity of these grammars, as well as their disambiguation declarations. This enables the detection of productions that are harmless due to disambiguation. The extentions are experimentally validated on another set of programming language grammars from practice, with similar results as before. Measurements show that, even though character-level grammars are more expensive to filter, the investment is still very worthwhile. Exhaustive search times were again reduced substantially. The second main contribution of this thesis is DR. AMBIGUITY, an expert system to help grammar developers to understand and solve found ambiguities. If applied to an ambiguous sentence, DR. AMBIGUITY analyzes the causes of the ambiguity and proposes a number of applicable solutions. A prototype implementation is presented and evaluated with a mature Java grammar. After removing disambiguation declarations from the grammar we analyze sentences that have become ambiguous by this removal. The results show that in all cases the removed filter is proposed by DR. AMBIGUITY as a possible cure for the ambiguity. Concluding, this thesis improves ambiguity detection with two novel methods. The first is the ambiguity detection method AMBIDEXTER that applies grammar filtering to substantially speed up exhaustive searching. The second is the expert system DR. AMBIGUITY that automatically analyzes found ambiguities and proposes applicable cures. The results obtained with both methods show that automatic ambiguity detection is now ready for realistic programming language grammars.},
  school = {Universiteit van Amsterdam},
  author = {Basten, Bas},
  month = dec,
  year = {2011},
  keywords = {static analysis,ambiguity detection,context-free grammars,programming languages,scannerless},
  hal_id = {tel-00644079},
  hal_version = {v1}
}

@inproceedings{kaminskiModularWellDefinednessAnalysis2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Modular {{Well}}-{{Definedness Analysis}} for {{Attribute Grammars}}},
  isbn = {978-3-642-36089-3},
  abstract = {We present a modular well-definedness analysis for attribute grammars. The global properties of completeness and non-circularity are ensured with checks on grammar modules that require only additional information from their dependencies. Local checks to ensure global properties are crucial for specifying extensible languages. They allow independent developers of language extensions to verify that their extension, when combined with other independently developed and similarly verified extensions to a specified host language, will result in a composed grammar that is well-defined. Thus, the composition of the host language and user-selected extensions can safely be performed by someone with no expertise in language design and implementation. The analysis is necessarily conservative and imposes some restrictions on the grammar. We argue that the analysis is practical and the restrictions are natural and not burdensome by applying it to the Silver specifications of Silver, our boot-strapped extensible attribute grammar system.},
  language = {en},
  booktitle = {Software {{Language Engineering}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Kaminski, Ted and Van Wyk, Eric},
  editor = {Czarnecki, Krzysztof and Hedin, G{\"o}rel},
  year = {2013},
  keywords = {Concrete Syntax,Attribute Equation,Attribute Grammar,Language Extension,Modular Analysis},
  pages = {352-371}
}

@book{comonTreeAutomataTechniques2007,
  title = {Tree {{Automata Techniques}} and {{Applications}}},
  author = {Comon, H. and Dauchet, M. and Gilleron, R. and L{\"o}ding, C. and Jacquemard, F. and Lugiez, D. and Tison, S. and Tommasi, M.},
  year = {2007},
  howpublished = {Available on: http://www.grappa.univ-lille3.fr/tata},
  note = {release October, 12th 2007}
}

@inproceedings{doczkalConstructiveTheoryRegular2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Constructive Theory}} of {{Regular Languages}} in {{Coq}}},
  isbn = {978-3-319-03545-1},
  abstract = {We present a formal constructive theory of regular languages consisting of about 1400 lines of Coq/Ssreflect. As representations we consider regular expressions, deterministic and nondeterministic automata, and Myhill and Nerode partitions. We construct computable functions translating between these representations and show that equivalence of representations is decidable. We also establish the usual closure properties, give a minimization algorithm for DFAs, and prove that minimal DFAs are unique up to state renaming. Our development profits much from Ssreflect's support for finite types and graphs.},
  language = {en},
  booktitle = {Certified {{Programs}} and {{Proofs}}},
  publisher = {{Springer International Publishing}},
  author = {Doczkal, Christian and Kaiser, Jan-Oliver and Smolka, Gert},
  editor = {Gonthier, Georges and Norrish, Michael},
  year = {2013},
  keywords = {Coq,finite automata,Myhill-Nerode,regular expressions,regular languages,Ssreflect},
  pages = {82-97}
}

@article{caralpTrimmingVisiblyPushdown2015,
  series = {Implementation and {{Application}} of {{Automata}}},
  title = {Trimming Visibly Pushdown Automata},
  volume = {578},
  issn = {0304-3975},
  abstract = {We study the problem of trimming visibly pushdown automata (VPA). We first describe a polynomial time procedure which, given a visibly pushdown automaton that accepts only well-nested words, returns an equivalent visibly pushdown automaton that is trimmed. We then show how this procedure can be lifted to the setting of arbitrary VPA. Furthermore, we present a way of building, given a VPA, an equivalent VPA which is both deterministic and trimmed. Last, our trimming procedures can be applied to weighted VPA.},
  language = {en},
  journal = {Theoretical Computer Science},
  doi = {10.1016/j.tcs.2015.01.018},
  author = {Caralp, Mathieu and Reynier, Pierre-Alain and Talbot, Jean-Marc},
  month = may,
  year = {2015},
  keywords = {Polynomial Time,Polynomial Time Complexity,Pushdown Automaton,Return Transition,Tree Automaton,Trimming,Visibly pushdown automata},
  pages = {13-29}
}

@inproceedings{giorgidzeEmbeddingFunctionalHybrid2011a,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Embedding a {{Functional Hybrid Modelling Language}} in {{Haskell}}},
  isbn = {978-3-642-24452-0},
  abstract = {In this paper we present the first investigation into the implementation of a Functional Hybrid Modelling language for non-causal modelling and simulation of physical systems. In particular, we present a simple way to handle connect constructs: a facility for composing model fragments present in some form in most non-causal modelling languages. Our implementation is realised as a domain-specific language embedded in Haskell. The method of embedding employs quasiquoting, thus demonstrating the effectiveness of this approach for languages that are not suitable for embedding in more traditional ways. Our implementation is available on-line, and thus the first publicly available prototype implementation of a Functional Hybrid Modelling language.},
  language = {en},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Giorgidze, George and Nilsson, Henrik},
  editor = {Scholz, Sven-Bodo and Chitil, Olaf},
  year = {2011},
  keywords = {Abstract Syntax,Functional Programming,Modelling Language,Signal Function,Signal Relation},
  pages = {138-155}
}

@inproceedings{schwerdfegerVerifiableCompositionDeterministic2009,
  address = {{New York, NY, USA}},
  series = {{{PLDI}} '09},
  title = {Verifiable {{Composition}} of {{Deterministic Grammars}}},
  isbn = {978-1-60558-392-1},
  abstract = {There is an increasing interest in extensible languages, (domain-specific) language extensions, and mechanisms for their specification and implementation. One challenge is to develop tools that allow non-expert programmers to add an eclectic set of language extensions to a host language. We describe mechanisms for composing and analyzing concrete syntax specifications of a host language and extensions to it. These specifications consist of context-free grammars with each terminal symbol mapped to a regular expression, from which a slightly-modified LR parser and context-aware scanner are generated. Traditionally, conflicts are detected when a parser is generated from the composed grammar, but this comes too late since it is the non-expert programmer directing the composition of independently developed extensions with the host language. The primary contribution of this paper is a modular analysis that is performed independently by each extension designer on her extension (composed alone with the host language). If each extension passes this modular analysis, then the language composed later by the programmer will compile with no conflicts or lexical ambiguities. Thus, extension writers can verify that their extension will safely compose with others and, if not, fix the specification so that it will. This is possible due to the context-aware scanner's lexical disambiguation and a set of reasonable restrictions limiting the constructs that can be introduced by an extension. The restrictions ensure that the parse table states can be partitioned so that each state can be attributed to the host language or a single extension.},
  booktitle = {Proceedings of the 30th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/1542476.1542499},
  author = {Schwerdfeger, August C. and Van Wyk, Eric R.},
  year = {2009},
  keywords = {context-aware scanning,extensible languages,grammar composition,language composition,lr parsing},
  pages = {199--210}
}

@inproceedings{katsPureDeclarativeSyntax2010,
  address = {{New York, NY, USA}},
  series = {{{OOPSLA}} '10},
  title = {Pure and {{Declarative Syntax Definition}}: {{Paradise Lost}} and {{Regained}}},
  isbn = {978-1-4503-0203-6},
  shorttitle = {Pure and {{Declarative Syntax Definition}}},
  abstract = {Syntax definitions are pervasive in modern software systems, and serve as the basis for language processing tools like parsers and compilers. Mainstream parser generators pose restrictions on syntax definitions that follow from their implementation algorithm. They hamper evolution, maintainability, and compositionality of syntax definitions. The pureness and declarativity of syntax definitions is lost. We analyze how these problems arise for different aspects of syntax definitions, discuss their consequences for language engineers, and show how the pure and declarative nature of syntax definitions can be regained.},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Object Oriented Programming Systems Languages}} and {{Applications}}},
  publisher = {{ACM}},
  doi = {10.1145/1869459.1869535},
  author = {Kats, Lennart C.L. and Visser, Eelco and Wachsmuth, Guido},
  year = {2010},
  keywords = {declarative,grammars,grammarware,parsers,sdf,sglr,syntax definition},
  pages = {918--932}
}


