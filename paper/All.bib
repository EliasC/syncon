
@inproceedings{antwerpenConstraintLanguageStatic2016,
  address = {New York, NY, USA},
  series = {{{PEPM}} '16},
  title = {A {{Constraint Language}} for {{Static Semantic Analysis Based}} on {{Scope Graphs}}},
  isbn = {978-1-4503-4097-7},
  abstract = {In previous work, we introduced scope graphs as a formalism for describing program binding structure and performing name resolution in an AST-independent way. In this paper, we show how to use scope graphs to build static semantic analyzers. We use constraints extracted from the AST to specify facts about binding, typing, and initialization. We treat name and type resolution as separate building blocks, but our approach can handle language constructs---such as record field access---for which binding and typing are mutually dependent. We also refine and extend our previous scope graph theory to address practical concerns including ambiguity checking and support for a wider range of scope relationships. We describe the details of constraint generation for a model language that illustrates many of the interesting static analysis issues associated with modules and records.},
  booktitle = {Proceedings of the 2016 {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  publisher = {{ACM}},
  doi = {10.1145/2847538.2847543},
  author = {van Antwerpen, Hendrik and N\'eron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
  year = {2016},
  keywords = {Domain Specific Languages,Language Specification,Meta-Theory,Name Binding,Types},
  pages = {49--60},
  file = {/home/vipa/Zotero/storage/7LK3QKJ2/Antwerpen et al. - 2016 - A Constraint Language for Static Semantic Analysis.pdf}
}

@inproceedings{augustssonParadiseTwostageDSL2008,
  address = {New York, NY, USA},
  series = {{{ICFP}} '08},
  title = {Paradise: {{A Two}}-Stage {{DSL Embedded}} in {{Haskell}}},
  isbn = {978-1-59593-919-7},
  shorttitle = {Paradise},
  abstract = {We have implemented a two-stage language, Paradise, for building reusable components which are used to price financial products. Paradise is embedded in Haskell and makes heavy use of type-class based overloading, allowing the second stage to be compiled into a variety of backend platforms. Paradise has enabled us to begin moving away from implementation directly in monolithic Excel spreadsheets and towards a more modular and retargetable approach.},
  booktitle = {Proceedings of the 13th {{ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/1411204.1411236},
  author = {Augustsson, Lennart and Mansell, Howard and Sittampalam, Ganesh},
  year = {2008},
  keywords = {dsels,Haskell,metaprogramming,paradise},
  pages = {225--228},
  file = {/home/vipa/Zotero/storage/EPSGJHGT/Augustsson et al. - 2008 - Paradise A Two-stage DSL Embedded in Haskell.pdf}
}

@inproceedings{axelssonAnalyzingContextFreeGrammars2008,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Analyzing {{Context}}-{{Free Grammars Using}} an {{Incremental SAT Solver}}},
  isbn = {978-3-540-70583-3},
  abstract = {We consider bounded versions of undecidable problems about context-free languages which restrict the domain of words to some finite length: inclusion, intersection, universality, equivalence, and ambiguity. These are in (co)-NP and thus solvable by a reduction to the (un-)satisfiability problem for propositional logic. We present such encodings \textendash{} fully utilizing the power of incrementat SAT solvers \textendash{} prove correctness and validate this approach with benchmarks.},
  language = {en},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Axelsson, Roland and Heljanko, Keijo and Lange, Martin},
  editor = {Aceto, Luca and Damg\aa{}rd, Ivan and Goldberg, Leslie Ann and Halld\'orsson, Magn\'us M. and Ing\'olfsd\'ottir, Anna and Walukiewicz, Igor},
  year = {2008},
  keywords = {Ambiguous Word,Conjunctive Normal Form,Parse Tree,Propositional Formula,Word Problem},
  pages = {410-422},
  file = {/home/vipa/Zotero/storage/V9SK6FKI/Axelsson et al. - 2008 - Analyzing Context-Free Grammars Using an Increment.pdf}
}

@article{aycockPracticalEarleyParsing2002,
  title = {Practical {{Earley Parsing}}},
  volume = {45},
  issn = {0010-4620},
  abstract = {Earley's parsing algorithm is a general algorithm, able to handle any context-free grammar. As with most parsing algorithms, however, the presence of grammar rules having empty right-hand sides complicates matters. By analyzing why Earley's algorithm struggles with these grammar rules, we have devised a simple solution to the problem. Our empty-rule solution leads to a new type of finite automaton expressly suited for use in Earley parsers and to a new statement of Earley's algorithm. We show that this new form of Earley parser is much more time efficient in practice than the original.},
  number = {6},
  journal = {The Computer Journal},
  doi = {10.1093/comjnl/45.6.620},
  author = {Aycock, John and Horspool, R. Nigel},
  year = {2002},
  keywords = {Algorithms,Article,Automation,Computing Milieux (General) (Ci),Copyrights,Grammars,Handles,Mathematical Analysis,Mathematical Models,Parsers,Parsing Algorithms},
  pages = {620--630},
  file = {/home/vipa/Zotero/storage/SNR548ZN/Aycock and Horspool - 2002 - Practical Earley Parsing.pdf}
}

@inproceedings{bjesseLavaHardwareDesign1998,
  address = {New York, NY, USA},
  series = {{{ICFP}} '98},
  title = {Lava: {{Hardware Design}} in {{Haskell}}},
  isbn = {978-1-58113-024-9},
  shorttitle = {Lava},
  abstract = {Lava is a tool to assist circuit designers in specifying, designing, verifying and implementing hardware. It is a collection of Haskell modules. The system design exploits functional programming language features, such as monads and type classes, to provide multiple interpretations of circuit descriptions. These interpretations implement standard circuit analyses such as simulation, formal verification and the generation of code for the production of real circuits.Lava also uses polymorphism and higher order functions to provide more abstract and general descriptions than are possible in traditional hardware description languages. Two Fast Fourier Transform circuit examples illustrate this.},
  booktitle = {Proceedings of the {{Third ACM SIGPLAN International Conference}} on {{Functional Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/289423.289440},
  author = {Bjesse, Per and Claessen, Koen and Sheeran, Mary and Singh, Satnam},
  year = {1998},
  pages = {174--184},
  file = {/home/vipa/Zotero/storage/SPFXCFX6/Bjesse et al. - 1998 - Lava Hardware Design in Haskell.pdf}
}

@inproceedings{brabrandAnalyzingAmbiguityContextFree2007,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Analyzing {{Ambiguity}} of {{Context}}-{{Free Grammars}}},
  isbn = {978-3-540-76336-9},
  abstract = {It has been known since 1962 that the ambiguity problem for context-free grammars is undecidable. Ambiguity in context-free grammars is a recurring problem in language design and parser generation, as well as in applications where grammars are used as models of real-world physical structures.We observe that there is a simple linguistic characterization of the grammar ambiguity problem, and we show how to exploit this to conservatively approximate the problem based on local regular approximations and grammar unfoldings. As an application, we consider grammars that occur in RNA analysis in bioinformatics, and we demonstrate that our static analysis of context-free grammars is sufficiently precise and efficient to be practically useful.},
  language = {en},
  booktitle = {Implementation and {{Application}} of {{Automata}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Brabrand, Claus and Giegerich, Robert and M\o{}ller, Anders},
  editor = {Holub, Jan and {\v Z}{\v d}\'arek, Jan},
  year = {2007},
  keywords = {CFG ambiguity,regular approximation,RNA analysis},
  pages = {214-225},
  file = {/home/vipa/Zotero/storage/LTUIAIMJ/Brabrand et al. - 2007 - Analyzing Ambiguity of Context-Free Grammars.pdf}
}

@article{bravenboerStrategoXT172008,
  title = {Stratego/{{XT}} 0.17. {{A Language}} and {{Toolset}} for {{Program Transformation}}},
  volume = {72},
  issn = {0167-6423},
  abstract = {Stratego/XT is a language and toolset for program transformation. The Stratego language provides rewrite rules for expressing basic transformations, programmable rewriting strategies for controlling the application of rules, concrete syntax for expressing the patterns of rules in the syntax of the object language, and dynamic rewrite rules for expressing context-sensitive transformations, thus supporting the development of transformation components at a high level of abstraction. The XT toolset offers a collection of flexible, reusable transformation components, and tools for generating such components from declarative specifications. Complete program transformation systems are composed from these components. This paper gives an overview of Stratego/XT 0.17, including a description of the Stratego language and XT transformation tools; a discussion of the implementation techniques and software engineering process; and a description of applications built with Stratego/XT.},
  number = {1-2},
  journal = {Sci. Comput. Program.},
  doi = {10.1016/j.scico.2007.11.003},
  author = {Bravenboer, Martin and Kalleberg, Karl Trygve and Vermaas, Rob and Visser, Eelco},
  month = jun,
  year = {2008},
  keywords = {Concrete syntax,Dynamic rewrite rules,Program transformation,Rewrite rules,Rewriting strategies,Stratego,Stratego/XT},
  pages = {52--70},
  file = {/home/vipa/Zotero/storage/UQGBYURH/Bravenboer et al. - 2008 - StrategoXT 0.17. A Language and Toolset for Progr.pdf}
}

@techreport{bromanModelyzeGraduallyTyped2012,
  title = {Modelyze: A {{Gradually Typed Host Language}} for {{Embedding Equation}}-{{Based Modeling Languages}}},
  number = {UCB/EECS-2012-173},
  institution = {{EECS Department, University of California, Berkeley}},
  author = {Broman, David and Siek, Jeremy G.},
  month = jun,
  year = {2012},
  file = {/home/vipa/Zotero/storage/75BXDBYN/Broman and Siek - Modelyze a Gradually Typed Host Language for Embe.pdf}
}

@inproceedings{bromanGraduallyTypedSymbolic2018,
  address = {New York, NY, USA},
  series = {{{PEPM}} '18},
  title = {Gradually {{Typed Symbolic Expressions}}},
  isbn = {978-1-4503-5587-2},
  abstract = {Embedding a domain-specific language (DSL) in a general purpose host language is an efficient way to develop a new DSL. Various kinds of languages and paradigms can be used as host languages, including object-oriented, functional, statically typed, and dynamically typed variants, all having their pros and cons. For deep embedding, statically typed languages enable early checking and potentially good DSL error messages, instead of reporting runtime errors. Dynamically typed languages, on the other hand, enable flexible transformations, thus avoiding extensive boilerplate code. In this paper, we introduce the concept of gradually typed symbolic expressions that mix static and dynamic typing for symbolic data. The key idea is to combine the strengths of dynamic and static typing in the context of deep embedding of DSLs. We define a gradually typed calculus {$\lambda{}<\star{}>$}, formalize its type system and dynamic semantics, and prove type safety. We introduce a host language called Modelyze that is based on {$\lambda{}<\star{}>$}, and evaluate the approach by embedding a series of equation-based domain-specific modeling languages, all within the domain of physical modeling and simulation.},
  booktitle = {Proceedings of the {{ACM SIGPLAN Workshop}} on {{Partial Evaluation}} and {{Program Manipulation}}},
  publisher = {{ACM}},
  doi = {10.1145/3162068},
  author = {Broman, David and Siek, Jeremy G.},
  year = {2018},
  keywords = {DSL,Symbolic expressions,Type systems},
  pages = {15--29},
  file = {/home/vipa/Zotero/storage/5WK3GCIK/Broman and Siek - 2018 - Gradually Typed Symbolic Expressions.pdf}
}

@article{cantorAmbiguityProblemBackus1962,
  title = {On {{The Ambiguity Problem}} of {{Backus Systems}}},
  volume = {9},
  issn = {0004-5411},
  abstract = {Backus [1] has developed an elegant method of defining well-formed formulas for computer languages such as ALGOL. It consists of (our notation is slightly different from that of Backus):    A finite alphabet: a1, a2, \ldots, at; Predicates: P1, P2, \ldots, P{$\epsilon$}; Productions, either of the form (a) aj {$\in$} Pi;},
  number = {4},
  journal = {J. ACM},
  doi = {10.1145/321138.321145},
  author = {Cantor, David G.},
  month = oct,
  year = {1962},
  pages = {477--479},
  file = {/home/vipa/Zotero/storage/9CEMZWYX/Cantor - 1962 - On The Ambiguity Problem of Backus Systems.pdf}
}

@incollection{chomskyAlgebraicTheoryContextFree1963,
  series = {Computer {{Programming}} and {{Formal Systems}}},
  title = {The {{Algebraic Theory}} of {{Context}}-{{Free Languages}}*},
  volume = {35},
  abstract = {This chapter discusses the several classes of sentence-generating devices that are closely related, in various ways, to the grammars of both natural languages and artificial languages of various kinds. By a language it simply mean a set of strings in some finite set V of symbols called the vocabulary of the language. By a grammar a set of rules that give a recursive enumeration of the strings belonging to the language. It can be said that the grammar generates these strings. The chapter discusses the aspect of the structural description of a sentence, namely, its subdivision into phrases belonging to various categories. A major concern of the general theory of natural languages is to define the class of possible strings; the class of possible grammars; the class of possible structural descriptions; a procedure for assigning structural descriptions to sentences, given a grammar; and to do all of this in such a way that the structural description assigned to a sentence by the grammar of a natural language will provide the basis for explaining how a speaker of this language would understand this sentence.},
  booktitle = {Studies in {{Logic}} and the {{Foundations}} of {{Mathematics}}},
  publisher = {{Elsevier}},
  author = {Chomsky, N. and Sch\"utzenberger, M. P.},
  editor = {Braffort, P. and Hirschberg, D.},
  month = jan,
  year = {1963},
  pages = {118-161},
  file = {/home/vipa/Zotero/storage/3TMVYR9W/Chomsky and Schützenberger - 1963 - The Algebraic Theory of Context-Free Languages.pdf;/home/vipa/Zotero/storage/3BXH7TYT/S0049237X08720238.html},
  doi = {10.1016/S0049-237X(08)72023-8}
}

@inproceedings{clingerMacrosThatWork1991,
  address = {New York, NY, USA},
  series = {{{POPL}} '91},
  title = {Macros {{That Work}}},
  isbn = {978-0-89791-419-2},
  booktitle = {Proceedings of the 18th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/99583.99607},
  author = {Clinger, William and Rees, Jonathan},
  year = {1991},
  pages = {155--162},
  file = {/home/vipa/Zotero/storage/TKPZME6Y/Clinger and Rees - 1991 - Macros That Work.pdf}
}

@article{dybvigSyntacticAbstractionScheme1993,
  title = {Syntactic Abstraction in Scheme},
  volume = {5},
  issn = {1573-0557},
  abstract = {Naive program transformations can have surprising effects due to the interaction between introduced identifier references and previously existing identifier bindings, or between introduced bindings and previously existing references. These interactions can result in inadvertent binding, or capturing, of identifiers. A further complication is that transformed programs may have little resemblance to original programs, making correlation of source and object code difficult. This article describes an efficient macro system that prevents inadvertent capturing while maintaining the correlation between source and object code. The macro system allows the programmer to define program transformations using an unrestricted, general-purpose language. Previous approaches to the capturing problem have been inadequate, overly restrictive, or inefficient, and the problem of source-object correlation has been largely unaddressed. The macro system is based on a new algorithm for implementing syntactic transformations and a new representation for syntactic expressions.},
  language = {en},
  number = {4},
  journal = {LISP and Symbolic Computation},
  doi = {10.1007/BF01806308},
  author = {Dybvig, R. Kent and Hieb, Robert and Bruggeman, Carl},
  month = dec,
  year = {1993},
  keywords = {Hygienic Macros,Macros,Program Transformation,Syntactic Abstraction},
  pages = {295-326},
  file = {/home/vipa/Zotero/storage/CRNNP7AX/Dybvig et al. - 1993 - Syntactic abstraction in scheme.pdf}
}

@article{earleyEfficientContextfreeParsing1970,
  title = {An {{Efficient Context}}-Free {{Parsing Algorithm}}},
  volume = {13},
  issn = {0001-0782},
  abstract = {A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick.},
  number = {2},
  journal = {Commun. ACM},
  doi = {10.1145/362007.362035},
  author = {Earley, Jay},
  month = feb,
  year = {1970},
  keywords = {compilers,computational complexity,context-free grammar,parsing,syntax analysis},
  pages = {94--102},
  file = {/home/vipa/Zotero/storage/JF9URNT6/Earley - 1970 - An Efficient Context-free Parsing Algorithm.pdf}
}

@article{earleyAmbiguityPrecedenceSyntax1975,
  title = {Ambiguity and Precedence in Syntax Description},
  volume = {4},
  issn = {1432-0525},
  abstract = {SummaryThis paper describes a method of syntax description for programming languages which allows one to factor out that part of the description which deals with the relative precedences of syntactic units. This has been found to produce simpler and more flexible syntax descriptions. It is done by allowing the normal part of the description, which is done in BNF, to be ambiguous; these ambiguities are then resolved by a separate part of the description which gives precedence relations between the conflicting productions from the grammar. The method can be used with any left-to-right parser which is capable of detecting ambiguities and recognizing which productions they come from; We have studied its use with an LR(1) parser, and it requires a small and localized addition to the parser to enable it to deal with the precedence relations.},
  language = {en},
  number = {2},
  journal = {Acta Informatica},
  doi = {10.1007/BF00288747},
  author = {Earley, Jay},
  month = jun,
  year = {1975},
  keywords = {Communication Network,Data Structure,Information System,Information Theory,Operating System},
  pages = {183-192},
  file = {/home/vipa/Zotero/storage/CNU8S7EC/Earley - 1975 - Ambiguity and precedence in syntax description.pdf}
}

@article{ekmanJastAddSystemModular2007,
  series = {Special Issue on {{Experimental Software}} and {{Toolkits}}},
  title = {The {{JastAdd}} System \textemdash{} Modular Extensible Compiler Construction},
  volume = {69},
  issn = {0167-6423},
  abstract = {The JastAdd system enables modular specifications of extensible compiler tools and languages. Java has been extended with the Rewritable Circular Reference Attributed Grammars formalism that supports modularization and extensibility through several synergistic mechanisms. Object-orientation and static aspect-oriented programming are combined with declarative attributes and context-dependent rewrites to allow highly modular specifications. The techniques have been verified by implementing a full Java 1.4 compiler with modular extensions for non-null types and Java 5 features.},
  number = {1},
  journal = {Science of Computer Programming},
  doi = {10.1016/j.scico.2007.02.003},
  author = {Ekman, Torbj\"orn and Hedin, G\"orel},
  month = dec,
  year = {2007},
  keywords = {Compiler construction,Extensible languages,Modular implementation},
  pages = {14-26},
  file = {/home/vipa/Zotero/storage/IIL2SA46/Ekman and Hedin - 2007 - The JastAdd system — modular extensible compiler c.pdf;/home/vipa/Zotero/storage/TARP2EM9/S0167642307001591.html}
}

@inproceedings{erdwegLayoutSensitiveGeneralizedParsing2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Layout-{{Sensitive Generalized Parsing}}},
  isbn = {978-3-642-36089-3},
  abstract = {The theory of context-free languages is well-understood and context-free parsers can be used as off-the-shelf tools in practice. In particular, to use a context-free parser framework, a user does not need to understand its internals but can specify a language declaratively as a grammar. However, many languages in practice are not context-free. One particularly important class of such languages is layout-sensitive languages, in which the structure of code depends on indentation and whitespace. For example, Python, Haskell, F\#, and Markdown use indentation instead of curly braces to determine the block structure of code. Their parsers (and lexers) are not declaratively specified but hand-tuned to account for layout-sensitivity.To support declarative specifications of layout-sensitive languages, we propose a parsing framework in which a user can annotate layout in a grammar. Annotations take the form of constraints on the relative positioning of tokens in the parsed subtrees. For example, a user can declare that a block consists of statements that all start on the same column. We have integrated layout constraints into SDF and implemented a layout-sensitive generalized parser as an extension of generalized LR parsing. We evaluate the correctness and performance of our parser by parsing 33 290 open-source Haskell files. Layout-sensitive generalized parsing is easy to use, and its performance overhead compared to layout-insensitive parsing is small enough for practical application.},
  language = {en},
  booktitle = {Software {{Language Engineering}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Erdweg, Sebastian and Rendel, Tillmann and K\"astner, Christian and Ostermann, Klaus},
  editor = {Czarnecki, Krzysztof and Hedin, G\"orel},
  year = {2013},
  keywords = {Parse Tree,Abstract Syntax Tree,Curly Brace,Parse Time,Statement List},
  pages = {244-263},
  file = {/home/vipa/Zotero/storage/FGX5HIV4/Erdweg et al. - 2013 - Layout-Sensitive Generalized Parsing.pdf}
}

@article{flattMacrosThatWork2012,
  title = {Macros That {{Work Together}}: {{Compile}}-Time Bindings, Partial Expansion, and Definition Contexts},
  volume = {22},
  issn = {1469-7653, 0956-7968},
  shorttitle = {Macros That {{Work Together}}},
  abstract = {Racket is a large language that is built mostly within itself. Unlike the usual approach taken by non-Lisp languages, the self-hosting of Racket is not a matter of bootstrapping one implementation through a previous implementation, but instead a matter of building a tower of languages and libraries via macros. The upper layers of the tower include a class system, a component system, pedagogic variants of Scheme, a statically typed dialect of Scheme, and more. The demands of this language-construction effort require a macro system that is substantially more expressive than previous macro systems. In particular, while conventional Scheme macro systems handle stand-alone syntactic forms adequately, they provide weak support for macros that share information or macros that use existing syntactic forms in new contexts. This paper describes and models features of the Racket macro system, including support for general compile-time bindings, sub-form expansion and analysis, and environment management. The presentation assumes a basic familiarity with Lisp-style macros, and it takes for granted the need for macros that respect lexical scope. The model, however, strips away the pattern and template system that is normally associated with Scheme macros, isolating a core that is simpler, can support pattern and template forms themselves as macros, and generalizes naturally to Racket's other extensions.},
  language = {en},
  number = {2},
  journal = {Journal of Functional Programming},
  doi = {10.1017/S0956796812000093},
  author = {Flatt, Matthew and Culpepper, Ryan and Darais, David and Findler, Robert Bruce},
  month = mar,
  year = {2012},
  pages = {181-216},
  file = {/home/vipa/Zotero/storage/R4TIYA5U/Flatt et al. - 2012 - Macros that Work Together Compile-time bindings, .pdf;/home/vipa/Zotero/storage/7GE65QYL/375043C6746405B22014D235FA4C90C3.html}
}

@inproceedings{flattBindingSetsScopes2016,
  address = {New York, NY, USA},
  series = {{{POPL}} '16},
  title = {Binding {{As Sets}} of {{Scopes}}},
  isbn = {978-1-4503-3549-2},
  abstract = {Our new macro expander for Racket builds on a novel approach to hygiene. Instead of basing macro expansion on variable renamings that are mediated by expansion history, our new expander tracks binding through a set of scopes that an identifier acquires from both binding forms and macro expansions. The resulting model of macro expansion is simpler and more uniform than one based on renaming, and it is sufficiently compatible with Racket's old expander to be practical.},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/2837614.2837620},
  author = {Flatt, Matthew},
  year = {2016},
  keywords = {Macros,binding,hygiene,scope},
  pages = {705--717},
  file = {/home/vipa/Zotero/storage/CC4NBR3P/Flatt - 2016 - Binding As Sets of Scopes.pdf}
}

@article{ginsburgAmbiguityContextFree1966,
  title = {Ambiguity in {{Context Free Languages}}},
  volume = {13},
  issn = {0004-5411},
  abstract = {Four principal results about ambiguity in languages (i.e., context free languages) are proved. It is first shown that the problem of determining whether an arbitrary language is inherently ambiguous is recursively unsolvable. Then a decision procedure is presented for determining whether an arbitrary bounded grammar is ambiguous. Next, a necessary and sufficient algebraic condition is given for a bounded language to be inherently ambiguous. Finally, it is shown that no language contained in w1*w2*, each w1 a word, is inherently ambiguous.},
  number = {1},
  journal = {J. ACM},
  doi = {10.1145/321312.321318},
  author = {Ginsburg, Seymour and Ullian, Joseph},
  month = jan,
  year = {1966},
  pages = {62--89},
  file = {/home/vipa/Zotero/storage/E4B6B9IG/Ginsburg and Ullian - 1966 - Ambiguity in Context Free Languages.pdf}
}

@inproceedings{giorgidzeEmbeddingFunctionalHybrid2011,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Embedding a {{Functional Hybrid Modelling Language}} in {{Haskell}}},
  isbn = {978-3-642-24452-0},
  abstract = {In this paper we present the first investigation into the implementation of a Functional Hybrid Modelling language for non-causal modelling and simulation of physical systems. In particular, we present a simple way to handle connect constructs: a facility for composing model fragments present in some form in most non-causal modelling languages. Our implementation is realised as a domain-specific language embedded in Haskell. The method of embedding employs quasiquoting, thus demonstrating the effectiveness of this approach for languages that are not suitable for embedding in more traditional ways. Our implementation is available on-line, and thus the first publicly available prototype implementation of a Functional Hybrid Modelling language.},
  language = {en},
  booktitle = {Implementation and {{Application}} of {{Functional Languages}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Giorgidze, George and Nilsson, Henrik},
  editor = {Scholz, Sven-Bodo and Chitil, Olaf},
  year = {2011},
  keywords = {Abstract Syntax,Functional Programming,Modelling Language,Signal Function,Signal Relation},
  pages = {138-155},
  file = {/home/vipa/Zotero/storage/4URDAB46/Giorgidze and Nilsson - 2011 - Embedding a Functional Hybrid Modelling Language i.pdf}
}

@article{heeringSyntaxDefinitionFormalism1989,
  title = {The {{Syntax Definition Formalism SDF}}\textemdash{{Reference Manual}}\textemdash{}},
  volume = {24},
  issn = {0362-1340},
  abstract = {SDF is a formalism for the definition of syntax which is comparable to BNF in some respects, but has a wider scope in that it also covers the definition of lexical and abstract syntax. Its design and implementation are tailored towards the language designer who wants to develop new languages as well as implement existing ones in a highly interactive manner. It emphasizes compactness of syntax definitions by offering (a) a standard interface between lexical and context-free syntax; (b) a standard correspondence between context-free and abstract syntax; (c) powerful disambiguation and list constructs; and (d) an efficient incremental implementation which accepts arbitrary context-free syntax definitions. SDF can be combined with a variety of programming and specification languages. In this way these obtain fully general user-definable syntax.},
  number = {11},
  journal = {SIGPLAN Not.},
  doi = {10.1145/71605.71607},
  author = {Heering, J. and Hendriks, P. R. H. and Klint, P. and Rekers, J.},
  month = nov,
  year = {1989},
  pages = {43--75},
  file = {/home/vipa/Zotero/storage/CFXEDL3D/Heering et al. - 1989 - The Syntax Definition Formalism SDF—Reference Manu.pdf}
}

@inproceedings{hermanTheoryHygienicMacros2008,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Theory}} of {{Hygienic Macros}}},
  isbn = {978-3-540-78739-6},
  abstract = {Hygienic macro systems, such as Scheme's, automatically rename variables to prevent unintentional variable capture\textemdash{}in short, they ``just work.'' Yet hygiene has never been formally presented as a specification rather than an algorithm. According to folklore, the definition of hygienic macro expansion hinges on the preservation of alpha-equivalence. But the only known notion of alpha-equivalence for programs with macros depends on the results of macro expansion! We break this circularity by introducing explicit binding specifications into the syntax of macro definitions, permitting a definition of alpha-equivalence independent of expansion. We define a semantics for a first-order subset of Scheme-like macros and prove hygiene as a consequence of confluence.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Herman, David and Wand, Mitchell},
  editor = {Drossopoulou, Sophia},
  year = {2008},
  keywords = {Core Form,Pattern Variable,Scheme Program,Shape Type,Type Annotation},
  pages = {48-62},
  file = {/home/vipa/Zotero/storage/2WZMV7AL/Herman and Wand - 2008 - A Theory of Hygienic Macros.pdf}
}

@inproceedings{hickeyClojureProgrammingLanguage2008,
  address = {New York, NY, USA},
  series = {{{DLS}} '08},
  title = {The {{Clojure Programming Language}}},
  isbn = {978-1-60558-270-2},
  abstract = {Customers and stakeholders have substantial investments in, and are comfortable with the performance, security and stability of, industry-standard platforms like the JVM and CLR. While Java and C\# developers on those platforms may envy the succinctness, flexibility and productivity of dynamic languages, they have concerns about running on customer-approved infrastructure, access to their existing code base and libraries, and performance. In addition, they face ongoing problems dealing with concurrency using native threads and locking. Clojure is an effort in pragmatic dynamic language design in this context. It endeavors to be a general-purpose language suitable in those areas where Java is suitable. It reflects the reality that, for the concurrent programming future, pervasive, unmoderated mutation simply has to go. Clojure meets its goals by: embracing an industry-standard, open platform - the JVM; modernizing a venerable language - Lisp; fostering functional programming with immutable persistent data structures; and providing built-in concurrency support via software transactional memory and asynchronous agents. The result is robust, practical, and fast. This talk will focus on the motivations, mechanisms and experiences of the implementation of Clojure.},
  booktitle = {Proceedings of the 2008 {{Symposium}} on {{Dynamic Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/1408681.1408682},
  author = {Hickey, Rich},
  year = {2008},
  pages = {1:1--1:1}
}

@article{hudakBuildingDomainspecificEmbedded1996,
  title = {Building {{Domain}}-Specific {{Embedded Languages}}},
  volume = {28},
  issn = {0360-0300},
  number = {4es},
  journal = {ACM Comput. Surv.},
  doi = {10.1145/242224.242477},
  author = {Hudak, Paul},
  month = dec,
  year = {1996},
  file = {/home/vipa/Zotero/storage/R5SFEV8C/Hudak - 1996 - Building Domain-specific Embedded Languages.pdf}
}

@article{kaminskiReliablyComposableLanguage2017,
  title = {Reliably Composable Language Extensions},
  abstract = {Many programming tasks are dramatically simpler when an appropriate domain-specific language can be used to accomplish them. These languages offer a variety of potential advantages, including programming at a higher level of abstraction, custom analyses specific to the problem domain, and the ability to generate very efficient code. But they also suffer many disadvantages as a result of their implementation techniques. Fully separate languages (such as YACC, or SQL) are quite flexible, but these are distinct monolithic entities and thus we are unable to draw on the features of several in combination to accomplish a single task. That is, we cannot compose their domain-specific features. "Embedded" DSLs (such as parsing combinators) accomplish something like a different language, but are actually implemented simply as libraries within a flexible host language. This approach allows different libraries to be imported and used together, enabling composition, but it is limited in analysis and translation capabilities by the host language they are embedded within. A promising combination of these two approaches is to allow a host language to be directly extended with new features (syntactic and semantic.) However, while there are plausible ways to attempt to compose language extensions, they can easily fail, making this approach unreliable. Previous methods of assuring reliable composition impose onerous restrictions, such as throwing out entirely the ability to introduce new analysis. This thesis introduces reliably composable language extensions as a technique for the implementation of DSLs. This technique preserves most of the advantages of both separate and "embedded" DSLs. Unlike many prior approaches to language extension, this technique ensures composition of multiple language extensions will succeed, and preserves strong properties about the behavior of the resulting composed compiler. We define an analysis on language extensions that guarantees the composition of several extensions will be well-defined, and we further define a set of testable properties that ensure the resulting compiler will behave as expected, along with a principle that assigns "blame" for bugs that may ultimately appear as a result of composition. Finally, to concretely compare our approach to our original goals for reliably composable language extension, we use these techniques to develop an extensible C compiler front-end, together with several example composable language extensions.},
  language = {en},
  doi = {https://doi.org/10.24926/2017.188954},
  author = {Kaminski, Ted},
  month = may,
  year = {2017},
  file = {/home/vipa/Zotero/storage/AYLWF7HR/Kaminski - 2017 - Reliably composable language extensions.pdf;/home/vipa/Zotero/storage/YEY5CYUS/188954.html}
}

@article{leoGeneralContextfreeParsing1991,
  title = {A General Context-Free Parsing Algorithm Running in Linear Time on Every {{LR}}(k) Grammar without Using Lookahead},
  volume = {82},
  issn = {0304-3975},
  abstract = {A new general context-free parsing algorithm is presented which runs in linear time and space on every LR(k) grammar without using any lookahead and without making use of the LR property. Most of the existing implementations of tabular parsing algorithms, including those using lookahead, can easily be adapted to this new algorithm without a noteworthy loss of efficiency. For some natural right recursive grammars both the time and space complexity will be improved from {$\Omega$}(n2) to O(n). This makes this algorithm not only of theoretical but probably of practical interest as well.},
  number = {1},
  journal = {Theoretical Computer Science},
  doi = {10.1016/0304-3975(91)90180-A},
  author = {Leo, Joop M. I. M.},
  month = may,
  year = {1991},
  pages = {165-176},
  file = {/home/vipa/Zotero/storage/38QMSIME/Leo - 1991 - A general context-free parsing algorithm running i.pdf;/home/vipa/Zotero/storage/TZZXSRUT/030439759190180A.html}
}

@inproceedings{lorenzenSoundTypedependentSyntactic2016,
  address = {New York, NY, USA},
  series = {{{POPL}} '16},
  title = {Sound {{Type}}-Dependent {{Syntactic Language Extension}}},
  isbn = {978-1-4503-3549-2},
  abstract = {Syntactic language extensions can introduce new facilities into a programming language while requiring little implementation effort and modest changes to the compiler. It is typical to desugar language extensions in a distinguished compiler phase after parsing or type checking, not affecting any of the later compiler phases. If desugaring happens before type checking, the desugaring cannot depend on typing information and type errors are reported in terms of the generated code. If desugaring happens after type checking, the code generated by the desugaring is not type checked and may introduce vulnerabilities. Both options are undesirable. We propose a system for syntactic extensibility where desugaring happens after type checking and desugarings are guaranteed to only generate well-typed code. A major novelty of our work is that desugarings operate on typing derivations instead of plain syntax trees. This provides desugarings access to typing information and forms the basis for the soundness guarantee we provide, namely that a desugaring generates a valid typing derivation. We have implemented our system for syntactic extensibility in a language-independent fashion and instantiated it for a substantial subset of Java, including generics and inheritance. We provide a sound Java extension for Scala-like for-comprehensions.},
  booktitle = {Proceedings of the 43rd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/2837614.2837644},
  author = {Lorenzen, Florian and Erdweg, Sebastian},
  year = {2016},
  keywords = {metaprogramming,automatic verification,Language extensibility,macros,type soundness,type-dependent desugaring},
  pages = {204--216},
  file = {/home/vipa/Zotero/storage/6AIKV522/Lorenzen and Erdweg - 2016 - Sound Type-dependent Syntactic Language Extension.pdf}
}

@inproceedings{matsakisRustLanguage2014,
  address = {New York, NY, USA},
  series = {{{HILT}} '14},
  title = {The {{Rust Language}}},
  isbn = {978-1-4503-3217-0},
  abstract = {Rust is a new programming language for developing reliable and efficient systems. It is designed to support concurrency and parallelism in building applications and libraries that take full advantage of modern hardware. Rust's static type system is safe and expressive and provides strong guarantees about isolation, concurrency, and memory safety.

Rust also offers a clear performance model, making it easier to predict and reason about program efficiency. One important way it accomplishes this is by allowing fine-grained control over memory representations, with direct support for stack allocation and contiguous record storage. The language balances such controls with the absolute requirement for safety: Rust's type system and runtime guarantee the absence of data races, buffer overflows, stack overflows, and accesses to uninitialized or deallocated memory.},
  booktitle = {Proceedings of the 2014 {{ACM SIGAda Annual Conference}} on {{High Integrity Language Technology}}},
  publisher = {{ACM}},
  doi = {10.1145/2663171.2663188},
  author = {Matsakis, Nicholas D. and Klock, II, Felix S.},
  year = {2014},
  keywords = {affine type systems,memory management,rust,systems programming},
  pages = {103--104},
  file = {/home/vipa/Zotero/storage/5V24PA6K/Matsakis, Klock II - 2014 - The rust language.pdf;/home/vipa/Zotero/storage/D8HZIDXS/Matsakis and Klock - 2014 - The Rust Language.pdf}
}

@inproceedings{neronTheoryNameResolution2015,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Theory}} of {{Name Resolution}}},
  isbn = {978-3-662-46669-8},
  abstract = {We describe a language-independent theory for name binding and resolution, suitable for programming languages with complex scoping rules including both lexical scoping and modules. We formulate name resolution as a two-stage problem. First a language-independent scope graph is constructed using language-specific rules from an abstract syntax tree. Then references in the scope graph are resolved to corresponding declarations using a language-independent resolution process. We introduce a resolution calculus as a concise, declarative, and languageindependent specification of name resolution. We develop a resolution algorithm that is sound and complete with respect to the calculus. Based on the resolution calculus we develop language-independent definitions of {$\alpha$}-equivalence and rename refactoring. We illustrate the approach using a small example language with modules. In addition, we show how our approach provides a model for a range of name binding patterns in existing languages.},
  language = {en},
  booktitle = {Programming {{Languages}} and {{Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Neron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
  editor = {Vitek, Jan},
  year = {2015},
  keywords = {Abstract Syntax Tree,Binding Pattern,Code Completion,Resolution Algorithm,Visibility Policy},
  pages = {205-231},
  file = {/home/vipa/Zotero/storage/9JGIUAPL/Neron et al. - 2015 - A Theory of Name Resolution.pdf}
}

@article{oderskySimplicitlyFoundationsApplications2017,
  title = {Simplicitly: {{Foundations}} and {{Applications}} of {{Implicit Function Types}}},
  volume = {2},
  issn = {2475-1421},
  shorttitle = {Simplicitly},
  abstract = {Understanding a program entails understanding its context; dependencies, configurations and even implementations are all forms of contexts. Modern programming languages and theorem provers offer an array of constructs to define contexts, implicitly. Scala offers implicit parameters which are used pervasively, but which cannot be abstracted over.  This paper describes a generalization of implicit parameters to implicit function types, a powerful way to abstract over the context in which some piece of code is run. We provide a formalization based on bidirectional type-checking that closely follows the semantics implemented by the Scala compiler.  To demonstrate their range of abstraction capabilities, we present several applications that make use of implicit function types. We show how to encode the builder pattern, tagless interpreters, reader and free monads and we assess the performance of the monadic structures presented.},
  number = {POPL},
  journal = {Proc. ACM Program. Lang.},
  doi = {10.1145/3158130},
  author = {Odersky, Martin and Blanvillain, Olivier and Liu, Fengyun and Biboudis, Aggelos and Miller, Heather and Stucki, Sandro},
  month = dec,
  year = {2017},
  keywords = {Dotty,implicit parameters,Scala},
  pages = {42:1--42:29},
  file = {/home/vipa/Zotero/storage/B8BTVUNF/Odersky et al. - 2017 - Simplicitly Foundations and Applications of Impli.pdf}
}

@book{palmkvistBuildingProgrammingLanguages2018,
  series = {{{TRITA}}-{{EECS}}-{{EX}}},
  title = {Building {{Programming Languages}}, {{Construction}} by {{Construction}}},
  abstract = {The task of implementing a programming language is a task that entails a great deal of work. Yet much of this work is similar for different programming languages: most languages require, e.g., parsing, name resolution, type-checking, and optimization. When implementing domain-specific languages (DSLs) the reimplementation of these largely similar tasks seems especially redundant. A number of approaches exist to alleviate this issue, including embedded DSLs, macro-rewriting systems, and more general systems intended for language implementation. However, these tend to have at least one of the following limitations: They present a leaky abstraction, e.g., error messages do not refer to the DSL but rather some other programming language, namely the one used to implement the DSL. They limit the flexibility of the DSL, either to the constructs present in another language, or merely to the syntax of some other language. They see an entire language as the unit of composition. Complete languages are extended with other complete language extensions. Instead, this thesis introduces the concept of a syntax construction, which represents a smaller unit of composition. A syntax construction defines a single language feature, e.g., an if-statement, an anonymous function, or addition. Each syntax construction specifies its own syntax, binding semantics, and runtime semantics, independent of the rest of the language. The runtime semantics are defined using a translation into another target language, similarly to macros. These translations can then be checked to ensure that they preserve binding semantics and introduce no binding errors. This checking ensures that binding errors can be presented in terms of code the programmer wrote, rather than generated code in some underlying language. During evaluation several limitations are encountered. Removing or minimizing these limitations appears possible, but is left for future work},
  language = {eng},
  number = {2018:408},
  publisher = {{KTH, School of Electrical Engineering and Computer Science (EECS)}},
  author = {Palmkvist, Viktor},
  year = {2018},
  keywords = {domain-specific language,programming language construction},
  file = {/home/vipa/Zotero/storage/8VSGV9KR/Palmkvist - 2018 - Building Programming Languages, Construction by Co.pdf;/home/vipa/Zotero/storage/G6UC36JC/record.html}
}

@inproceedings{parrLLFoundationANTLR2011,
  address = {New York, NY, USA},
  series = {{{PLDI}} '11},
  title = {{{LL}}(*): {{The Foundation}} of the {{ANTLR Parser Generator}}},
  isbn = {978-1-4503-0663-8},
  shorttitle = {{{LL}}(*)},
  abstract = {Despite the power of Parser Expression Grammars (PEGs) and GLR, parsing is not a solved problem. Adding nondeterminism (parser speculation) to traditional LL and LR parsers can lead to unexpected parse-time behavior and introduces practical issues with error handling, single-step debugging, and side-effecting embedded grammar actions. This paper introduces the LL(*) parsing strategy and an associated grammar analysis algorithm that constructs LL(*) parsing decisions from ANTLR grammars. At parse-time, decisions gracefully throttle up from conventional fixed k{$>$}=1 lookahead to arbitrary lookahead and, finally, fail over to backtracking depending on the complexity of the parsing decision and the input symbols. LL(*) parsing strength reaches into the context-sensitive languages, in some cases beyond what GLR and PEGs can express. By statically removing as much speculation as possible, LL(*) provides the expressivity of PEGs while retaining LL's good error handling and unrestricted grammar actions. Widespread use of ANTLR (over 70,000 downloads/year) shows that it is effective for a wide variety of applications.},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/1993498.1993548},
  author = {Parr, Terence and Fisher, Kathleen},
  year = {2011},
  keywords = {augmented transition networks,backtracking,context-sensitive parsing,deterministic finite automata,glr,memoization,nondeterministic parsing,peg,semantic predicates,subset construction,syntactic predicates},
  pages = {425--436},
  file = {/home/vipa/Zotero/storage/Y9FRARB7/Parr and Fisher - 2011 - LL() The Foundation of the ANTLR Parser Generato.pdf}
}

@inproceedings{rompfLightweightModularStaging2010,
  address = {New York, NY, USA},
  series = {{{GPCE}} '10},
  title = {Lightweight {{Modular Staging}}: {{A Pragmatic Approach}} to {{Runtime Code Generation}} and {{Compiled DSLs}}},
  isbn = {978-1-4503-0154-1},
  shorttitle = {Lightweight {{Modular Staging}}},
  abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used. We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process. We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  publisher = {{ACM}},
  doi = {10.1145/1868294.1868314},
  author = {Rompf, Tiark and Odersky, Martin},
  year = {2010},
  keywords = {code generation,domain-specific languages,language virtualization,multi-stage programming},
  pages = {127--136},
  file = {/home/vipa/Zotero/storage/82G4CHYH/Rompf and Odersky - 2010 - Lightweight Modular Staging A Pragmatic Approach .pdf}
}

@inproceedings{sheardTemplateMetaprogrammingHaskell2002,
  address = {New York, NY, USA},
  series = {Haskell '02},
  title = {Template {{Meta}}-Programming for {{Haskell}}},
  isbn = {978-1-58113-605-0},
  abstract = {We propose a new extension to the purely functional programming language Haskell that supports compile-time meta-programming. The purpose of the system is to support the algorithmic construction of programs at compile-time.The ability to generate code at compile time allows the programmer to implement such features as polytypic programs, macro-like expansion, user directed optimization (such as inlining), and the generation of supporting data structures and functions from existing data structures and functions.Our design is being implemented in the Glasgow Haskell Compiler, ghc.},
  booktitle = {Proceedings of the 2002 {{ACM SIGPLAN Workshop}} on {{Haskell}}},
  publisher = {{ACM}},
  doi = {10.1145/581690.581691},
  author = {Sheard, Tim and Jones, Simon Peyton},
  year = {2002},
  keywords = {meta programming,templates},
  pages = {1--16},
  file = {/home/vipa/Zotero/storage/LK92M7E7/Sheard and Jones - 2002 - Template Meta-programming for Haskell.pdf}
}

@inproceedings{silkensenWellTypedIslandsParse2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Well-{{Typed Islands Parse Faster}}},
  isbn = {978-3-642-40447-4},
  abstract = {This paper addresses the problem of specifying and parsing the syntax of domain-specific languages (DSLs) in a modular, user-friendly way. We want to enable the design of composable DSLs that combine the natural syntax of external DSLs with the easy implementation of internal DSLs. The challenge in parsing these DSLs is that the composition of several languages is likely to contain ambiguities. We present the design of a system that uses a type-oriented variant of island parsing to efficiently parse the syntax of composable DSLs. In particular, we argue that the running time of type-oriented island parsing doesn't depend on the number of DSLs imported. We also show how to use our tool to implement DSLs on top of a host language such as Typed Racket.},
  language = {en},
  booktitle = {Trends in {{Functional Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Silkensen, Erik and Siek, Jeremy},
  editor = {Loidl, Hans-Wolfgang and Pe\~na, Ricardo},
  year = {2013},
  keywords = {Parse Tree,Concrete Syntax,Deductive System,Grammar Rule,Input String},
  pages = {69-84},
  file = {/home/vipa/Zotero/storage/6NZRH6B6/Silkensen and Siek - 2013 - Well-Typed Islands Parse Faster.pdf}
}

@article{stansiferRomeoSystemMore2016,
  title = {Romeo: {{A}} System for More Flexible Binding-Safe Programming*},
  volume = {26},
  issn = {0956-7968, 1469-7653},
  shorttitle = {Romeo},
  abstract = {Current systems for safely manipulating values containing names only support simple binding structures for those names. As a result, few tools exist to safely manipulate code in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects {$\alpha$}-equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complex-binding support of David Herman's {$\lambda$}m
, but is a full-fledged binding-safe language like Pure FreshML.},
  language = {en},
  journal = {Journal of Functional Programming},
  doi = {10.1017/S0956796816000137},
  author = {Stansifer, Paul and Wand, Mitchell},
  year = {2016/ed},
  file = {/home/vipa/Zotero/storage/EFS37BVK/Stansifer and Wand - 2016 - Romeo A system for more flexible binding-safe pro.pdf;/home/vipa/Zotero/storage/QGXXGJKH/F9C128D1D92B6BCD489E0FE41D255AE0.html}
}

@inproceedings{steeleOverviewCOMMONLISP1982,
  address = {New York, NY, USA},
  series = {{{LFP}} '82},
  title = {An {{Overview}} of {{COMMON LISP}}},
  isbn = {978-0-89791-082-8},
  abstract = {A dialect of LISP called ``COMMON LISP'' is being cooperatively developed and implemented at several sites. It is a descendant of the MACLISP family of LISP dialects, and is intended to unify the several divergent efforts of the last five years. We first give an extensive history of LISP, particularly of the MACLISP branch, in order to explain in context the motivation for COMMON LISP. We enumerate the goals and non-goals of the language design, discuss the language features of primary interest, and then consider how these features help to meet the expressed goals. Finally, the status (as of May 1982) of six implementations of COMMON LISP is summarized.},
  booktitle = {Proceedings of the 1982 {{ACM Symposium}} on {{LISP}} and {{Functional Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/800068.802140},
  author = {Steele, Jr., Guy L.},
  year = {1982},
  pages = {98--107},
  file = {/home/vipa/Zotero/storage/3T8T9C8N/Steele - 1982 - An Overview of COMMON LISP.pdf}
}

@article{sujeethDeliteCompilerArchitecture2014,
  title = {Delite: {{A Compiler Architecture}} for {{Performance}}-{{Oriented Embedded Domain}}-{{Specific Languages}}},
  volume = {13},
  issn = {1539-9087},
  shorttitle = {Delite},
  abstract = {Developing high-performance software is a difficult task that requires the use of low-level, architecture-specific programming models (e.g., OpenMP for CMPs, CUDA for GPUs, MPI for clusters). It is typically not possible to write a single application that can run efficiently in different environments, leading to multiple versions and increased complexity. Domain-Specific Languages (DSLs) are a promising avenue to enable programmers to use high-level abstractions and still achieve good performance on a variety of hardware. This is possible because DSLs have higher-level semantics and restrictions than general-purpose languages, so DSL compilers can perform higher-level optimization and translation. However, the cost of developing performance-oriented DSLs is a substantial roadblock to their development and adoption. In this article, we present an overview of the Delite compiler framework and the DSLs that have been developed with it. Delite simplifies the process of DSL development by providing common components, like parallel patterns, optimizations, and code generators, that can be reused in DSL implementations. Delite DSLs are embedded in Scala, a general-purpose programming language, but use metaprogramming to construct an Intermediate Representation (IR) of user programs and compile to multiple languages (including C++, CUDA, and OpenCL). DSL programs are automatically parallelized and different parts of the application can run simultaneously on CPUs and GPUs. We present Delite DSLs for machine learning, data querying, graph analysis, and scientific computing and show that they all achieve performance competitive to or exceeding C++ code.},
  number = {4s},
  journal = {ACM Trans. Embed. Comput. Syst.},
  doi = {10.1145/2584665},
  author = {Sujeeth, Arvind K. and Brown, Kevin J. and Lee, Hyoukjoong and Rompf, Tiark and Chafi, Hassan and Odersky, Martin and Olukotun, Kunle},
  month = apr,
  year = {2014},
  keywords = {code generation,language virtualization,Domain-specific languages,multistage programming},
  pages = {134:1--134:25},
  file = {/home/vipa/Zotero/storage/STUG7GZL/Sujeeth et al. - 2014 - Delite A Compiler Architecture for Performance-Or.pdf}
}

@article{tobin-hochstadtExtensiblePatternMatching2011,
  title = {Extensible {{Pattern Matching}} in an {{Extensible Language}}},
  language = {en},
  author = {{Tobin-Hochstadt}, Sam},
  month = jun,
  year = {2011},
  keywords = {Computer Science - Programming Languages},
  file = {/home/vipa/Zotero/storage/3NCFYXR8/Tobin-Hochstadt - 2011 - Extensible Pattern Matching in an Extensible Langu.pdf;/home/vipa/Zotero/storage/MW6J4GDP/1106.html}
}

@inproceedings{tobin-hochstadtLanguagesLibraries2011,
  address = {New York, NY, USA},
  series = {{{PLDI}} '11},
  title = {Languages {{As Libraries}}},
  isbn = {978-1-4503-0663-8},
  abstract = {Programming language design benefits from constructs for extending the syntax and semantics of a host language. While C's string-based macros empower programmers to introduce notational shorthands, the parser-level macros of Lisp encourage experimentation with domain-specific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope.  The design of Racket---a descendant of Scheme---goes even further with the introduction of a full-fledged interface to the static semantics of the language. A Racket extension programmer can thus add constructs that are indistinguishable from "native" notation, large and complex embedded domain-specific languages, and even optimizing transformations for the compiler backend. This power to experiment with language design has been used to create a series of sub-languages for programming with first-class classes and modules, numerous languages for implementing the Racket system, and the creation of a complete and fully integrated typed sister language to Racket's untyped base language. This paper explains Racket's language extension API via an implementation of a small typed sister language. The new language provides a rich type system that accommodates the idioms of untyped Racket. Furthermore, modules in this typed language can safely exchange values with untyped modules. Last but not least, the implementation includes a type-based optimizer that achieves promising speedups. Although these extensions are complex, their Racket implementation is just a library, like any other library, requiring no changes to the Racket implementation.},
  booktitle = {Proceedings of the {{32Nd ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/1993498.1993514},
  author = {{Tobin-Hochstadt}, Sam and {St-Amour}, Vincent and Culpepper, Ryan and Flatt, Matthew and Felleisen, Matthias},
  year = {2011},
  keywords = {macros,extensible languages,modules,typed racket},
  pages = {132--141},
  file = {/home/vipa/Zotero/storage/6JIYM55T/Tobin-Hochstadt et al. - 2011 - Languages As Libraries.pdf}
}

@inproceedings{vanwykContextawareScanningParsing2007,
  address = {New York, NY, USA},
  series = {{{GPCE}} '07},
  title = {Context-Aware {{Scanning}} for {{Parsing Extensible Languages}}},
  isbn = {978-1-59593-855-8},
  abstract = {This paper introduces new parsing and context-aware scanning algorithms in which the scanner uses contextual information to disambiguate lexical syntax. The parser uses a slightly modified LR-style algorithm that passes to the scanner the set of valid symbols that the scanner may return at that point in parsing. This set is those terminals whose entries in the parse table for the current parse state are shift, reduce, or accept, but not error. The scanner then only returns tokens in this set. An analysis is given that can statically verify that the scanner will never return more than one token for a single input. Context-aware scanning is especially useful when parsing and scanning extensible languages in which domain specific languages can be embedded. It has been used in extensible versions of Java 1.4 and ANSI C. We illustrate this approach with a declarative specification of a subset of Java and extensions that embed SQL queries and Boolean expression tables into Java.},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Generative Programming}} and {{Component Engineering}}},
  publisher = {{ACM}},
  doi = {10.1145/1289971.1289983},
  author = {Van Wyk, Eric R. and Schwerdfeger, August C.},
  year = {2007},
  keywords = {extensible languages,context-aware scanning},
  pages = {63--72},
  file = {/home/vipa/Zotero/storage/BL4I4DNX/Van Wyk and Schwerdfeger - 2007 - Context-aware Scanning for Parsing Extensible Lang.pdf}
}

@article{vanwykSilverExtensibleAttribute2010,
  series = {Special {{Issue}} on {{ETAPS}} 2006 and 2007 {{Workshops}} on {{Language Descriptions}}, {{Tools}}, and {{Applications}} ({{LDTA}} '06 and '07)},
  title = {Silver: {{An}} Extensible Attribute Grammar System},
  volume = {75},
  issn = {0167-6423},
  shorttitle = {Silver},
  abstract = {Attribute grammar specification languages, like many domain-specific languages, offer significant advantages to their users, such as high-level declarative constructs and domain-specific analyses. Despite these advantages, attribute grammars are often not adopted to the degree that their proponents envision. One practical obstacle to their adoption is a perceived lack of both domain-specific and general purpose language features needed to address the many different aspects of a problem. Here we describe Silver, an extensible attribute grammar specification system, and show how it can be extended with general purpose features such as pattern matching and domain-specific features such as collection attributes and constructs for supporting data-flow analysis of imperative programs. The result is an attribute grammar specification language with a rich set of language features. Silver is implemented in itself by a Silver attribute grammar and utilizes forwarding to implement the extensions in a cost-effective manner.},
  number = {1},
  journal = {Science of Computer Programming},
  doi = {10.1016/j.scico.2009.07.004},
  author = {Van Wyk, Eric and Bodin, Derek and Gao, Jimin and Krishnan, Lijesh},
  month = jan,
  year = {2010},
  keywords = {Extensible languages,Attribute grammars,Extensible compilers,Forwarding,Silver attribute grammar system},
  pages = {39-54},
  file = {/home/vipa/Zotero/storage/CFSWWB64/Van Wyk et al. - 2010 - Silver An extensible attribute grammar system.pdf;/home/vipa/Zotero/storage/58HY84IZ/S0167642309001099.html}
}

@inproceedings{wanFunctionalReactiveProgramming2000,
  address = {New York, NY, USA},
  series = {{{PLDI}} '00},
  title = {Functional {{Reactive Programming}} from {{First Principles}}},
  isbn = {978-1-58113-199-4},
  abstract = {Functional Reactive Programming, or FRP, is a general framework for programming hybrid systems in a high-level, declarative manner. The key ideas in FRP are its notions of behaviors and events. Behaviors are time-varying, reactive values, while events are time-ordered sequences of discrete-time event occurrences. FRP is the essence of Fran, a domain-specific language embedded in Haskell for programming reactive animations, but FRP is now also being used in vision, robotics and other control systems applications. 
In this paper we explore the formal semantics of FRP and how it
relates to an implementation based on streams that represent (and therefore only approximate) continuous behaviors. We show that, in the limit as the sampling interval goes to zero, the implementation is faithful to the formal, continuous semantics, but only when certain constraints on behaviors are observed. We explore the nature of these constraints, which vary amongst the FRP primitives. Our results show both the power and limitations of this approach to language design and implementation. As an example of a limitation, we show that streams are incapable of representing instantaneous predicate events over behaviors.},
  booktitle = {Proceedings of the {{ACM SIGPLAN}} 2000 {{Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/349299.349331},
  author = {Wan, Zhanyong and Hudak, Paul},
  year = {2000},
  pages = {242--252},
  file = {/home/vipa/Zotero/storage/J8CF6LMM/Wan and Hudak - 2000 - Functional Reactive Programming from First Princip.pdf}
}

@article{kaminskiReliableAutomaticComposition2017,
  title = {Reliable and {{Automatic Composition}} of {{Language Extensions}} to {{C}}: {{The ableC Extensible Language Framework}}},
  volume = {1},
  issn = {2475-1421},
  shorttitle = {Reliable and {{Automatic Composition}} of {{Language Extensions}} to {{C}}},
  abstract = {This paper describes an extensible language framework, ableC, that allows programmers to import new, domain-specific, independently-developed language features into their programming language, in this case C. Most importantly, this framework ensures that the language extensions will automatically compose to form a working translator that does not terminate abnormally. This is possible due to two modular analyses that extension developers can apply to their language extension to check its composability. Specifically, these ensure that the composed concrete syntax specification is non-ambiguous and the composed attribute grammar specifying the semantics is well-defined. This assurance and the expressiveness of the supported extensions is a distinguishing characteristic of the approach.   The paper describes a number of techniques for specifying a host language, in this case C at the C11 standard, to make it more amenable to language extension. These include techniques that make additional extensions pass these modular analyses, refactorings of the host language to support a wider range of extensions, and the addition of semantic extension points to support, for example, operator overloading and non-local code transformations.},
  number = {OOPSLA},
  journal = {Proc. ACM Program. Lang.},
  doi = {10.1145/3138224},
  author = {Kaminski, Ted and Kramer, Lucas and Carlson, Travis and Van Wyk, Eric},
  month = oct,
  year = {2017},
  keywords = {context-aware scanning,attribute grammars,domain specific languages,extensible compiler frameworks,language composition},
  pages = {98:1--98:29},
  file = {/home/vipa/Zotero/storage/J5Q5UTZY/Kaminski et al. - 2017 - Reliable and Automatic Composition of Language Ext.pdf}
}

@inproceedings{farrowComposableAttributeGrammars1992,
  address = {New York, NY, USA},
  series = {{{POPL}} '92},
  title = {Composable {{Attribute Grammars}}: {{Support}} for {{Modularity}} in {{Translator Design}} and {{Implementation}}},
  isbn = {978-0-89791-453-6},
  shorttitle = {Composable {{Attribute Grammars}}},
  abstract = {This paper introduces Composable Attribute Grammars (CAGs), a formalism that extends classical attribute grammars to allow for the modular composition of translation specifications and of translators. CAGs bring to complex translator writing systems the same benefits of modularity found in modern programming languages, including comprehensibility, reusability, and incremental meta-compilation.
A CAG is built from several smaller component AGs, each of which solves a particular subproblem, such as name analysis or register allocation. A component AG is based upon a simplified phrase-structure that reflects the properties of its subproblem rather than the phrase-structure of the source language. Different component phrase-structures for various subproblems are combined by mapping them into a phrase-structure for the source language. Both input and output attributes can be associated with the terminal symbols of a component AG. Output attributes enable the results of solving a subproblem to be distributed back to anywhere that originally contributed part of the subproblem, e.g. transparently distributing the results of global name analysis back to every symbolic reference in the source program.
After introducing CAGs by way of an example, we provide a formal definition of CAGs and their semantics. We describe a subclass of CAGs and their semantics. We describe a subclass of CAGs, called separable CAGs, that have favorable implementation properties. We discuss the novel aspects of CAGs, compare them to other proposals for inserting modularity into attribute grammars, and relate our experience using CAGs in the Linguist translator-writing system.},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  publisher = {{ACM}},
  doi = {10.1145/143165.143210},
  author = {Farrow, R. and Marlowe, T. J. and Yellin, D. M.},
  year = {1992},
  pages = {223--234},
  file = {/home/vipa/Zotero/storage/LWGKUVTC/Farrow et al. - 1992 - Composable Attribute Grammars Support for Modular.pdf}
}

@inproceedings{omarSafelyComposableTypeSpecific2014,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Safely {{Composable Type}}-{{Specific Languages}}},
  isbn = {978-3-662-44202-9},
  abstract = {Programming languages often include specialized syntax for common datatypes (e.g. lists) and some also build in support for specific specialized datatypes (e.g. regular expressions), but user-defined types must use general-purpose syntax. Frustration with this causes developers to use strings, rather than structured data, with alarming frequency, leading to correctness, performance, security, and usability issues. Allowing library providers to modularly extend a language with new syntax could help address these issues. Unfortunately, prior mechanisms either limit expressiveness or are not safely composable: individually unambiguous extensions can still cause ambiguities when used together. We introduce type-specific languages (TSLs): logic associated with a type that determines how the bodies of generic literals, able to contain arbitrary syntax, are parsed and elaborated, hygienically. The TSL for a type is invoked only when a literal appears where a term of that type is expected, guaranteeing non-interference. We give evidence supporting the applicability of this approach and formally specify it with a bidirectionally typed elaboration semantics for the Wyvern programming language.},
  language = {en},
  booktitle = {{{ECOOP}} 2014 \textendash{} {{Object}}-{{Oriented Programming}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Omar, Cyrus and Kurilova, Darya and Nistor, Ligia and Chung, Benjamin and Potanin, Alex and Aldrich, Jonathan},
  editor = {Jones, Richard},
  year = {2014},
  keywords = {parsing,hygiene,extensible languages,bidirectional typechecking},
  pages = {105-130},
  file = {/home/vipa/Zotero/storage/KT9AALQD/Omar et al. - 2014 - Safely Composable Type-Specific Languages.pdf}
}

@techreport{flattReferenceRacket2010,
  title = {Reference: {{Racket}}},
  number = {PLT-TR-2010-1},
  institution = {{PLT Design Inc.}},
  author = {Flatt, Matthew and {PLT}},
  year = {2010}
}

@article{hermanTheoryTypedHygienic2010,
  title = {A {{Theory}} of {{Typed Hygienic Macros}}},
  volume = {4960},
  issn = {03029743},
  abstract = {We present the {$\lambda$}m-calculus, a semantics for a language of hygienic macros with a non-trivial theory. Unlike Scheme, where programs must be macro- expanded to be analyzed, our semantics admits reasoning about programs as they appear to programmers. Our contributions include a semantics of hygienic macro expansion, a formal definition of {$\alpha$}-equivalence that is independent of expansion, and a proof that expansion preserves {$\alpha$}-equivalence. The key technical component of our language is a type system similar to Culpepper and Felleisens shape types, but with the novel contribution of binding signature types, which specify the bindings and scope of a macros arguments.},
  journal = {Proceedings of the 17th European Symposium on Programming},
  doi = {10.1007/978-3-540-78739-6_4},
  author = {Herman, David and Wand, Mitchell},
  year = {2010},
  pages = {48}
}

@inproceedings{stansiferRomeo2014,
  address = {New York, New York, USA},
  title = {Romeo},
  volume = {49},
  isbn = {978-1-4503-2873-9},
  abstract = {Current languages for safely manipulating values with names only support term languages with simple binding syntax. As a result, no tools exist to safely manipulate code written in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects {$\alpha$}-equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complex-binding support of David Herman's {$\lambda$}m, but is a full-fledged binding-safe language like Pure FreshML.},
  booktitle = {Proceedings of the 19th {{ACM SIGPLAN}} International Conference on {{Functional}} Programming - {{ICFP}} '14},
  publisher = {{ACM Press}},
  doi = {10.1145/2628136.2628162},
  author = {Stansifer, Paul and Wand, Mitchell},
  year = {2014},
  pages = {53--65}
}

@article{augustssonParadiseTwostageDSL2008a,
  title = {Paradise: {{A Two}}-Stage {{DSL Embedded}} in {{Haskell}}},
  volume = {43},
  issn = {0362-1340},
  number = {9},
  journal = {SIGPLAN Not.},
  doi = {10.1145/1411203.1411236},
  author = {Augustsson, Lennart and Mansell, Howard and Sittampalam, Ganesh},
  month = sep,
  year = {2008},
  keywords = {dsels,Haskell,metaprogramming,paradise},
  pages = {225--228}
}

@article{veldhuizenUsingTemplateMetaprograms1995a,
  title = {Using {{C}}++ Template Metaprograms},
  volume = {7},
  number = {4},
  journal = {C++ Report},
  author = {Veldhuizen, Todd},
  year = {1995},
  pages = {36--43}
}

@inproceedings{giorgidzeEmbeddingFunctionalHybrid2008,
  title = {Embedding a {{Functional Hybrid Modelling Language}} in {{Haskell}}},
  booktitle = {Proceedings of the 20th {{International Symposium}} on the {{Implementation}} and {{Application}} of {{Functional Languages}}},
  author = {Giorgidze, George and Nilsson, Henrik},
  year = {2008}
}

@inproceedings{brabrandTypedUnambiguousPattern2010,
  address = {New York, NY, USA},
  series = {{{PPDP}} '10},
  title = {Typed and {{Unambiguous Pattern Matching}} on {{Strings Using Regular Expressions}}},
  isbn = {978-1-4503-0132-9},
  abstract = {We show how to achieve typed and unambiguous declarative pattern matching on strings using regular expressions extended with a simple recording operator. We give a characterization of ambiguity of regular expressions that leads to a sound and complete static analysis. The analysis is capable of pinpointing all ambiguities in terms of the structure of the regular expression and report shortest ambiguous strings. We also show how pattern matching can be integrated into statically typed programming languages for deconstructing strings and reproducing typed and structured values. We validate our approach by giving a full implementation of the approach presented in this paper. The resulting tool, reg-exp-rec, adds typed and unambiguous pattern matching to Java in a standalone and non-intrusive manner. We evaluate the approach using several realistic examples.},
  booktitle = {Proceedings of the 12th {{International ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Declarative Programming}}},
  publisher = {{ACM}},
  doi = {10.1145/1836089.1836120},
  author = {Brabrand, Claus and Thomsen, Jakob G.},
  year = {2010},
  keywords = {parsing,ambiguity,disambiguation,pattern matching,regular expressions,static analysis,type inference},
  pages = {243--254},
  file = {/home/vipa/Zotero/storage/U2CTN8FZ/Brabrand and Thomsen - 2010 - Typed and Unambiguous Pattern Matching on Strings .pdf}
}

@inproceedings{caralpTrimmingVisiblyPushdown2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Trimming {{Visibly Pushdown Automata}}},
  isbn = {978-3-642-39274-0},
  abstract = {We study the problem of trimming visibly pushdown automata (VPA). We first describe a polynomial time procedure which, given a visibly pushdown automaton that accepts only well-nested words, returns an equivalent visibly pushdown automaton that is trimmed. We then show how this procedure can be lifted to the setting of arbitrary VPA. Furthermore, we present a way of building, given a VPA, an equivalent VPA which is both deterministic and trimmed.},
  language = {en},
  booktitle = {Implementation and {{Application}} of {{Automata}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Caralp, Mathieu and Reynier, Pierre-Alain and Talbot, Jean-Marc},
  editor = {Konstantinidis, Stavros},
  year = {2013},
  keywords = {Polynomial Time,Polynomial Time Complexity,Pushdown Automaton,Return Transition,Tree Automaton},
  pages = {84-96},
  file = {/home/vipa/Zotero/storage/UM59WUBV/Caralp et al. - 2013 - Trimming Visibly Pushdown Automata.pdf}
}

@inproceedings{palmkvistCreatingDomainSpecificLanguages2019,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Creating {{Domain}}-{{Specific Languages}} by {{Composing Syntactical Constructs}}},
  isbn = {978-3-030-05998-9},
  abstract = {Creating a programming language is a considerable undertaking, even for relatively small domain-specific languages (DSLs). Most approaches to ease this task either limit the flexibility of the DSL or consider entire languages as the unit of composition. This paper presents a new approach using syntactical constructs (also called syncons) for defining DSLs in much smaller units of composition while retaining flexibility. A syntactical construct defines a single language feature, such as an if statement or an anonymous function. Each syntactical construct is fully self-contained: it specifies its own concrete syntax, binding semantics, and runtime semantics, independently of the rest of the language. The runtime semantics are specified as a translation to a user defined target language, while the binding semantics allow name resolution before expansion. Additionally, we present a novel approach for dealing with syntactical ambiguity that arises when combining languages, even if the languages are individually unambiguous. The work is implemented and evaluated in a case study, where small subsets of OCaml and Lua have been defined and composed using syntactical constructs.},
  language = {en},
  booktitle = {Practical {{Aspects}} of {{Declarative Languages}}},
  publisher = {{Springer International Publishing}},
  author = {Palmkvist, Viktor and Broman, David},
  editor = {Alferes, Jos\'e J\'ulio and Johansson, Moa},
  year = {2019},
  pages = {187-203},
  file = {/home/vipa/Zotero/storage/T5FH2I62/Palmkvist and Broman - 2019 - Creating Domain-Specific Languages by Composing Sy.pdf}
}

@inproceedings{alurVisiblyPushdownLanguages2004,
  address = {New York, NY, USA},
  series = {{{STOC}} '04},
  title = {Visibly {{Pushdown Languages}}},
  isbn = {978-1-58113-852-8},
  abstract = {We propose the class of visibly pushdown languages as embeddings of context-free languages that is rich enough to model program analysis questions and yet is tractable and robust like the class of regular languages. In our definition, the input symbol determines when the pushdown automaton can push or pop, and thus the stack depth at every position. We show that the resulting class Vpl of languages is closed under union, intersection, complementation, renaming, concatenation, and Kleene-*, and problems such as inclusion that are undecidable for context-free languages are Exptime-complete for visibly pushdown automata. Our framework explains, unifies, and generalizes many of the decision procedures in the program analysis literature, and allows algorithmic verification of recursive programs with respect to many context-free properties including access control properties via stack inspection and correctness of procedures with respect to pre and post conditions. We demonstrate that the class Vpl is robust by giving two alternative characterizations: a logical characterization using the monadic second order (MSO) theory over words augmented with a binary matching predicate, and a correspondence to regular tree languages. We also consider visibly pushdown languages of infinite words and show that the closure properties, MSO-characterization and the characterization in terms of regular trees carry over. The main difference with respect to the case of finite words turns out to be determinizability: nondeterministic B\"uchi visibly pushdown automata are strictly more expressive than deterministic Muller visibly pushdown automata.},
  booktitle = {Proceedings of the {{Thirty}}-Sixth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  doi = {10.1145/1007352.1007390},
  author = {Alur, Rajeev and Madhusudan, P.},
  year = {2004},
  keywords = {$ømega$-languages,context-free languages,logic,pushdown automata,regular tree languages,verification},
  pages = {202--211},
  file = {/home/vipa/Zotero/storage/8DLIPGMN/Alur and Madhusudan - 2004 - Visibly Pushdown Languages.pdf}
}

@inproceedings{doczkalConstructiveTheoryRegular2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {A {{Constructive Theory}} of {{Regular Languages}} in {{Coq}}},
  isbn = {978-3-319-03545-1},
  abstract = {We present a formal constructive theory of regular languages consisting of about 1400 lines of Coq/Ssreflect. As representations we consider regular expressions, deterministic and nondeterministic automata, and Myhill and Nerode partitions. We construct computable functions translating between these representations and show that equivalence of representations is decidable. We also establish the usual closure properties, give a minimization algorithm for DFAs, and prove that minimal DFAs are unique up to state renaming. Our development profits much from Ssreflect's support for finite types and graphs.},
  language = {en},
  booktitle = {Certified {{Programs}} and {{Proofs}}},
  publisher = {{Springer International Publishing}},
  author = {Doczkal, Christian and Kaiser, Jan-Oliver and Smolka, Gert},
  editor = {Gonthier, Georges and Norrish, Michael},
  year = {2013},
  keywords = {regular expressions,Coq,finite automata,Myhill-Nerode,regular languages,Ssreflect},
  pages = {82-97},
  file = {/home/vipa/Zotero/storage/W7G5H6J5/Doczkal et al. - 2013 - A Constructive Theory of Regular Languages in Coq.pdf}
}

@article{vantangTighterBoundDeterminization2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0911.3275},
  title = {A {{Tighter Bound}} for the {{Determinization}} of {{Visibly Pushdown Automata}}},
  volume = {10},
  issn = {2075-2180},
  abstract = {Visibly pushdown automata (VPA), introduced by Alur and Madhusuan in 2004, is a subclass of pushdown automata whose stack behavior is completely determined by the input symbol according to a fixed partition of the input alphabet. Since its introduce, VPAs have been shown to be useful in various context, e.g., as specification formalism for verification and as automaton model for processing XML streams. Due to high complexity, however, implementation of formal verification based on VPA framework is a challenge. In this paper we consider the problem of implementing VPA-based model checking algorithms. For doing so, we first present an improvement on upper bound for determinization of VPA. Next, we propose simple on-the-fly algorithms to check universality and inclusion problems of this automata class. Then, we implement the proposed algorithms in a prototype tool. Finally, we conduct experiments on randomly generated VPAs. The experimental results show that the proposed algorithms are considerably faster than the standard ones.},
  journal = {Electronic Proceedings in Theoretical Computer Science},
  doi = {10.4204/EPTCS.10.5},
  author = {Van Tang, Nguyen},
  month = nov,
  year = {2009},
  keywords = {Computer Science - Formal Languages and Automata Theory,Computer Science - Logic in Computer Science,F.4.1,F.4.3},
  pages = {62-76},
  file = {/home/vipa/Zotero/storage/PMPSTZL9/Van Tang - 2009 - A Tighter Bound for the Determinization of Visibly.pdf;/home/vipa/Zotero/storage/B7N4P3B8/0911.html}
}

@article{comonTreeAutomataTechniques2007,
  title = {Tree {{Automata Techniques}} and {{Applications}}},
  author = {Comon, H. and Dauchet, M. and Gilleron, R. and L\"oding, C. and Jacquemard, F. and Lugiez, D. and Tison, S. and Tommasi, M.},
  year = {2007},
  file = {/home/vipa/Zotero/storage/C5C5FSYG/Comon et al. - 2007 - Tree Automata Techniques and Applications.pdf},
  howpublished = {Available on: http://www.grappa.univ-lille3.fr/tata},
  note = {release October, 12th 2007}
}


