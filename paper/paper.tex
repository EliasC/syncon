%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}


%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{CONF} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2018}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usepackage{tikz-qtree} % Used for syntax trees
\usepackage{tikz-cd} % Used for commutative diagrams

\usepackage{syntax}

\usepackage{semantic}

\usepackage{marginnote}

\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  basewidth={.5em,.5em},
}
\newcommand{\ocaml}{\lstinline[language={[objective]caml}]}

%% symbol definitions used throughout the paper

\newcommand{\NT}{\mathbb{N}} % Set of nonterminals
\newcommand{\T}{\Sigma} % Set of terminals
\newcommand{\Labels}{\mathbb{L}} % Set of labels
\newcommand{\yield}{\mathit{yield}} % yield of a parse tree
\newcommand{\semantic}{\mathit{unparen}} % remove semantically unimportant productions from a w' \in L(G')
\newcommand{\parse}{\mathit{parse}} % go from a w' \in L(G'_w) to a subset of L(G_t)
\newcommand{\words}{\mathit{words}} % go from a t \in L(G_t) to a subset of L(G'_w)
\newcommand{\alt}{\mathit{alt}} % go from a w \in L(G'_w) to a tuple of a basic word and a range bag.
\newcommand{\basic}{\mathit{basic}} % go from a w \in L(G'_w) to the first element of \alt(w).
\newcommand{\altset}{\mathit{altset}} % go from a w \in L(G'_w) to \alt(w), except the second element is replaced by a set (that contains an element if the bag contained at least one of that element).
\newcommand{\lattice}{\mathit{lattice}} % go from a t \in L(G_t) to its lattice, partitioned by equality on \altset and ordered by subset on the rangeset.
\newcommand{\localize}{\mathit{localize}} % go from a F \subseteq L(G_t) to a set of localized ambiguities, i.e., a set of subforests.
\newcommand{\range}[2]{#1\!-\!#2}

\begin{document}

% Submission may be up to 25 pages, excluding references

%% Title information
\title{Resolvable Ambiguity}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followe by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{Viktor Palmkvist}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{KTH Royal Institute of Technology}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{Stockholm}
  \state{State1}
  \postcode{Post-Code1}
  \country{Sweden}                    %% \country is recommended
}
\email{vipa@kth.se}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
Text of abstract \ldots.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{keyword1, keyword2, keyword3}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

Text of paper \ldots

\subsection{Motivating Ambiguity in Programming Languages}

% TODO: reference PADL paper for this example
Consider the following nested match expression in OCaml:

\begin{lstlisting}[language={[objective]caml}]
match 1 with
  | 1 -> match "one" with
         | str -> str
  | 2 -> "two"
\end{lstlisting}

\noindent The OCaml compiler, when presented with this code, will give a type error for the last line:

\begin{lstlisting}
Error: This pattern matches values of type int
       but a pattern was expected which matches
       values of type string
\end{lstlisting}

\noindent The compiler sees the last line as belonging to the inner \ocaml{match} rather than the outer, as was intended. The fix is simple; put parentheses around the inner match:

\begin{lstlisting}[language={[objective]caml}]
match 1 with
  | 1 -> (match "one" with
          | str -> str)
  | 2 -> "two"
\end{lstlisting}

\noindent The connection between the error message and the fix is not a clear one however; adding parentheses around an expression does not change the type of anything.

To come up with an alternative error to present in this case we look to the OCaml manual for inspiration. It contains an informal description of the syntax of the language\footnote{\url{https://caml.inria.fr/pub/docs/manual-ocaml/language.html}}, in the form of an EBNF-like grammar. Below is an excerpt of the productions for expressions, written in a more standard variant of EBNF:

\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= 'match' <expr> 'with' <pattern-matching>

<pattern-matching> ::= ('|' <pattern> '->' <expr>)+
\end{grammar}

Note that \synt{pattern-matching} is slightly simplified, the original grammar supports \ocaml{when} guards and makes the first \lit{|} optional. If we use this grammar to parse the nested match we find an ambiguity: the last match arm can belong to either the inner match or the outer match. The OCaml compiler makes an arbitrary choice to remove the ambiguity, which may or may not be the alternative the user intended.

We instead argue that the grammar should be left ambiguous for this sort of corner cases that are likely to trip a user, allowing the compiler to present an ambiguity error, which lets the user select the intended alternative.

\subsection{Unresolvable Ambiguity}

Unfortunately, not all ambiguities can be resolved by adding parentheses. Again, looking to the informal OCaml grammar:

\setlength{\grammarindent}{5em}
\begin{grammar}
<expr> ::= <expr> ';' <expr>
  \alt '[' <expr> (';' <expr>)* ';'? ']'
  \alt <constant>
\end{grammar}

\noindent The first production is sequential composition, the second is lists (the empty list is under \synt{constant}). Now consider the following expression: ''\ocaml{[1; 2]}''.

We find that it is ambiguous with two alternatives:
\begin{enumerate}
  \item A list with two elements.
  \item A list with one element, namely a sequential composition.
\end{enumerate}

We can select the second option by putting parentheses around ''\ocaml{1; 2}'', but there is no way to select the first. If the user intended the first option we have a problem: we can present an accurate error message, but there is no way for an end-user to solve it; it requires changes to the grammar itself.

To prevent the possibility of an end-user encountering such an error we must ensure that the grammar cannot give rise to an unresolvable ambiguity. It is worth mentioning here that statically checking if a context-free grammar is ambiguous has long been known to be undecidable \cite{cantorAmbiguityProblemBackus1962}. Unresolvable ambiguity, however, turns out to be decidable\footnote{With some caveats, I'll talk more about this during the meeting.}.

Note that, as for ambiguity, the shape of the grammar is important, since the property considers parse trees rather than merely words. For this paper, we consider context-free grammars with EBNF operators.

\subsection{Contributions}

\begin{itemize}
  \item Building on the work by \citet{palmkvistCreatingDomainSpecificLanguages2019}, which merely isolates ambiguities, an algorithm that suggests solutions to ambiguity errors.
  \item A formalization of the unresolvable ambiguity property for context-free EBNF grammars.
  \item An algorithm for deciding if a grammar is unresolvably ambiguous or not.
\end{itemize}

\section{Preliminaries}

This section briefly describes the theoretical foundations we build upon. Sections~\ref{sec:preliminaries-cfgs} and \ref{sec:preliminaries-automata} describe context-free grammars and various forms of automata, the latter with some non-standard notation. Section~\ref{sec:preliminaries-vpls} then describes visibly pushdown languages, which enable the analyses described in Sections~\ref{sec:static} and \ref{sec:dynamic}. Finally, Sections~\ref{sec:preliminaries-trees} and \ref{sec:preliminaries-ambiguity} describe trees and ambiguity, the former with some non-standard notation and the latter with a slightly wider definition than normal.

\subsection{Context-Free Grammars} \label{sec:preliminaries-cfgs}

A context-free grammar $G$ is a tuple $(\NT, \T, P, S)$ where:

\begin{itemize}
\item $\NT$ is a set of non-terminals.
\item $\T$, disjoint from $\NT$, is a set of terminals.
\item $P$, a subset of $\NT \times (\NT \cup \T)^{*}$, is a set of productions.
\item $S \in \NT$ is the starting non-terminal.
\end{itemize}

\noindent A word $w \in \T^{*}$ is recognized by $G$ if there is a sequence of steps starting with $S$ and ending with $w$, where each step replaces a single non-terminal using a production in $P$. The set of words recognized by $G$ is written $L(G)$, and is said to be the word language of $G$.

\subsection{Automata} \label{sec:preliminaries-automata}

A nondeterministic finite automaton (NFA) is a tuple $(Q, \Sigma, \delta, q_0, F)$:

\begin{itemize}
\item A finite set of states $Q$.
\item A finite set of input symbols $\Sigma$, i.e., an input alphabet.
\item A transition function $\delta: Q \times \Sigma -> 2^Q$.
\item An initial state $q_0 \in Q$.
\item A set of final states $F \subseteq Q$.
\end{itemize}

\noindent A successful run is a sequence of states $r_0, r_1, \ldots, r_n$ and a word $a_0a_1\ldots a_n$ such that:

\begin{itemize}
\item $r_0 = q_0$.
\item $\forall i \in \{0, 1, \ldots, n-1\}.\ r_{i+1} \in \delta(r_i, a_i)$.
\item $r_n \in F$.
\end{itemize}

\noindent We say that the automaton accepts the word $a_0a_1\ldots a_n$ iff there is such a succesful run.

A deterministic finite automaton (DFA) has the same definition, except $\delta : Q \times \Sigma -> Q$, i.e., given a state and a symbol there is always a single state we can transition to.

A pushdown automaton extends a finite automaton with a stack, and lets each transition push or pop symbols from it. Formally, a nondeterministic pushdown automaton is a tuple $(Q, \Sigma, \Gamma, \delta, q_0, F)$:

\begin{itemize}
\item A finite set of states $Q$.
\item A finite set of input symbols $\Sigma$, i.e., an input alphabet.
\item A finite set of stack symbols $\Gamma$, i.e., a stack alphabet, including a special bottom of stack symbol $\bot$.
\item A transition function $\delta: Q \times \Sigma \times \Gamma -> 2^{Q \times \Gamma^{*}}$.
\item An initial state $q_0 \in Q$.
\item A set of final states $F \subseteq Q$.
\end{itemize}

\noindent A successful run is now a sequence of \emph{configurations}, elements of $Q \times \Gamma^{*}$, starting with $(q_0, \epsilon)$, ending with $(f, \gamma)$ for some $f \in F$ and $\gamma \in \Gamma*$.

However, in this paper we will only consider pushdown automata with relatively limited stack manipulation, and will thus use some convenient shorthand:

\begin{itemize}
\item $p \xrightarrow{a} q$, a transition that recognizes the terminal $a$ and does not interact with the stack at all.
\item $p \xrightarrow{a, +g} q$, a transition that recognizes the terminal $a$ and pushes the symbol $g$ on the stack.
\item $p \xrightarrow{a, -g} q$, a transition that recognizes the terminal $a$ and pops the symbol $g$ from the stack (i.e., this transition cannot be taken if $g$ is not on top of the stack).
\end{itemize}

\subsection{Visibly Pushdown Languages} \label{sec:preliminaries-vpls}

A visibly pushdown language \cite{alurVisiblyPushdownLanguages2004} is a language that can be recognized by a visibly pushdown automaton. A visibly pushdown automaton is a pushdown automaton where the input alphabet $\Sigma$ can be partitioned into three disjoint sets $\Sigma_c$, $\Sigma_i$, and $\Sigma_r$, such that all transitions in the automaton has one of the following three forms:

\begin{itemize}
\item $p \xrightarrow{c, +s} q$, where $c \in \Sigma_c$.
\item $p \xrightarrow{i} q$, where $i \in \Sigma_i$.
\item $p \xrightarrow{r, -s} q$, where $r \in \Sigma_r$.
\end{itemize}

\noindent i.e., the terminal recognized by a transition fully determines the change to the stack height.

This gives us some nice properties. Of particular relevance to this paper are the following two points:

\begin{itemize}
\item If two visibly pushdown automata have the same partitions $\Sigma_c$, $\Sigma_i$, and $\Sigma_r$, then we can construct a product automaton that simulates two simultaneous runs through both automata. This product automaton has the same input alphabet partitions, each state is a pair of states (one from each automaton), and each stack symbol is a pair of stack symbols (one from each automaton).
\item A visibly pushdown automaton can be trimmed \cite{caralpTrimmingVisiblyPushdown2013}, i.e., modified in such a way that all remaining states and transitions are part of at least one successful run, none are redundant.
\end{itemize}

\subsection{Unranked Regular Tree Grammars} \label{sec:preliminaries-trees}

Trees generalize words by allowing each terminal to have more than one successor. The successors have an associated ordering, i.e., in the presence of multiple successors, one is considered to be first, one second, etc. Most literature considers \emph{ranked} tree languages, where each terminal has a fixed arity, i.e., the same terminal must always have the same number of successors. This is as opposed to \emph{unranked} tree languages, where the arity of a terminal is not fixed. In this case, the sequence of successors to a single terminal tends to be described by a word language (referred to as a horizontal language in \cite{comonTreeAutomataTechniques2007}), often a regular language.

Our later results and properties are more naturally described through unranked trees, thus all further references to trees are to unranked trees, despite ranked being more common in the literature. We further distinguish terminals used solely as leaves from terminals that may be either nodes or leaves, to give a more natural definition of $\yield$.

An unranked tree grammar $T$ is a tuple $(\NT, \T, X, P, S)$ where:

\begin{itemize}
\item $\NT$ is a set of (zero-arity) non-terminals.
\item $\T$ is a set of zero-arity terminals, used as leaves.
\item $X$ is a set of terminals without fixed arity, used as inner nodes or leaves.
\item $P$ is a set of productions, where a production has the form $N -> x(r)$ for $N \in \NT$, $x \in X$, and $r$ a regular expression over $\NT$ and $\T$.
\end{itemize}

\noindent A tree $t$ is recognized by $T$ if there is a sequence of steps starting with $S$ and ending with $t$, where each step either replaces a single non-terminal using a production in $P$, or replaces a regular expression $r$ with a sequence in $L(r)$. The set of trees recognized by $T$ is written $L(T)$\footnote{All word grammars will be named $G$ (possibly with a subscript), while all tree grammars will be named $T$ (possibly with a subscript), lessening the risk of confusion between the two cases.}, and is said to be the tree language of $T$.

$\yield$ then, is the sequence $w \in \T^{*}$ of terminals $t \in \T$ obtained by a left-to-right traversal of a tree. Informally, it is the flattening of a tree after all internal nodes have been removed.

\subsection{Ambiguity} \label{sec:preliminaries-ambiguity}

The standard definition of ambiguity, given a grammar $G$ and its parse trees $T_G$, is as follows:

\begin{definition}
A word $w \in L(G)$ is ambiguous if:
  \[ \exists t_1, t_2 \in L(T_G).\ t_1 \neq t_2 \land \yield(t_1) = w = \yield(t_2) \]
\end{definition}

\noindent For this paper, we widen the definition slightly to the following:

\begin{definition}
  A word $w \in L(G)$ is ambiguous relative a tree grammar $T$ and a function $\parse : L(G) -> 2^{L(T)}$ if:
  \[ |\parse(w)| > 1 \]
\end{definition}

\noindent where $2^S$ is the powerset of $S$ and $|S|$ is the cardinality of $S$. The standard ambiguity definition is regained by choosing

\[ \parse(w) = \{ t \mid w = \yield(t) \land t \in L(T_G) \}\ \]

\section{Parse-time Disambiguation}

We begin this section with some motivation, then list our definition of \emph{resolvable ambiguity} and what disambiguation we will consider, and finally an alternative definition of a word, which will be useful in later sections.

We can divide the productions present in a programming language grammar in two groups: those that are semantically important, and those that are semantically \emph{un}important. The former group covers most productions, while the latter contains, e.g., parentheses used for explicit grouping. If programs were written directly as syntax trees then the latter would be unnecessary; two syntax trees that differ only by parentheses are semantically the same.

Thus we wish the output of parsing to be a syntax tree consisting entirely of semantically important productions. This distinction is useful to make, because it allows us to decouple \emph{what} we want to be expressible and \emph{how} it is to be expressed.

\begin{figure}
\centering
\begin{minipage}{.47\textwidth}
  \centering
  \begin{tabular}{@{}ll@{}}
      Terminals & $t \in \T$ \\
      Non-terminals & $N \in \NT$ \\
      Labels & $l \in \Labels$ \\
      Marks & $m \subseteq \Labels$ \\
      Regular expressions & $r ::= t \mid N_m \mid r \cdot r $ \\
      & \hphantom{$r ::=$}$\mid r + r \mid \epsilon \mid r^{*}$ \\
      Labelled productions & $N -> l : r$ \\
  \end{tabular}
  \captionof{figure}{The abstract syntax of a language definition.}
  \label{fig:input-language-definition}
\end{minipage}\quad
\begin{minipage}{.45\textwidth}
  \centering
  \begin{tabular}{@{}l@{\quad$->$\quad}c@{ $:$\quad}l@{}}
    $E$ & $l$ & \verb|'['| ($E$ (\verb|';'| $E$)$^{*}$ $+$ $\epsilon$) \verb|']'| \\
    $E$ & $a$ & $E$ \verb|'+'| $E$ \\
    $E$ & $m$ & $E_{\{a\}}$ \verb|'*'| $E_{\{a\}}$ \\
    $E$ & $n$ & $N$ \\
  \end{tabular}
  \captionof{figure}{The input language definition used as a running example, an expression language with lists, addition, and multiplication, with precedence defined, but not associativity. Assumes that $N$ matches a numeric terminal.}
  \label{fig:running-example-definition}
\end{minipage}
\end{figure}

With that in mind, we define a language definition $D$ as a set of labelled productions, as described in Figure~\ref{fig:input-language-definition}. Note that we require the labels to uniquely identify the production, i.e., there can be no two distinct productions in $D$ that share the same label. A \emph{mark} is used for disambiguation, and forbids certain productions from taking the place of the marked non-terminal. To lessen clutter, we will write $E_\emptyset$ as $E$. As an example, consider the language definition in Figure~\ref{fig:running-example-definition}, which will be used as a running example. In the production describing multiplication ($m$) both non-terminals are marked with $\{a\}$, which thus forbids addition from being a child of a multiplication, enforcing conventional precedence.

From $D$ we then generate four grammars: $G_D$, $T_D$, $G'_D$, and $T'_D$. The first two represent the \emph{what}, while the latter two represent the \emph{how}:

\begin{itemize}
\item $G_D$ represents a word language describing all semantically distinct programs.
\item $T_D$ represents a tree language describing the parse trees of words in $L(G_D)$.
\item $G'_D$ is essentially a modified version of $G_D$, e.g., adding parentheses and other forms of disambiguation.
\item $T'_D$ represents a tree language describing the parse trees of words in $L(G'_D)$.
\end{itemize}

\begin{figure}

  \begin{subfigure}[t]{.45\linewidth}
    \centering
    \begin{tabular}{@{}l@{\quad$->$\quad}c@{$($ }l@{}}
      \toprule
      $E$ & $l$ & \verb|'['| $(\epsilon + E($\verb|';'| $E)^{*})$ \verb|']'| $)$ \\
      $E$ & $a$ & $E$ \verb|'+'| $E$ $)$ \\
      $E$ & $m$ & $E$ \verb|'*'| $E$ $)$ \\
      $E$ & $n$ & $N$ $)$ \\
      \bottomrule
    \end{tabular}
    \caption{$T_D$, the parse trees of $G_D$.}
    \label{fig:running-example-generated:t}
  \end{subfigure}%
%
  \begin{subfigure}[t]{.45\linewidth}
    \centering
    \begin{tabular}{@{}l@{\quad$->$\quad}c@{$($ }l@{}}
      \toprule
      $E$ & $l$ & \verb|'['| $(\epsilon + E($\verb|';'| $E)^{*})$ \verb|']'| $)$ \\
      $E$ & $a$ & $E$ \verb|'+'| $E$ $)$ \\
      $E$ & $m$ & $E_{\{a\}}$ \verb|'*'| $E_{\{a\}}$ $)$ \\
      $E$ & $n$ & $N$ $)$ \\
      $E$ & $g$ & \verb|'('| $E$ \verb|')'| $)$ \\
      \midrule
      $E_{\{a\}}$ & $l$ & \verb|'['| $(\epsilon + E($\verb|';'| $E)^{*})$ \verb|']'| $)$ \\
      $E_{\{a\}}$ & $m$ & $E_{\{a\}}$ \verb|'*'| $E_{\{a\}}$ $)$ \\
      $E_{\{a\}}$ & $n$ & $N$ $)$ \\
      $E_{\{a\}}$ & $g$ & \verb|'('| $E$ \verb|')'| $)$ \\
      \bottomrule
    \end{tabular}
    \caption{$T'_D$, the parse trees of $G'_D$.}
    \label{fig:running-example-generated:t-prime}
  \end{subfigure}

  \begin{subfigure}[t]{.45\linewidth}
    \centering
    \begin{tabular}{@{}l@{\quad$->$\quad}l@{}}
      \toprule
      $E$ & \verb|'['| $E_{l1}$ \verb|']'| \\
      $E$ & $E$ \verb|'+'| $E$ \\
      $E$ & $E$ \verb|'*'| $E$ \\
      $E$ & $N$ \\
      \midrule
      $E_{l1}$ & $\epsilon$ \\
      $E_{l1}$ & $E$ $E_{l2}$ \\
      \midrule
      $E_{l2}$ & $\epsilon$ \\
      $E_{l2}$ & \verb|';'| $E$ $E_{l2}$ \\
      \bottomrule
    \end{tabular}
    \caption{$G_D$, the generated abstract syntax.}
    \label{fig:running-example-generated:w}
  \end{subfigure}%
%
  \begin{subfigure}[t]{.45\linewidth}
    \centering
    \begin{tabular}{@{}l@{\quad$->$\quad}l@{}}
      \toprule
      $E$ & \verb|'['| $E_{l1}$ \verb|']'| \\
      $E$ & $E$ \verb|'+'| $E$ \\
      $E$ & $E_{\{a\}}$ \verb|'*'| $E_{\{a\}}$ \\
      $E$ & $N$ \\
      $E$ & \verb|'('| $E$ \verb|')'| \\
      \midrule
      $E_{\{a\}}$ & \verb|'['| $E_{l1}$ \verb|']'| \\
      $E_{\{a\}}$ & $E_{\{a\}}$ \verb|'*'| $E_{\{a\}}$ \\
      $E_{\{a\}}$ & $N$ \\
      $E_{\{a\}}$ & \verb|'('| $E$ \verb|')'| \\
      \midrule
      $E_{l1}$ & $\epsilon$ \\
      $E_{l1}$ & $E$ $E_{l2}$ \\
      \midrule
      $E_{l2}$ & $\epsilon$ \\
      $E_{l2}$ & \verb|';'| $E$ $E_{l2}$ \\
      \bottomrule
    \end{tabular}
    \caption{$G'_D$, the generated concrete syntax.}
    \label{fig:running-example-generated:w-prime}
  \end{subfigure}
  \caption{The generated grammars.}
  \label{fig:running-example-generated}
\end{figure}

\noindent Examples of words in each of these four languages can be seen in Figure~\ref{fig:example-words-in-square}, along with visualizations of the syntax trees.

{
\newcommand{\terminal}[1]{\ \underline{#1}\ }

\begin{figure}
  \begin{subfigure}[b]{.45\linewidth}
    \begin{center}
    \Tree [.{$m$}
        [.{$a$}
          [.{$n$} \terminal{1} ]
          \terminal{+}
          [.{$n$} \terminal{2} ] ]
        \terminal{*}
        [.{$n$} \terminal{3} ] ]
    \end{center}
    \[m(a(n(\terminal{1}) \terminal{+} n(\terminal{2})) \terminal{*} n(\terminal{3}))\]
    \caption{Example tree in $L(T_D)$ and visualization.}
  \end{subfigure}
  \begin{subfigure}[b]{.45\linewidth}
    \begin{center}
    \Tree [.{$m$}
        [.{$g$}
          \terminal{(}
          [.{$a$}
            [.{$n$} \terminal{1} ]
            \terminal{+}
            [.{$n$} \terminal{2} ] ]
          \terminal{)} ]
        \terminal{*}
        [.{$n$} \terminal{3} ] ]
    \end{center}
    \[m(g( \terminal{(} a(n( \terminal{1} ) \terminal{+} n( \terminal{2} )) \terminal{)} ) \terminal{*} n( \terminal{3} ))\]
    \caption{Example tree in $L(T'_D)$ and visualization.}
  \end{subfigure}

  \begin{subfigure}[b]{.40\linewidth}
    \[1 + 2 * 3\]
    \caption{Example word in $L(G_D)$.}
  \end{subfigure}
  \begin{subfigure}[b]{.40\linewidth}
    \[(1 + 2) * 3\]
    \caption{Example word in $L(G'_D)$.}
  \end{subfigure}
  \caption{Example with elements from each generated language that correspond to each other. The leaf terminals in the tree languages appear underlined to distinguish the two kinds of parentheses.}
  \label{fig:example-words-in-square}
\end{figure}
}

At this point we also note that the shape of $D$ determines where the final concrete syntax permits grouping parentheses. For example, $G_D$ in Figure~\ref{fig:running-example-generated:w} can be seen as a valid language definition (if we generate new unique labels for each of the productions). However, starting with that language definition would allow the expression ''$[1(;2)]$'', which makes no intuitive sense; grouping parentheses should only be allowed around complete expressions, but ''$;2$'' is not a valid expression.

We also require a function $\semantic : L(T'_D) -> L(T_D)$ that removes the semantically unimportant productions from a parse tree, i.e., it replaces every subtree $g($ \verb|'('| $t$ \verb|')'| $)$ with $t$. The relation between the four grammars can be seen in Figure~\ref{fig:grammar-square}. Finally, we define the function $\parse : L(G'_D) -> 2^{L(T_D)}$, and its inverse $\words : L(T_D) -> 2^{L(G'_D)}$:

$$
\begin{array}{rcl}
\parse(w') & = & \{ \semantic(t') \mid t' \in L(T'_D) \land \yield(t') = w' \} \\
\words(t) & = & \{ w' \mid t \in \parse(w') \} \\
\end{array}
$$

\noindent The latter will be useful later, while $\parse$ is used directly in the definition of \emph{resolvable ambiguity}:

\begin{figure}
  \begin{tikzcd}
    L(T_D) \arrow[d, "\yield"] & L(T'_D) \arrow[l, "\semantic"'] \arrow[d, "\yield"] \\
    L(G_D) & L(G'_D)
  \end{tikzcd}
  \caption{The grammars considered, and their relation to each other.}
  \label{fig:grammar-square}
\end{figure}

\begin{definition}
  A word $w \in L(G'_D)$ defined by the language definition $D$ is resolvably ambiguous if:
  $$
  \begin{array}{l}
  \forall t \in \parse(w).\\
  \quad \exists w' \in L(G'_D).\ \parse(w') = \{t\}
  \end{array}
  $$
\end{definition}

\noindent As an example, in the language defined in Figure~\ref{fig:running-example-definition}, the word \verb|'1 + 2 + 3'| is ambiguous, since $\parse($\verb|'1 + 2 + 3'|$) = \{t_1, t_2\}$ where

\begin{center}
  \begin{tabular}{l}
    $t_1 = a($ \hphantom{$a($} $n($ \verb|'1'| $)$ \verb|'+'| $a($ $n($ \verb|'2'| $)$ \hphantom{$)$} \verb|'+'| $n($ \verb|'3'| $)$ $)$ $)$ \\
    $t_2 = a($ $a($ $n($ \verb|'1'| $)$ \verb|'+'| \hphantom{$a($} $n($ \verb|'2'| $)$ $)$ \verb|'+'| $n($ \verb|'3'| $)$ \hphantom{$)$} $)$ \\
  \end{tabular}
\end{center}

\noindent This ambiguity can be resolved, since $\parse($\verb|'1 + (2 + 3)'|$) = \{t_1\}$ and $\parse($\verb|'(1 + 2) + 3'|$) = \{t_2\}$. To demonstrate the unresolvable case, we add the production $E -> s: E$ \verb|';'| $E$, at which point we find that the word \verb|'[1 ; 2]'| is unresolvably ambiguous; $\parse($\verb|'[1 ; 2]'|$) = \{t_3, t_4\}$ where:

\begin{center}
  \begin{tabular}{l}
    $t_3 = l($ \verb|'['| \hphantom{$s($} $n($ \verb|'1'| $)$ \verb|';'| $n($ \verb|'2'| $)$ \hphantom{$)$} \verb|']'| $)$ \\
    $t_4 = l($ \verb|'['| $s($ $n($ \verb|'1'| $)$ \verb|';'| $n($ \verb|'2'| $)$ $)$ \verb|']'| $)$ \\
  \end{tabular}
\end{center}

\noindent In this case, $t_4$ has an unambigous word (namely \verb|'[(1 ; 2)]'|), but $t_3$ does not. The solution is to modify the language definition in Figure~\ref{fig:running-example-definition} so that both non-terminals in the production $l$ are marked with $s$ (i.e., they look like $E_{\{s\}}$), at which point $\parse($\verb|'[1 ; 2]'|$) = \{t_3\}$.

We can expand this to an entire language, stating that a language is resolvably ambiguous iff all its words are resolvably ambiguous, or more concisely:

\begin{definition}
  A language defined by the language definition $D$ is resolvably ambiguous if:
  $$
  \begin{array}{l}
  \forall t \in L(T_D).\\
  \quad \exists w' \in L(G'_D).\ \parse(w') = \{t\}
  \end{array}
  $$
\end{definition}

\noindent Intuitively, a language is resolvably ambiguous if there is at least one (unambiguous\footnote{Using $\parse$ and $T_D$, i.e., relating $G'_D$ and $T_D$, instead of $G'_D$ and its parse trees.}) word for each semantically distinct tree. Note that neither $G_D$ nor $G'_D$ necessarily need to be unambiguous for this to hold.

We can now state the two central problems we wish to consider:

\begin{itemize}
\item Static resolvability analysis: determine if a given language definition $D$ is resolvably ambiguous, otherwise produce an unresolvably ambiguous word $w \in L(G'_D)$. Section~\ref{sec:static} provides a partial solution to this problem.
\item Dynamic resolvability analysis: given a language definition $D$ and a word $w \in L(G'_D)$, determine if it is resolvably ambiguous, and if so, present an unambiguous word for every tree $t \in \parse(w)$. Section~\ref{sec:dynamic} provides a full solution with one caveat: it only considers languages with balanced parentheses.
\end{itemize}

\noindent Additionally, on the dynamic side, there is another property of great practical relevance; we want ambiguities to be minimal and independent. In a practical setting, it is rarely helpful to merely state that an entire program is ambiguous. Instead, we wish to minimize the reported ambiguous portion of the program to, e.g., a single expression. Section~\ref{sec:local} addresses this.

\section{Static Resolvability Analysis} \label{sec:static}

This section describes a decision algorithm for detecting unresolvable ambiguities in a language definition, starting with a version with several limitations, most of which are later lifted. The overarching goal is to find a tree $t \in L(G_t)$ such that there is no $w' \in L(G'_w)$ for which $\parse(w') = \{t\}$, or prove that no such tree exists. Alternatively, find a tree that has no unambiguous words.

\subsection{An Alternative Word View} \label{sec:word-view}

This section introduces an alternative (isomorphic) definition of a word that will be used to motivate the correctness of later algorithms. The method by which we generate $G_D$ and $G'_D$ limits the possible differences between them significantly. In particular, no new terminals are introduced, except '(' and ')', and they are always introduced in a well-balanced fashion.

If we thus delimit ourselves to only consider languages where words have no unbalanced parentheses\footnote{I.e., the vast, vast majority of programming languages currently in use.} we can give the following alternative definition of a word: a word is a two-tuple containing a sequence of non-parenthesis terminals and a bag (or multiset) of ranges covered by parentheses. For example, the word ''$(1 + 2) * 3$'' is equivalent to $(\text{''}1 + 2 * 3\text{''}, \{\range{1}{3}\})$, while ''$((1 + 2)) * 3$'' is equivalent to $(\text{''}1 + 2 * 3\text{''}, \{\range{1}{3}, \range{1}{3}\})$. The first component will be referred to as a \emph{basic word}, the second as a \emph{range bag}

We will now note a few things about the words $w' \in \words(t)$ for some given $t \in L(G_t)$:

\begin{itemize}
\item All $w'$ have the same basic word. $G'_w$ is constructed in such a way that the only possible difference is one of parentheses. This also implies that two trees that share a word (i.e., that can be ambiguous) must also share a basic word.
\item Some parentheses are required, i.e., some ranges must be present in all $w'$. For example, removing the parentheses in ''$(1 + 2) * 3$'' changes the tree produced, since multiplication has higher precedence than addition.
\item There is a finite set of possible ranges in the range bags of $w'$. Grouping parentheses can only be added if they exactly cover a node in the parse tree, and other parentheses can only be added where $G_w$ allows them.
\item Duplicated grouping parentheses do not matter. For example, $((1 + 2)) * 3$ permits the same syntax trees as $(1 + 2) * 3$. If there are no parentheses present in $G_w$ then all parentheses are grouping parentheses, thus we can consider the range bag as a \emph{set} instead.
\end{itemize}

\subsection{Basic Algorithm}

Our initial limitations / assumptions are as follows:

\begin{enumerate}
  \item The input language definition contains no parentheses. This implies that all parentheses in $G'_w$ are grouping parentheses, thus we only need to consider words $w' \in L(G'_w)$ whose range bag is a set.

  \item No non-terminals in the input language definition are marked, i.e., there are no forbidden children. This implies that there are no required grouping parentheses.

  \item No production has a right-hand side that matches a single non-terminal, i.e., $\forall (N->i:r) \in D.\ \NT \cap L(r) = \emptyset$\footnote{I haven't really written a proper description for the language definition, but I'm here using $D$ for it, and considering it essentially as a set of ''productions''.}.
\end{enumerate}

\noindent The first two limitations allow us to place all words in $\words(t)$ for some given $t$ in a lattice, whose structure is formed by the subset ordering of the range sets. For example, the two trees in Figure~\ref{fig:ambig-example-square} have the lattices of words seen in Figures~\ref{fig:lattice1} and \ref{fig:lattice2} respectively.

The bottom word corresponds to the set of required parentheses (i.e., the empty set, in this sub-problem), while the top word corresponds to the set of possible parentheses ranges. All trees that can be ambiguous must thus share the same bottom word, while the top may differ.

Of particular use is to examine whether the top word (call it $w'$) is ambiguous. There are two cases:

\begin{itemize}
\item The top word is unambiguous, then this is not a tree we are looking for (since it has at least one unambiguous word).
\item It is ambiguous. That means that there is some other lattice for some other tree that also contains the word. Call the top word of this other lattice $w'_2$. For $w'$ to be in the lattice whose top is $w'_2$ its rangeset must be a subset of the rangeset of the latter. But this is true also for all the other words in the lattice, thus there are no unambiguous words in the lattice, i.e., this tree is what we are looking for.
\end{itemize}

\noindent The algorithm thus looks for two trees, where the set of possible parentheses for one is a superset of the other. To do this, we use a linear representation of each lattice: namely the top word. If two trees have the same top word, that implies that their sets of possible parentheses are equal. If we can add parentheses to one word and get the other word, that implies that the rangeset of the former is a subset of the latter.

We will now construct a pushdown automaton that recognizes these top words in such a way that there is a bijection between successful runs and trees in $L(G_t)$. As a running example, we will use the following (very simple) language definition $D$:

\begin{center}
\begin{tabular}{@{}l@{\quad$->$\quad}l@{ $:$\quad}l@{}}
  \synt{N} & Succ & 's' \synt{N}\\
  \synt{N} & Zero & 'z' \\
\end{tabular}
\end{center}

\noindent We begin by constructing a DFA per production. This can be done in the standard way by constructing an NFA, then determinizing it, and optionally minimizing it.

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
    scale = 1,transform shape]

  \node[state,initial] (1) {$1$};
  \node[state] (2) [right of=1] {$2$};
  \node[state,accepting] (3) [right of=2] {$3$};
  \node[state,initial] (4) [below of=1, xshift=1cm] {$4$};
  \node[state,accepting] (5) [right of=4] {$5$};

  \path (1) edge              node {$s$} (2)
  (2) edge              node {$N$} (3)
  (4) edge              node {$z$} (5);

\end{tikzpicture}
\end{center}

\noindent We then combine them into a single pushdown automata by replacing each edge with a non-terminal label $p \xrightarrow{N} q$ with:

\begin{itemize}
  \item an edge $p \xrightarrow{'(', +(p, q)} p'$ for every initial state $p'$ in some DFA belonging to non-terminal $N$, and
  \item an edge $q' \xrightarrow{')', -(p, q)} q$ for every final state $q'$ in some DFA belonging to non-terminal $N$,
\end{itemize}

\noindent Intuitively, we parse a child node, but put parentheses around it, then return. This is where we use assumption 3, without it we might introduce double parentheses here.

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
    scale = 1,transform shape]

  \node[state,initial] (1) {$1$};
  \node[state] (2) [right of=1] {$2$};
  \node[state,accepting] (3) [right of=2] {$3$};
  \node[state,initial] (4) [below of=1, xshift=1cm] {$4$};
  \node[state,accepting] (5) [right of=4] {$5$};

  \draw
  (1) edge[above] node{$s$} (2)
  (4) edge[above] node{$z$} (5)
  (2) edge[bend left, below] node{$'(', +(2, 3)$} (1)
  (2) edge[bend left, left] node{$'(', +(2, 3)$} (4)
  (3) edge[loop above] node{$')', -(2, 3)$} (3)
  (5) edge[left, bend right] node{$')', -(2, 3)$} (3);

\end{tikzpicture}
\end{center}

\noindent Finally, we add a new initial state and a new final state, and connect them with the initial and final states belonging to the starting non-terminal:

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
    scale = 1,transform shape]

  \node[state] (1) {$1$};
  \node[state] (2) [right of=1] {$2$};
  \node[state] (3) [right of=2] {$3$};
  \node[state] (4) [below of=1, xshift=1cm] {$4$};
  \node[state] (5) [right of=4] {$5$};
  \node[state,initial] (s) [left of=4] {};
  \node[state,accepting] (e) [right of=5] {};

  \draw
  (s) edge[left] node{$'(', +s$} (1)
  (s) edge[above] node{$'(', +s$} (4)
  (3) edge[right] node{$')', -s$} (e)
  (5) edge[above] node{$')', -s$} (e)
  (1) edge[above] node{$s$} (2)
  (4) edge[above] node{$z$} (5)
  (2) edge[bend left, below] node{$'(', +(2, 3)$} (1)
  (2) edge[bend left, left] node{$'(', +(2, 3)$} (4)
  (3) edge[loop above] node{$')', -(2, 3)$} (3)
  (5) edge[left, bend right] node{$')', -(2, 3)$} (3);

\end{tikzpicture}
\end{center}

\noindent The resulting pushdown automaton has only a single source of non-determinism: the edges labelled \verb|'('|. Each one corresponds to one of the allowable child productions at that point in the parse tree.

We now have a pushdown automaton (call it $A_{()}$) that recognizes the top word for each tree in $L(G_t)$. Next we need to be able to add arbitrary parentheses, to produce a word with a rangeset superset. To do this we create a copy of $A_{()}$ with one modification: for every state $s$ in $A_{()}$ (except the initial and final states), add two transitions $s \xrightarrow{'(', +p} s$ and $s \xrightarrow{')', -p} s$. To avoid cluttering the graph too much, these transitions are shown unlabeled below:

\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2cm,
    scale = 1,transform shape]

  \node[state] (1) {$1$};
  \node[state] (2) [right of=1] {$2$};
  \node[state] (3) [right of=2] {$3$};
  \node[state] (4) [below of=1, xshift=1cm] {$4$};
  \node[state] (5) [right of=4] {$5$};
  \node[state,initial] (s) [left of=4] {};
  \node[state,accepting] (e) [right of=5] {};

  \draw
  (1) edge[loop, in=80, out=100, looseness=7] node{} (1)
  (1) edge[loop, in=70, out=110, looseness=6] node{} (1)
  (2) edge[loop, in=80, out=100, looseness=7] node{} (2)
  (2) edge[loop, in=70, out=110, looseness=6] node{} (2)
  (3) edge[loop, in=-10, out=10, looseness=7] node{} (3)
  (3) edge[loop, in=-20, out=20, looseness=6] node{} (3)
  (4) edge[loop, in=-80, out=-100, looseness=7] node{} (4)
  (4) edge[loop, in=-70, out=-110, looseness=6] node{} (4)
  (5) edge[loop, in=-80, out=-100, looseness=7] node{} (5)
  (5) edge[loop, in=-70, out=-110, looseness=6] node{} (5)
  (s) edge[left] node{$'(', +s$} (1)
  (s) edge[above] node{$'(', +s$} (4)
  (3) edge[right] node{$')', -s$} (e)
  (5) edge[above] node{$')', -s$} (e)
  (1) edge[above] node{$s$} (2)
  (4) edge[above] node{$z$} (5)
  (2) edge[bend left, below] node{$'(', +(2, 3)$} (1)
  (2) edge[bend left, left] node{$'(', +(2, 3)$} (4)
  (3) edge[loop above] node{$')', -(2, 3)$} (3)
  (5) edge[left, bend right] node{$')', -(2, 3)$} (3);

\end{tikzpicture}
\end{center}

\noindent We will call this automaton $A'_{()}$. Successful runs in this automaton have a surjection to runs in $A_($ (ignore transitions along the newly added edges), and thus also have a surjection to parse trees in $L(G_t)$. We can also note that every sucessful run in $A_{()}$ is also a successful run in $A'_{()}$, since the latter has all states and transitions of the former. Furthermore, two distinct runs, one in $A_{()}$ (call it $p$) and one in $A'_{()}$ (call it $p'$), that both recognize the same word must represent different parse trees in $L(G_t)$. To see why, we consider two cases:

\begin{enumerate}
  \item $p'$ only uses transitions present in $A_{()}$. This is a successful run in $A_{()}$, and distinct from $p$. But there is a bijection between runs in $A_{()}$ and parse trees in $L(T)$, thus $p'$ represents a different parse tree.
  \item $p'$ uses at least one transition added in $A'_{()}$. Using the surjection between runs in $A'_{()}$ and $A_{()}$ we find a new successful run that produces a different word (at least one fewer pair of parentheses). Since this run produces a different word, it must be distinct from $p$, and thus represent a different parse tree.
\end{enumerate}

\noindent Two distinct successful runs that accept the same word thus represent two trees where one permits a superset rangeset of the other. To find such runs we construct a product automaton and trim it. We can construct a product automaton since both $A_{()}$ and $A'_{()}$ are visibly pushdown, with the same partition of the input alphabet (push on open parenthesis, pop on close parenthesis, do nothing otherwise). We can trim the product since it retains the same partitioning and thus is also visibly pushdown.

In this product automaton, if any transition pushes a stack symbol $(a, b)$ where $a \neq b$, or transitions to a state $(p, q)$ where $p \neq q$, then there is a successful run that corresponds to two distinct runs through $A_{()}$ and $A'_{()}$ (since the automaton is trim).

\section{Dynamic Resolvability Analysis} \label{sec:dynamic}

\section{Local Ambiguities} \label{sec:local}

% TODO: this section should be moved to be in the appropriate position {
\newpage

\begin{figure*}[ht]
  \begin{tikzcd}
    L(G_w) & L(G_t) \arrow[l, "\yield"] \\
    L(G'_w) & L(G'_t) \arrow[u, "\semantic"'] \arrow[l, "\yield"]
  \end{tikzcd}
  \caption{The generated grammars and their relation to each other.}
\end{figure*}



\begin{figure*}[ht]
  \begin{subfigure}[b]{.45\linewidth}
    \begin{align*}
      1 + 2 + 3
    \end{align*}
    \caption{Example word in $L(G_w)$.}
  \end{subfigure}
  \begin{subfigure}[b]{.45\linewidth}
      \begin{align*}
        [_S           [_S  &[_N 1 ]_N + \hphantom{[_S} [_N 2 ]_N           ]_S  + [_N 3 ]_N \hphantom{]_S} ]_S \\
        [_S \hphantom{[_S} &[_N 1 ]_N +           [_S  [_N 2 ]_N \hphantom{]_S} + [_N 3 ]_N           ]_S  ]_S
      \end{align*}
    \caption{Example words in $L(G_T)$.}
  \end{subfigure}

  \begin{subfigure}[b]{.45\linewidth}
    \begin{align*}
      1 + 2 + 3
    \end{align*}
    \caption{Example word in $L(G'_w)$.}
  \end{subfigure}
  \begin{subfigure}[b]{.45\linewidth}
      \begin{align*}
        [_S            [_S &[_N 1 ]_N + \hphantom{[_S} [_N 2 ]_N            ]_S + [_N 3 ]_N \hphantom{]_S}]_S \\
        [_S \hphantom{[_S} &[_N 1 ]_N +            [_S [_N 2 ]_N \hphantom{]_S} + [_N 3 ]_N ]_S ]_S
      \end{align*}
    \caption{Example words in $L(G'_T)$.}
  \end{subfigure}
  \caption{Example with an ambiguous word in $L(G'_w)$ with corresponding words from the other grammars.}
  \label{fig:ambig-example-square}
\end{figure*}

\begin{figure*}[ht]
  \begin{tikzpicture}[every node/.style={rectangle,draw}]
    \node (a) [draw=white] at (0,5) {$((1) + (2)) + (3)$};

    \node (b1) at (-3.6,4.2) {$(1) + (2) + (3)$};
    \node (b2) [draw=white] at (-1.2,4.2) {$((1) + (2)) + 3$};
    \node (b3) [draw=white] at ( 1.2,4.2) {$((1) + 2) + (3)$};
    \node (b4) [draw=white] at ( 3.6,4.2) {$(1 + (2)) + (3)$};

    \node (c1) at (-5.5,3) {$(1) + (2) + 3$};
    \node (c2) at (-3.3,3) {$(1) + 2 + (3)$};
    \node (c3) [draw=white] at (-1.1,3) {$((1) + 2) + 3$};
    \node (c4) at (1.1,3) {$1 + (2) + (3)$};
    \node (c5) [draw=white] at (3.3,3) {$(1 + (2)) + 3$};
    \node (c6) [draw=white] at (5.5,3) {$(1 + 2) + (3)$};

    \node (d1) at (-3.6,1.8) {$(1) + 2 + 3$};
    \node (d2) at (-1.2,1.8) {$1 + (2) + 3$};
    \node (d3) at ( 1.2,1.8) {$1 + 2 + (3)$};
    \node (d4) [draw=white] at ( 3.6,1.8) {$(1 + 2) + 3$};

    \node (e) at (0, 1) {$1 + 2 + 3$};

  \draw (e) -- (d1);
  \draw (e) -- (d2);
  \draw (e) -- (d3);
  \draw (e) -- (d4);

  \draw (d1.90) -- (c1.270);
  \draw (d1.90) -- (c2.270);
  \draw (d1.90) -- (c3.270);
  \draw (d2.90) -- (c1.270);
  \draw (d2.90) -- (c4.270);
  \draw (d2.90) -- (c5.270);
  \draw (d3.90) -- (c2.270);
  \draw (d3.90) -- (c4.270);
  \draw (d3.90) -- (c6.270);
  \draw (d4.90) -- (c3.270);
  \draw (d4.90) -- (c5.270);
  \draw (d4.90) -- (c6.270);

  \draw (c1.90) -- (b1.270);
  \draw (c1.90) -- (b2.270);
  \draw (c2.90) -- (b1.270);
  \draw (c2.90) -- (b3.270);
  \draw (c3.90) -- (b2.270);
  \draw (c3.90) -- (b3.270);
  \draw (c4.90) -- (b1.270);
  \draw (c4.90) -- (b4.270);
  \draw (c5.90) -- (b2.270);
  \draw (c5.90) -- (b4.270);
  \draw (c6.90) -- (b3.270);
  \draw (c6.90) -- (b4.270);

  \draw (b1) -- (a);
  \draw (b2) -- (a);
  \draw (b3) -- (a);
  \draw (b4) -- (a);
    %%   \draw[preaction={draw=white, -,line width=6pt}] (a) -- (e) -- (c);
  \end{tikzpicture}
  \caption{The lattice of words for the tree $[_S [_S [_N 1 ]_N + [_N 2 ]_N ]_S + [_N 3 ]_N ]_S$ (in $L(G_t)$). The boxed words are shared with Figure~\ref{fig:lattice2}.}
  \label{fig:lattice1}
\end{figure*}


\begin{figure*}[t]

\begin{tikzpicture}[every node/.style={rectangle,draw}]
  \node (a) [draw=white] at (0,5) {$(1) + ((2) + (3))$};

  \node (b1) at (-3.6,4.2) {$(1) + (2) + (3)$};
  \node (b2) [draw=white] at (-1.2,4.2) {$(1) + ((2) + 3)$};
  \node (b3) [draw=white] at ( 1.2,4.2) {$(1) + (2 + (3))$};
  \node (b4) [draw=white] at ( 3.6,4.2) {$1 + ((2) + (3))$};

  \node (c1) at (-5.5,3) {$(1) + (2) + 3$};
  \node (c2) at (-3.3,3) {$(1) + 2 + (3)$};
  \node (c3) [draw=white] at (-1.1,3) {$(1) + (2 + 3)$};
  \node (c4) at (1.1,3) {$1 + (2) + (3)$};
  \node (c5) [draw=white] at (3.3,3) {$1 + ((2) + 3)$};
  \node (c6) [draw=white] at (5.5,3) {$1 + (2 + (3))$};

  \node (d1) at (-3.6,1.8) {$(1) + 2 + 3$};
  \node (d2) at (-1.2,1.8) {$1 + (2) + 3$};
  \node (d3) at ( 1.2,1.8) {$1 + 2 + (3)$};
  \node (d4) [draw=white] at ( 3.6,1.8) {$1 + (2 + 3)$};

  \node (e) at (0, 1) {$1 + 2 + 3$};

  \draw (e) -- (d1);
  \draw (e) -- (d2);
  \draw (e) -- (d3);
  \draw (e) -- (d4);

  \draw (d1.90) -- (c1.270);
  \draw (d1.90) -- (c2.270);
  \draw (d1.90) -- (c3.270);
  \draw (d2.90) -- (c1.270);
  \draw (d2.90) -- (c4.270);
  \draw (d2.90) -- (c5.270);
  \draw (d3.90) -- (c2.270);
  \draw (d3.90) -- (c4.270);
  \draw (d3.90) -- (c6.270);
  \draw (d4.90) -- (c3.270);
  \draw (d4.90) -- (c5.270);
  \draw (d4.90) -- (c6.270);

  \draw (c1.90) -- (b1.270);
  \draw (c1.90) -- (b2.270);
  \draw (c2.90) -- (b1.270);
  \draw (c2.90) -- (b3.270);
  \draw (c3.90) -- (b2.270);
  \draw (c3.90) -- (b3.270);
  \draw (c4.90) -- (b1.270);
  \draw (c4.90) -- (b4.270);
  \draw (c5.90) -- (b2.270);
  \draw (c5.90) -- (b4.270);
  \draw (c6.90) -- (b3.270);
  \draw (c6.90) -- (b4.270);

  \draw (b1) -- (a);
  \draw (b2) -- (a);
  \draw (b3) -- (a);
  \draw (b4) -- (a);
%%   \draw[preaction={draw=white, -,line width=6pt}] (a) -- (e) -- (c);
\end{tikzpicture}
\caption{The lattice of words for the tree $[_S [_N 1 ]_N + [_S [_N 2 ]_N + [_N 3 ]_N ]_S ]_S$ (in $L(G_t)$). The boxed words are shared with Figure~\ref{fig:lattice1}.}
\label{fig:lattice2}
\end{figure*}

\clearpage

% }

\section{Parsetime Ambiguity Reporting} \label{sec:parse-time-reporting}

\section{Properties We May Want to Prove}

\subsection{Static Stuff}

\begin{lemma}
There is an injective function ($\alt$) from well-balanced words to tuples of basic words and range bags.
\end{lemma}

\begin{lemma}
$\forall t \in L(G_t), w_1, w_2 \in L(G'_w).\ \{w_1, w_2\} \subseteq \words(t) -> \basic(w_1) = \basic(w_2)$
\label{lemma:tree-single-basic}
\end{lemma}

\begin{lemma}
$\forall t \in L(G_t).\ \words(t) \neq \emptyset$
\label{lemma:words-nonempty}
\end{lemma}

\begin{lemma}
$\forall t_1, t_2 \in L(G_t).\ \words(t_1) \cap \words(t_2) \neq \emptyset -> \basic(t_1) = \basic(t_2)$ \\
(We're using \ref{lemma:tree-single-basic} and \ref{lemma:words-nonempty} to (ab)use ''$\basic$'' on trees as well)
\end{lemma}

\begin{definition}
A language definition is in version 1 or 2 if $\T \cap \mathit{par} = \emptyset$.
\end{definition}

\begin{theorem}
In version 1 and 2: $\forall w_1, w_2 \in L(G'_w).\ \altset(w_1) = \altset(w_2) -> \parse(w_1) = \parse(w_2)$, where $\altset(w)$ is the same as $\alt(w)$, but with the second component (the rangebag) replaced with a corresponding set.
\end{theorem}

\begin{lemma}
In version 1 and 2, given a $t \in L(G_t)$, the words in $\words(t)$ can be partitioned into a lattice. Each partition contains the words that are equal by $\altset$, and ordering is by subset on the rangeset. Furthermore, this lattice is bounded, and uniquely described by its bottom and top elements; it contains all elements between bottom and top. We denote this lattice as $\lattice(t)$.
\label{lemma:bounded-lattice}
\end{lemma}

\begin{definition}
A language definition is in version 1 if it contains no marked non-terminals and $\T \cap \mathit{par} = \emptyset$.
\end{definition}

\begin{lemma}
In version 1: the bottom element of the lattice for a tree has a rangeset that is the empty set.
\end{lemma}

\begin{theorem}
Given a tree $t \in L(G_t)$ in version 1, $\exists w \in L(G'_w).\ \parse(w) = \{t\} <-> \neg \exists t' \in L(G_t).\ t \neq t' \land \lattice(t) \subseteq \lattice(t')$, i.e., a tree is resolvable iff there is no other tree that entirely covers its lattice.
\end{theorem}

\begin{lemma}
(Version 1 and 2) We can construct a VPDA that recognizes a linear encoding of the lattices of trees in $L(G_t)$, such that there is a bijection between successful runs and trees.
\label{lemma:linear-lattices}
\end{lemma}

\begin{lemma}
(Version 1 and 2) Using \ref{lemma:linear-lattices}, we can construct a VPDA that reconizes a linear encoding of ''superlattices'' of trees in $L(G_t)$ (i.e., lattices that completely cover the original lattices), where each successful run corresponds to exactly one tree.
\label{lemma:linear-super-lattices}
\end{lemma}

\begin{theorem}
(Version 1 and 2) Using \ref{lemma:linear-lattices}, \ref{lemma:linear-super-lattices}, and their product automaton, we can construct a VPDA such that there is a bijection between successful runs and pairs of trees in $L(G_t)$ such that the lattice of one is entirely covered by the other.
\end{theorem}

\begin{theorem}
We can construct a sound, decidable algorithm that takes a language definition in version 1 or 2 and answers ''resolvably ambiguous'', ''unresolvably ambiguous'', or ''unknown''. Additionally, in version 1, this algorithm will never answer ''unknown'', i.e., it is then complete.
\label{lemma:static-algorithm}
\end{theorem}

\begin{definition}
Version 2.5 allows a language definition to use parentheses, but restricts it to never produce double parentheses, and changes $G'_w$ to also never recognize double parentheses.
\end{definition}

\begin{theorem}
\ref{lemma:bounded-lattice} holds also for version 2.5, thus we can extend \ref{lemma:static-algorithm} to cover version 2.5 as well.
\end{theorem}

\subsection{Dynamic Stuff}

This section assumes that we at no point produce an infinite amount of parse trees. My intuition (corroborated by some quick googling) suggests that this can only happen if there is a productive, accessible nonterminal $N$ such that $N =>^{+} N$. This is not checked at the moment.

\begin{lemma}
$\forall t \in L(G_t).\ \words(t)$ is a visibly pushdown language.
\end{lemma}

\begin{theorem}
  Given a set of trees in $L(G_t)$, we can construct a corresponding set of VPDAs that recognize the words that are unique to each tree (i.e., isn't in $\words(t)$ for some other tree $t$ in the set).
\end{theorem}

\begin{theorem}
  We can construct an algorithm $\localize(F)$ that takes a finite parse forest $F \subseteq L(G_t)$ and selects a set of subforests\footnote{This is a term that I have introduced, and so it will need to be described. Basically a parse forest for a part of a word, where each tree is a subtree of the original parse forest.} such that:
  \begin{itemize}
  \item The unselected parts are identical across the different syntax trees, including range (i.e., we report all ambiguities).
  \item No selected subforest is contained in another (i.e., we only report one ambiguity per range).
  \item No selected subforest can be replaced by zero or more of its children while still satisfying the points above (i.e., the selected subforests are minimal\footnote{By covered range, not by count.}).
  \end{itemize}
\end{theorem}

\begin{theorem}
  Given an ambiguous word $w \in L(G'_w)$, a localized ambiguity $F' \in \localize(\parse(w))$ covering the subword $w' \in L_{G'_w}(N)$ (i.e., $\parse_N(w') = F'$), and a resolution $w'' \in L_{G'_w}(N)$ such that $\parse_N(w'') = \{t\} \subset \parse_N(w')$ for some $t \in L_{G_t}(N)$, the following holds:
  \begin{itemize}
  \item $\localize(\parse(w[w'/w''])) = \localize(\parse(w)) \setminus F'$
  \end{itemize}
  where $w[w'/w'']$ denotes replacing a subword $w'$ with $w''$.
\end{theorem}

%% \noindent Generalizing this to a complete grammar, we get the following property: a grammar $G$ is resolvable iff:

%% $$
%% \begin{array}{l}
%%   \forall t \in L(T).\\
%%   \quad \exists w' \in L(G').\\
%%   \qquad \forall t' \in L(T').\ \yield(t') = w' -> \unparen(t') = t
%% \end{array}
%% $$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \begin{theorem}
%%   If $\{(, )\} \cap \T = \emptyset$, then any unambiguous word $w \in L(G)$ is resolvable.
%% \end{theorem}

%% \noindent \textbf{Proof.} Since $w$ is unambiguous in $G$ there is exactly one parse tree $t \in L(T)$ such that $\yield(t) = w$. Since $L(G) \subseteq L(G')$ we have that $w \in L(G')$. Furthermore, $w$ is unambiguous in $L(G')$ as well, since no added productions can apply (they all contain parentheses, which cannot be present in $w$). Additionally, since $L(T) \subseteq L(T')$, we have that $t \in L(T')$, and since $\yield(t) = w$, it must be the only parse tree for $w$ in $L(T')$. Finally, $\unparen(t) = t$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% %% Acknowledgments
%% \begin{acks}                            %% acks environment is optional
%%                                         %% contents suppressed with 'anonymous'
%%   %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%%   %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%%   %% acknowledge financial support and will be used by metadata
%%   %% extraction tools.
%%   This material is based upon work supported by the
%%   \grantsponsor{GS100000001}{National Science
%%     Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%%   No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%%   No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%%   conclusions or recommendations expressed in this material are those
%%   of the author and do not necessarily reflect the views of the
%%   National Science Foundation.
%% \end{acks}


%% Bibliography
\bibliography{All}


%% %% Appendix
%% \appendix
%% \section{Appendix}

%% Text of appendix \ldots

\end{document}
